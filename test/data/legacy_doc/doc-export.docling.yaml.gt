body:
  children:
  - $ref: '#/texts/0'
  - $ref: '#/texts/1'
  - $ref: '#/texts/2'
  - $ref: '#/texts/3'
  - $ref: '#/texts/4'
  - $ref: '#/texts/5'
  - $ref: '#/texts/6'
  - $ref: '#/texts/7'
  - $ref: '#/texts/8'
  - $ref: '#/texts/9'
  - $ref: '#/texts/10'
  - $ref: '#/texts/11'
  - $ref: '#/pictures/0'
  - $ref: '#/texts/12'
  - $ref: '#/texts/13'
  - $ref: '#/texts/14'
  - $ref: '#/texts/15'
  - $ref: '#/texts/16'
  - $ref: '#/texts/17'
  - $ref: '#/texts/18'
  - $ref: '#/texts/19'
  - $ref: '#/texts/20'
  - $ref: '#/texts/21'
  - $ref: '#/texts/22'
  - $ref: '#/texts/23'
  - $ref: '#/texts/24'
  - $ref: '#/texts/25'
  - $ref: '#/texts/26'
  - $ref: '#/texts/27'
  - $ref: '#/texts/28'
  - $ref: '#/texts/29'
  - $ref: '#/pictures/1'
  - $ref: '#/texts/31'
  - $ref: '#/texts/32'
  - $ref: '#/texts/33'
  - $ref: '#/texts/34'
  - $ref: '#/texts/35'
  - $ref: '#/texts/36'
  - $ref: '#/texts/37'
  - $ref: '#/texts/38'
  - $ref: '#/texts/39'
  - $ref: '#/texts/40'
  - $ref: '#/texts/41'
  - $ref: '#/texts/42'
  - $ref: '#/texts/43'
  - $ref: '#/texts/44'
  - $ref: '#/texts/45'
  - $ref: '#/groups/0'
  - $ref: '#/texts/48'
  - $ref: '#/texts/49'
  - $ref: '#/texts/50'
  - $ref: '#/pictures/2'
  - $ref: '#/texts/52'
  - $ref: '#/texts/53'
  - $ref: '#/texts/54'
  - $ref: '#/texts/55'
  - $ref: '#/texts/56'
  - $ref: '#/texts/57'
  - $ref: '#/groups/1'
  - $ref: '#/texts/59'
  - $ref: '#/texts/60'
  - $ref: '#/texts/61'
  - $ref: '#/texts/62'
  - $ref: '#/texts/63'
  - $ref: '#/texts/64'
  - $ref: '#/texts/65'
  - $ref: '#/texts/66'
  - $ref: '#/texts/67'
  - $ref: '#/texts/68'
  - $ref: '#/pictures/3'
  - $ref: '#/texts/70'
  - $ref: '#/texts/71'
  - $ref: '#/texts/72'
  - $ref: '#/texts/73'
  - $ref: '#/texts/74'
  - $ref: '#/texts/75'
  - $ref: '#/tables/0'
  - $ref: '#/texts/77'
  - $ref: '#/texts/78'
  - $ref: '#/texts/79'
  - $ref: '#/texts/80'
  - $ref: '#/texts/81'
  - $ref: '#/tables/1'
  - $ref: '#/texts/83'
  - $ref: '#/texts/84'
  - $ref: '#/pictures/4'
  - $ref: '#/texts/86'
  - $ref: '#/texts/87'
  - $ref: '#/texts/88'
  - $ref: '#/texts/89'
  - $ref: '#/pictures/5'
  - $ref: '#/texts/91'
  - $ref: '#/texts/92'
  - $ref: '#/texts/93'
  - $ref: '#/texts/94'
  - $ref: '#/texts/95'
  - $ref: '#/texts/96'
  - $ref: '#/texts/97'
  - $ref: '#/groups/2'
  - $ref: '#/texts/99'
  - $ref: '#/texts/100'
  - $ref: '#/texts/101'
  - $ref: '#/texts/102'
  - $ref: '#/texts/103'
  - $ref: '#/texts/104'
  - $ref: '#/texts/105'
  - $ref: '#/texts/106'
  - $ref: '#/texts/107'
  - $ref: '#/texts/108'
  - $ref: '#/texts/109'
  - $ref: '#/texts/110'
  - $ref: '#/texts/111'
  - $ref: '#/texts/112'
  - $ref: '#/texts/113'
  - $ref: '#/texts/114'
  - $ref: '#/texts/115'
  - $ref: '#/texts/116'
  - $ref: '#/texts/117'
  - $ref: '#/texts/118'
  - $ref: '#/texts/119'
  - $ref: '#/groups/3'
  - $ref: '#/texts/121'
  - $ref: '#/texts/122'
  - $ref: '#/texts/123'
  content_layer: body
  label: unspecified
  name: _root_
  self_ref: '#/body'
furniture:
  children: []
  content_layer: furniture
  label: unspecified
  name: _root_
  self_ref: '#/furniture'
groups:
- children:
  - $ref: '#/texts/46'
  - $ref: '#/texts/47'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/0'
- children:
  - $ref: '#/texts/58'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/1'
- children:
  - $ref: '#/texts/98'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/2'
- children:
  - $ref: '#/texts/120'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/3'
key_value_items: []
name: '2305'
origin:
  binary_hash: 8240558336632491037
  filename: '2305.03393'
  mimetype: application/pdf
pages:
  '1':
    page_no: 1
    size:
      height: 792.0
      width: 612.0
  '10':
    page_no: 10
    size:
      height: 792.0
      width: 612.0
  '11':
    page_no: 11
    size:
      height: 792.0
      width: 612.0
  '12':
    page_no: 12
    size:
      height: 792.0
      width: 612.0
  '13':
    page_no: 13
    size:
      height: 792.0
      width: 612.0
  '14':
    page_no: 14
    size:
      height: 792.0
      width: 612.0
  '2':
    page_no: 2
    size:
      height: 792.0
      width: 612.0
  '3':
    page_no: 3
    size:
      height: 792.0
      width: 612.0
  '4':
    page_no: 4
    size:
      height: 792.0
      width: 612.0
  '5':
    page_no: 5
    size:
      height: 792.0
      width: 612.0
  '6':
    page_no: 6
    size:
      height: 792.0
      width: 612.0
  '7':
    page_no: 7
    size:
      height: 792.0
      width: 612.0
  '8':
    page_no: 8
    size:
      height: 792.0
      width: 612.0
  '9':
    page_no: 9
    size:
      height: 792.0
      width: 612.0
pictures:
- annotations: []
  captions: []
  children: []
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 366.73089599609375
      coord_origin: BOTTOMLEFT
      l: 150.2235870361328
      r: 464.26727294921875
      t: 583.0986938476562
    charspan:
    - 0
    - 574
    page_no: 2
  references: []
  self_ref: '#/pictures/0'
- annotations: []
  captions:
  - $ref: '#/texts/30'
  children:
  - $ref: '#/texts/30'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 452.6416320800781
      coord_origin: BOTTOMLEFT
      l: 137.50827026367188
      r: 476.2366638183594
      t: 558.8683471679688
    charspan:
    - 0
    - 73
    page_no: 5
  references: []
  self_ref: '#/pictures/1'
- annotations: []
  captions:
  - $ref: '#/texts/51'
  children:
  - $ref: '#/texts/51'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 512.275146484375
      coord_origin: BOTTOMLEFT
      l: 164.66944885253906
      r: 448.8334045410156
      t: 627.7261352539062
    charspan:
    - 0
    - 197
    page_no: 7
  references: []
  self_ref: '#/pictures/2'
- annotations: []
  captions:
  - $ref: '#/texts/69'
  children:
  - $ref: '#/texts/69'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 198.9825897216797
      coord_origin: BOTTOMLEFT
      l: 141.89651489257812
      r: 471.9888610839844
      t: 284.9159851074219
    charspan:
    - 0
    - 104
    page_no: 8
  references: []
  self_ref: '#/pictures/3'
- annotations: []
  captions:
  - $ref: '#/texts/85'
  children:
  - $ref: '#/texts/85'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 129.50628662109375
      coord_origin: BOTTOMLEFT
      l: 163.2478790283203
      r: 451.5814208984375
      t: 347.82171630859375
    charspan:
    - 0
    - 271
    page_no: 10
  references: []
  self_ref: '#/pictures/4'
- annotations: []
  captions:
  - $ref: '#/texts/90'
  children:
  - $ref: '#/texts/90'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 158.00119018554688
      coord_origin: BOTTOMLEFT
      l: 168.10372924804688
      r: 447.4647216796875
      t: 608.7948608398438
    charspan:
    - 0
    - 391
    page_no: 11
  references: []
  self_ref: '#/pictures/5'
schema_name: DoclingDocument
tables:
- captions:
  - $ref: '#/texts/76'
  children:
  - $ref: '#/texts/76'
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 160.3699951171875
          r: 168.0479278564453
          t: 450.2650451660156
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: '#'
      - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 207.9739990234375
          r: 215.6519317626953
          t: 450.2650451660156
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: '#'
      - bbox:
          b: 436.7162780761719
          coord_origin: BOTTOMLEFT
          l: 239.79800415039062
          r: 278.3176574707031
          t: 444.7860412597656
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: Language
      - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 324.6700134277344
          r: 348.2641906738281
          t: 450.2650451660156
        col_span: 3
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 324.6700134277344
          r: 348.2641906738281
          t: 450.2650451660156
        col_span: 3
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 324.6700134277344
          r: 348.2641906738281
          t: 450.2650451660156
        col_span: 3
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 431.2362976074219
          coord_origin: BOTTOMLEFT
          l: 394.927001953125
          r: 418.4727783203125
          t: 450.2650451660156
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 0
        text: mAP (0.75)
      - bbox:
          b: 442.1952819824219
          coord_origin: BOTTOMLEFT
          l: 430.77099609375
          r: 467.1423034667969
          t: 450.2650451660156
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 0
        text: Inference
    - - bbox:
          b: 429.2442932128906
          coord_origin: BOTTOMLEFT
          l: 144.5919952392578
          r: 183.82806396484375
          t: 437.3140563964844
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: enc-layers
      - bbox:
          b: 429.2442932128906
          coord_origin: BOTTOMLEFT
          l: 192.1949920654297
          r: 231.43106079101562
          t: 437.3140563964844
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: dec-layers
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: ''
      - bbox:
          b: 429.2442932128906
          coord_origin: BOTTOMLEFT
          l: 286.6860046386719
          r: 312.3326110839844
          t: 437.3140563964844
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 1
        text: simple
      - bbox:
          b: 429.2442932128906
          coord_origin: BOTTOMLEFT
          l: 320.7019958496094
          r: 353.7198791503906
          t: 437.3140563964844
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 1
        text: complex
      - bbox:
          b: 429.2442932128906
          coord_origin: BOTTOMLEFT
          l: 369.3059997558594
          r: 379.03094482421875
          t: 437.3140563964844
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 1
        text: all
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 1
        text: ''
      - bbox:
          b: 431.2362976074219
          coord_origin: BOTTOMLEFT
          l: 427.14801025390625
          r: 470.76055908203125
          t: 439.3060607910156
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 1
        text: time (secs)
    - - bbox:
          b: 410.4142761230469
          coord_origin: BOTTOMLEFT
          l: 161.906005859375
          r: 166.512939453125
          t: 418.4840393066406
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: '6'
      - bbox:
          b: 410.4142761230469
          coord_origin: BOTTOMLEFT
          l: 209.50900268554688
          r: 214.11593627929688
          t: 418.4840393066406
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: '6'
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 245.17599487304688
          r: 272.9395446777344
          t: 423.96405029296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: OTSL HTML
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 289.0169982910156
          r: 310.0037536621094
          t: 423.96405029296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 2
        text: 0.965 0.969
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 326.7170104980469
          r: 347.7037658691406
          t: 423.96405029296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 2
        text: 0.934 0.927
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 363.6759948730469
          r: 384.6627502441406
          t: 423.96405029296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 2
        text: 0.955 0.955
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 396.20599365234375
          r: 417.1927490234375
          t: 424.0268249511719
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 2
        text: 0.88 0.857
      - bbox:
          b: 402.9422912597656
          coord_origin: BOTTOMLEFT
          l: 439.5270080566406
          r: 458.3842468261719
          t: 424.0268249511719
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 2
        text: 2.73 5.39
    - - bbox:
          b: 384.11328125
          coord_origin: BOTTOMLEFT
          l: 161.906005859375
          r: 166.512939453125
          t: 392.18304443359375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: '4'
      - bbox:
          b: 384.11328125
          coord_origin: BOTTOMLEFT
          l: 209.50900268554688
          r: 214.11593627929688
          t: 392.18304443359375
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: '4'
      - bbox:
          b: 376.64129638671875
          coord_origin: BOTTOMLEFT
          l: 245.17599487304688
          r: 272.9395446777344
          t: 397.66204833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: OTSL HTML
      - bbox:
          b: 376.64129638671875
          coord_origin: BOTTOMLEFT
          l: 289.0169982910156
          r: 310.0037536621094
          t: 397.66204833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 3
        text: 0.938 0.952
      - bbox:
          b: 376.64129638671875
          coord_origin: BOTTOMLEFT
          l: 326.7170104980469
          r: 347.7037658691406
          t: 397.66204833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 3
        text: 0.904 0.909
      - bbox:
          b: 376.8475341796875
          coord_origin: BOTTOMLEFT
          l: 362.0880126953125
          r: 386.2488708496094
          t: 397.66204833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 3
        text: 0.927 0.938
      - bbox:
          b: 376.64129638671875
          coord_origin: BOTTOMLEFT
          l: 394.6180114746094
          r: 418.77886962890625
          t: 397.7248229980469
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 3
        text: 0.853 0.843
      - bbox:
          b: 376.64129638671875
          coord_origin: BOTTOMLEFT
          l: 439.5270080566406
          r: 458.3842468261719
          t: 397.7248229980469
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 3
        text: 1.97 3.77
    - - bbox:
          b: 357.8122863769531
          coord_origin: BOTTOMLEFT
          l: 161.906005859375
          r: 166.512939453125
          t: 365.8820495605469
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: '2'
      - bbox:
          b: 357.8122863769531
          coord_origin: BOTTOMLEFT
          l: 209.50900268554688
          r: 214.11593627929688
          t: 365.8820495605469
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: '4'
      - bbox:
          b: 350.3403015136719
          coord_origin: BOTTOMLEFT
          l: 245.17599487304688
          r: 272.9395446777344
          t: 371.3610534667969
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: OTSL HTML
      - bbox:
          b: 350.3403015136719
          coord_origin: BOTTOMLEFT
          l: 289.0169982910156
          r: 310.0037536621094
          t: 371.3610534667969
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 4
        text: 0.923 0.945
      - bbox:
          b: 350.3403015136719
          coord_origin: BOTTOMLEFT
          l: 326.7170104980469
          r: 347.7037658691406
          t: 371.3610534667969
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 4
        text: 0.897 0.901
      - bbox:
          b: 350.5465393066406
          coord_origin: BOTTOMLEFT
          l: 362.0880126953125
          r: 386.2488708496094
          t: 371.3610534667969
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 4
        text: 0.915 0.931
      - bbox:
          b: 350.3403015136719
          coord_origin: BOTTOMLEFT
          l: 394.6180114746094
          r: 418.77886962890625
          t: 371.423828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 4
        text: 0.859 0.834
      - bbox:
          b: 350.3403015136719
          coord_origin: BOTTOMLEFT
          l: 439.5270080566406
          r: 458.3842468261719
          t: 371.423828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 4
        text: 1.91 3.81
    - - bbox:
          b: 331.5102844238281
          coord_origin: BOTTOMLEFT
          l: 161.906005859375
          r: 166.512939453125
          t: 339.5800476074219
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: '4'
      - bbox:
          b: 331.5102844238281
          coord_origin: BOTTOMLEFT
          l: 209.50900268554688
          r: 214.11593627929688
          t: 339.5800476074219
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 5
        text: '2'
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 245.17599487304688
          r: 272.9395446777344
          t: 345.06005859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 5
        text: OTSL HTML
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 289.0169982910156
          r: 310.0037536621094
          t: 345.06005859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 5
        text: 0.952 0.944
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 326.7170104980469
          r: 347.7037658691406
          t: 345.06005859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 5
        text: 0.92 0.903
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 362.0880126953125
          r: 386.2488708496094
          t: 345.1228332519531
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 5
        text: 0.942 0.931
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 394.6180114746094
          r: 418.77886962890625
          t: 345.1228332519531
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 5
        text: 0.857 0.824
      - bbox:
          b: 324.0382995605469
          coord_origin: BOTTOMLEFT
          l: 439.5270080566406
          r: 458.3842468261719
          t: 345.1228332519531
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 5
        text: 1.22 2
    num_cols: 8
    num_rows: 6
    table_cells:
    - bbox:
        b: 442.1952819824219
        coord_origin: BOTTOMLEFT
        l: 160.3699951171875
        r: 168.0479278564453
        t: 450.2650451660156
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: '#'
    - bbox:
        b: 442.1952819824219
        coord_origin: BOTTOMLEFT
        l: 207.9739990234375
        r: 215.6519317626953
        t: 450.2650451660156
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 0
      text: '#'
    - bbox:
        b: 436.7162780761719
        coord_origin: BOTTOMLEFT
        l: 239.79800415039062
        r: 278.3176574707031
        t: 444.7860412597656
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: Language
    - bbox:
        b: 442.1952819824219
        coord_origin: BOTTOMLEFT
        l: 324.6700134277344
        r: 348.2641906738281
        t: 450.2650451660156
      col_span: 3
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 0
      text: TEDs
    - bbox:
        b: 431.2362976074219
        coord_origin: BOTTOMLEFT
        l: 394.927001953125
        r: 418.4727783203125
        t: 450.2650451660156
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 0
      text: mAP (0.75)
    - bbox:
        b: 442.1952819824219
        coord_origin: BOTTOMLEFT
        l: 430.77099609375
        r: 467.1423034667969
        t: 450.2650451660156
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 0
      text: Inference
    - bbox:
        b: 429.2442932128906
        coord_origin: BOTTOMLEFT
        l: 144.5919952392578
        r: 183.82806396484375
        t: 437.3140563964844
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: enc-layers
    - bbox:
        b: 429.2442932128906
        coord_origin: BOTTOMLEFT
        l: 192.1949920654297
        r: 231.43106079101562
        t: 437.3140563964844
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 1
      text: dec-layers
    - col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 1
      text: ''
    - bbox:
        b: 429.2442932128906
        coord_origin: BOTTOMLEFT
        l: 286.6860046386719
        r: 312.3326110839844
        t: 437.3140563964844
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 1
      text: simple
    - bbox:
        b: 429.2442932128906
        coord_origin: BOTTOMLEFT
        l: 320.7019958496094
        r: 353.7198791503906
        t: 437.3140563964844
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 1
      text: complex
    - bbox:
        b: 429.2442932128906
        coord_origin: BOTTOMLEFT
        l: 369.3059997558594
        r: 379.03094482421875
        t: 437.3140563964844
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 1
      text: all
    - col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 1
      text: ''
    - bbox:
        b: 431.2362976074219
        coord_origin: BOTTOMLEFT
        l: 427.14801025390625
        r: 470.76055908203125
        t: 439.3060607910156
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 1
      text: time (secs)
    - bbox:
        b: 410.4142761230469
        coord_origin: BOTTOMLEFT
        l: 161.906005859375
        r: 166.512939453125
        t: 418.4840393066406
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: '6'
    - bbox:
        b: 410.4142761230469
        coord_origin: BOTTOMLEFT
        l: 209.50900268554688
        r: 214.11593627929688
        t: 418.4840393066406
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 2
      text: '6'
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 245.17599487304688
        r: 272.9395446777344
        t: 423.96405029296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 2
      text: OTSL HTML
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 289.0169982910156
        r: 310.0037536621094
        t: 423.96405029296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 2
      text: 0.965 0.969
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 326.7170104980469
        r: 347.7037658691406
        t: 423.96405029296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 2
      text: 0.934 0.927
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 363.6759948730469
        r: 384.6627502441406
        t: 423.96405029296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 2
      text: 0.955 0.955
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 396.20599365234375
        r: 417.1927490234375
        t: 424.0268249511719
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 2
      text: 0.88 0.857
    - bbox:
        b: 402.9422912597656
        coord_origin: BOTTOMLEFT
        l: 439.5270080566406
        r: 458.3842468261719
        t: 424.0268249511719
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 2
      text: 2.73 5.39
    - bbox:
        b: 384.11328125
        coord_origin: BOTTOMLEFT
        l: 161.906005859375
        r: 166.512939453125
        t: 392.18304443359375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: '4'
    - bbox:
        b: 384.11328125
        coord_origin: BOTTOMLEFT
        l: 209.50900268554688
        r: 214.11593627929688
        t: 392.18304443359375
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 3
      text: '4'
    - bbox:
        b: 376.64129638671875
        coord_origin: BOTTOMLEFT
        l: 245.17599487304688
        r: 272.9395446777344
        t: 397.66204833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 3
      text: OTSL HTML
    - bbox:
        b: 376.64129638671875
        coord_origin: BOTTOMLEFT
        l: 289.0169982910156
        r: 310.0037536621094
        t: 397.66204833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 3
      text: 0.938 0.952
    - bbox:
        b: 376.64129638671875
        coord_origin: BOTTOMLEFT
        l: 326.7170104980469
        r: 347.7037658691406
        t: 397.66204833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 3
      text: 0.904 0.909
    - bbox:
        b: 376.8475341796875
        coord_origin: BOTTOMLEFT
        l: 362.0880126953125
        r: 386.2488708496094
        t: 397.66204833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 3
      text: 0.927 0.938
    - bbox:
        b: 376.64129638671875
        coord_origin: BOTTOMLEFT
        l: 394.6180114746094
        r: 418.77886962890625
        t: 397.7248229980469
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 3
      text: 0.853 0.843
    - bbox:
        b: 376.64129638671875
        coord_origin: BOTTOMLEFT
        l: 439.5270080566406
        r: 458.3842468261719
        t: 397.7248229980469
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 3
      text: 1.97 3.77
    - bbox:
        b: 357.8122863769531
        coord_origin: BOTTOMLEFT
        l: 161.906005859375
        r: 166.512939453125
        t: 365.8820495605469
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: '2'
    - bbox:
        b: 357.8122863769531
        coord_origin: BOTTOMLEFT
        l: 209.50900268554688
        r: 214.11593627929688
        t: 365.8820495605469
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 4
      text: '4'
    - bbox:
        b: 350.3403015136719
        coord_origin: BOTTOMLEFT
        l: 245.17599487304688
        r: 272.9395446777344
        t: 371.3610534667969
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 4
      text: OTSL HTML
    - bbox:
        b: 350.3403015136719
        coord_origin: BOTTOMLEFT
        l: 289.0169982910156
        r: 310.0037536621094
        t: 371.3610534667969
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 4
      text: 0.923 0.945
    - bbox:
        b: 350.3403015136719
        coord_origin: BOTTOMLEFT
        l: 326.7170104980469
        r: 347.7037658691406
        t: 371.3610534667969
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 4
      text: 0.897 0.901
    - bbox:
        b: 350.5465393066406
        coord_origin: BOTTOMLEFT
        l: 362.0880126953125
        r: 386.2488708496094
        t: 371.3610534667969
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 4
      text: 0.915 0.931
    - bbox:
        b: 350.3403015136719
        coord_origin: BOTTOMLEFT
        l: 394.6180114746094
        r: 418.77886962890625
        t: 371.423828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 4
      text: 0.859 0.834
    - bbox:
        b: 350.3403015136719
        coord_origin: BOTTOMLEFT
        l: 439.5270080566406
        r: 458.3842468261719
        t: 371.423828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 4
      text: 1.91 3.81
    - bbox:
        b: 331.5102844238281
        coord_origin: BOTTOMLEFT
        l: 161.906005859375
        r: 166.512939453125
        t: 339.5800476074219
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: '4'
    - bbox:
        b: 331.5102844238281
        coord_origin: BOTTOMLEFT
        l: 209.50900268554688
        r: 214.11593627929688
        t: 339.5800476074219
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 5
      text: '2'
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 245.17599487304688
        r: 272.9395446777344
        t: 345.06005859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 5
      text: OTSL HTML
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 289.0169982910156
        r: 310.0037536621094
        t: 345.06005859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 5
      text: 0.952 0.944
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 326.7170104980469
        r: 347.7037658691406
        t: 345.06005859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 5
      text: 0.92 0.903
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 362.0880126953125
        r: 386.2488708496094
        t: 345.1228332519531
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 5
      text: 0.942 0.931
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 394.6180114746094
        r: 418.77886962890625
        t: 345.1228332519531
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 5
      text: 0.857 0.824
    - bbox:
        b: 324.0382995605469
        coord_origin: BOTTOMLEFT
        l: 439.5270080566406
        r: 458.3842468261719
        t: 345.1228332519531
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 5
      text: 1.22 2
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 322.41912841796875
      coord_origin: BOTTOMLEFT
      l: 140.09193420410156
      r: 474.6266174316406
      t: 454.7212829589844
    charspan:
    - 0
    - 0
    page_no: 9
  references: []
  self_ref: '#/tables/0'
- captions:
  - $ref: '#/texts/82'
  children:
  - $ref: '#/texts/82'
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 617.371337890625
          coord_origin: BOTTOMLEFT
          l: 160.78199768066406
          r: 194.99778747558594
          t: 625.4410400390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: Data set
      - bbox:
          b: 617.3963012695312
          coord_origin: BOTTOMLEFT
          l: 215.52499389648438
          r: 254.04464721679688
          t: 625.4660034179688
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: Language
      - bbox:
          b: 622.851318359375
          coord_origin: BOTTOMLEFT
          l: 300.3970031738281
          r: 323.9911804199219
          t: 630.9210205078125
        col_span: 3
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 622.851318359375
          coord_origin: BOTTOMLEFT
          l: 300.3970031738281
          r: 323.9911804199219
          t: 630.9210205078125
        col_span: 3
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 622.851318359375
          coord_origin: BOTTOMLEFT
          l: 300.3970031738281
          r: 323.9911804199219
          t: 630.9210205078125
        col_span: 3
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: TEDs
      - bbox:
          b: 617.371337890625
          coord_origin: BOTTOMLEFT
          l: 370.3450012207031
          r: 414.7466125488281
          t: 625.4410400390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 0
        text: mAP(0.75)
      - bbox:
          b: 611.892333984375
          coord_origin: BOTTOMLEFT
          l: 423.114013671875
          r: 466.7265625
          t: 630.9210205078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 0
        text: Inference time (secs)
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: ''
      - bbox:
          b: 609.8992919921875
          coord_origin: BOTTOMLEFT
          l: 262.4129943847656
          r: 288.0596008300781
          t: 617.968994140625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: simple
      - bbox:
          b: 609.8992919921875
          coord_origin: BOTTOMLEFT
          l: 296.4289855957031
          r: 329.4468688964844
          t: 617.968994140625
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 1
        text: complex
      - bbox:
          b: 609.8992919921875
          coord_origin: BOTTOMLEFT
          l: 345.0329895019531
          r: 354.7579345703125
          t: 617.968994140625
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 1
        text: all
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 1
        text: ''
    - - bbox:
          b: 591.0703125
          coord_origin: BOTTOMLEFT
          l: 154.53799438476562
          r: 201.2412872314453
          t: 599.1400146484375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: PubTabNet
      - bbox:
          b: 596.54931640625
          coord_origin: BOTTOMLEFT
          l: 222.43699645996094
          r: 247.13226318359375
          t: 604.6190185546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: OTSL
      - bbox:
          b: 596.54931640625
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 604.6190185546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: '0.965'
      - bbox:
          b: 596.54931640625
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 604.6190185546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 2
        text: '0.934'
      - bbox:
          b: 596.54931640625
          coord_origin: BOTTOMLEFT
          l: 339.40301513671875
          r: 360.3897705078125
          t: 604.6190185546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 2
        text: '0.955'
      - bbox:
          b: 596.7554931640625
          coord_origin: BOTTOMLEFT
          l: 383.1159973144531
          r: 401.9732360839844
          t: 604.6818237304688
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 2
        text: '0.88'
      - bbox:
          b: 596.7554931640625
          coord_origin: BOTTOMLEFT
          l: 435.4930114746094
          r: 454.3502502441406
          t: 604.6818237304688
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 2
        text: '2.73'
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: ''
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 220.9029998779297
          r: 248.66656494140625
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: HTML
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: '0.969'
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 3
        text: '0.927'
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 339.40301513671875
          r: 360.3897705078125
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 3
        text: '0.955'
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 382.052001953125
          r: 403.03875732421875
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 3
        text: '0.857'
      - bbox:
          b: 583.5983276367188
          coord_origin: BOTTOMLEFT
          l: 436.73199462890625
          r: 453.11181640625
          t: 591.6680297851562
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 3
        text: '5.39'
    - - bbox:
          b: 564.768310546875
          coord_origin: BOTTOMLEFT
          l: 155.94500732421875
          r: 199.833740234375
          t: 572.8380126953125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: FinTabNet
      - bbox:
          b: 570.248291015625
          coord_origin: BOTTOMLEFT
          l: 222.43699645996094
          r: 247.13226318359375
          t: 578.3179931640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: OTSL
      - bbox:
          b: 570.248291015625
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 578.3179931640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: '0.955'
      - bbox:
          b: 570.248291015625
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 578.3179931640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 4
        text: '0.961'
      - bbox:
          b: 570.4544677734375
          coord_origin: BOTTOMLEFT
          l: 337.81500244140625
          r: 361.9758605957031
          t: 578.3807983398438
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 4
        text: '0.959'
      - bbox:
          b: 570.4544677734375
          coord_origin: BOTTOMLEFT
          l: 380.4639892578125
          r: 404.6248474121094
          t: 578.3807983398438
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 4
        text: '0.862'
      - bbox:
          b: 570.4544677734375
          coord_origin: BOTTOMLEFT
          l: 435.4930114746094
          r: 454.3502502441406
          t: 578.3807983398438
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 4
        text: '1.85'
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: ''
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 220.9029998779297
          r: 248.66656494140625
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 5
        text: HTML
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 5
        text: '0.917'
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 5
        text: '0.922'
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 341.70599365234375
          r: 358.0858154296875
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 5
        text: '0.92'
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 382.052001953125
          r: 403.03875732421875
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 5
        text: '0.722'
      - bbox:
          b: 557.2963256835938
          coord_origin: BOTTOMLEFT
          l: 436.73199462890625
          r: 453.11181640625
          t: 565.3660278320312
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 5
        text: '3.26'
    - - bbox:
          b: 538.4673461914062
          coord_origin: BOTTOMLEFT
          l: 148.62600708007812
          r: 207.15240478515625
          t: 546.5370483398438
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: PubTables-1M
      - bbox:
          b: 543.9473266601562
          coord_origin: BOTTOMLEFT
          l: 222.43699645996094
          r: 247.13226318359375
          t: 552.0170288085938
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 6
        text: OTSL
      - bbox:
          b: 543.9473266601562
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 552.0170288085938
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 6
        text: '0.987'
      - bbox:
          b: 543.9473266601562
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 552.0170288085938
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 6
        text: '0.964'
      - bbox:
          b: 544.1535034179688
          coord_origin: BOTTOMLEFT
          l: 337.81500244140625
          r: 361.9758605957031
          t: 552.079833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 6
        text: '0.977'
      - bbox:
          b: 544.1535034179688
          coord_origin: BOTTOMLEFT
          l: 380.4639892578125
          r: 404.6248474121094
          t: 552.079833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 6
        text: '0.896'
      - bbox:
          b: 544.1535034179688
          coord_origin: BOTTOMLEFT
          l: 435.4930114746094
          r: 454.3502502441406
          t: 552.079833984375
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 6
        text: '1.79'
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: ''
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 220.9029998779297
          r: 248.66656494140625
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 7
        text: HTML
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 264.7439880371094
          r: 285.7307434082031
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 7
        text: '0.983'
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 302.4440002441406
          r: 323.4307556152344
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 7
        text: '0.944'
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 339.40301513671875
          r: 360.3897705078125
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 7
        text: '0.966'
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 382.052001953125
          r: 403.03875732421875
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 7
        text: '0.889'
      - bbox:
          b: 530.9953002929688
          coord_origin: BOTTOMLEFT
          l: 436.73199462890625
          r: 453.11181640625
          t: 539.0650024414062
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 7
        text: '3.26'
    num_cols: 7
    num_rows: 8
    table_cells:
    - bbox:
        b: 617.371337890625
        coord_origin: BOTTOMLEFT
        l: 160.78199768066406
        r: 194.99778747558594
        t: 625.4410400390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: Data set
    - bbox:
        b: 617.3963012695312
        coord_origin: BOTTOMLEFT
        l: 215.52499389648438
        r: 254.04464721679688
        t: 625.4660034179688
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 0
      text: Language
    - bbox:
        b: 622.851318359375
        coord_origin: BOTTOMLEFT
        l: 300.3970031738281
        r: 323.9911804199219
        t: 630.9210205078125
      col_span: 3
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: TEDs
    - bbox:
        b: 617.371337890625
        coord_origin: BOTTOMLEFT
        l: 370.3450012207031
        r: 414.7466125488281
        t: 625.4410400390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 0
      text: mAP(0.75)
    - bbox:
        b: 611.892333984375
        coord_origin: BOTTOMLEFT
        l: 423.114013671875
        r: 466.7265625
        t: 630.9210205078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 0
      text: Inference time (secs)
    - col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: ''
    - col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 1
      text: ''
    - bbox:
        b: 609.8992919921875
        coord_origin: BOTTOMLEFT
        l: 262.4129943847656
        r: 288.0596008300781
        t: 617.968994140625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 1
      text: simple
    - bbox:
        b: 609.8992919921875
        coord_origin: BOTTOMLEFT
        l: 296.4289855957031
        r: 329.4468688964844
        t: 617.968994140625
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 1
      text: complex
    - bbox:
        b: 609.8992919921875
        coord_origin: BOTTOMLEFT
        l: 345.0329895019531
        r: 354.7579345703125
        t: 617.968994140625
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 1
      text: all
    - col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 1
      text: ''
    - col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 1
      text: ''
    - bbox:
        b: 591.0703125
        coord_origin: BOTTOMLEFT
        l: 154.53799438476562
        r: 201.2412872314453
        t: 599.1400146484375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: PubTabNet
    - bbox:
        b: 596.54931640625
        coord_origin: BOTTOMLEFT
        l: 222.43699645996094
        r: 247.13226318359375
        t: 604.6190185546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 2
      text: OTSL
    - bbox:
        b: 596.54931640625
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 604.6190185546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 2
      text: '0.965'
    - bbox:
        b: 596.54931640625
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 604.6190185546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 2
      text: '0.934'
    - bbox:
        b: 596.54931640625
        coord_origin: BOTTOMLEFT
        l: 339.40301513671875
        r: 360.3897705078125
        t: 604.6190185546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 2
      text: '0.955'
    - bbox:
        b: 596.7554931640625
        coord_origin: BOTTOMLEFT
        l: 383.1159973144531
        r: 401.9732360839844
        t: 604.6818237304688
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 2
      text: '0.88'
    - bbox:
        b: 596.7554931640625
        coord_origin: BOTTOMLEFT
        l: 435.4930114746094
        r: 454.3502502441406
        t: 604.6818237304688
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 2
      text: '2.73'
    - col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: ''
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 220.9029998779297
        r: 248.66656494140625
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 3
      text: HTML
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 3
      text: '0.969'
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 3
      text: '0.927'
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 339.40301513671875
        r: 360.3897705078125
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 3
      text: '0.955'
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 382.052001953125
        r: 403.03875732421875
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 3
      text: '0.857'
    - bbox:
        b: 583.5983276367188
        coord_origin: BOTTOMLEFT
        l: 436.73199462890625
        r: 453.11181640625
        t: 591.6680297851562
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 3
      text: '5.39'
    - bbox:
        b: 564.768310546875
        coord_origin: BOTTOMLEFT
        l: 155.94500732421875
        r: 199.833740234375
        t: 572.8380126953125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: FinTabNet
    - bbox:
        b: 570.248291015625
        coord_origin: BOTTOMLEFT
        l: 222.43699645996094
        r: 247.13226318359375
        t: 578.3179931640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 4
      text: OTSL
    - bbox:
        b: 570.248291015625
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 578.3179931640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 4
      text: '0.955'
    - bbox:
        b: 570.248291015625
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 578.3179931640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 4
      text: '0.961'
    - bbox:
        b: 570.4544677734375
        coord_origin: BOTTOMLEFT
        l: 337.81500244140625
        r: 361.9758605957031
        t: 578.3807983398438
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 4
      text: '0.959'
    - bbox:
        b: 570.4544677734375
        coord_origin: BOTTOMLEFT
        l: 380.4639892578125
        r: 404.6248474121094
        t: 578.3807983398438
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 4
      text: '0.862'
    - bbox:
        b: 570.4544677734375
        coord_origin: BOTTOMLEFT
        l: 435.4930114746094
        r: 454.3502502441406
        t: 578.3807983398438
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 4
      text: '1.85'
    - col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: ''
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 220.9029998779297
        r: 248.66656494140625
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 5
      text: HTML
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 5
      text: '0.917'
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 5
      text: '0.922'
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 341.70599365234375
        r: 358.0858154296875
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 5
      text: '0.92'
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 382.052001953125
        r: 403.03875732421875
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 5
      text: '0.722'
    - bbox:
        b: 557.2963256835938
        coord_origin: BOTTOMLEFT
        l: 436.73199462890625
        r: 453.11181640625
        t: 565.3660278320312
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 5
      text: '3.26'
    - bbox:
        b: 538.4673461914062
        coord_origin: BOTTOMLEFT
        l: 148.62600708007812
        r: 207.15240478515625
        t: 546.5370483398438
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: PubTables-1M
    - bbox:
        b: 543.9473266601562
        coord_origin: BOTTOMLEFT
        l: 222.43699645996094
        r: 247.13226318359375
        t: 552.0170288085938
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 6
      text: OTSL
    - bbox:
        b: 543.9473266601562
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 552.0170288085938
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 6
      text: '0.987'
    - bbox:
        b: 543.9473266601562
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 552.0170288085938
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 6
      text: '0.964'
    - bbox:
        b: 544.1535034179688
        coord_origin: BOTTOMLEFT
        l: 337.81500244140625
        r: 361.9758605957031
        t: 552.079833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 6
      text: '0.977'
    - bbox:
        b: 544.1535034179688
        coord_origin: BOTTOMLEFT
        l: 380.4639892578125
        r: 404.6248474121094
        t: 552.079833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 6
      text: '0.896'
    - bbox:
        b: 544.1535034179688
        coord_origin: BOTTOMLEFT
        l: 435.4930114746094
        r: 454.3502502441406
        t: 552.079833984375
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 6
      text: '1.79'
    - col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 7
      text: ''
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 220.9029998779297
        r: 248.66656494140625
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 7
      text: HTML
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 264.7439880371094
        r: 285.7307434082031
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 7
      text: '0.983'
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 302.4440002441406
        r: 323.4307556152344
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 7
      text: '0.944'
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 339.40301513671875
        r: 360.3897705078125
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 7
      text: '0.966'
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 382.052001953125
        r: 403.03875732421875
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 7
      text: '0.889'
    - bbox:
        b: 530.9953002929688
        coord_origin: BOTTOMLEFT
        l: 436.73199462890625
        r: 453.11181640625
        t: 539.0650024414062
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 7
      text: '3.26'
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 528.8377685546875
      coord_origin: BOTTOMLEFT
      l: 143.82015991210938
      r: 470.7203063964844
      t: 636.0093383789062
    charspan:
    - 0
    - 0
    page_no: 10
  references: []
  self_ref: '#/tables/1'
texts:
- children: []
  content_layer: body
  label: page_header
  orig: arXiv:2305.03393v1 [cs.CV] 5 May 2023
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 236.99996948242188
      coord_origin: BOTTOMLEFT
      l: 16.726131439208984
      r: 36.339786529541016
      t: 582.52001953125
    charspan:
    - 0
    - 37
    page_no: 1
  self_ref: '#/texts/0'
  text: arXiv:2305.03393v1 [cs.CV] 5 May 2023
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 645.4561157226562
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 480.59735107421875
      t: 677.035400390625
    charspan:
    - 0
    - 60
    page_no: 1
  self_ref: '#/texts/1'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: text
  orig: Maksym Lysak [0000-0002-3723-$^{6960]}$, Ahmed Nassar[0000-0002-9468-$^{0822]}$,
    Nikolaos Livathinos [0000-0001-8513-$^{3491]}$, Christoph Auer[0000-0001-5761-$^{0422]}$,
    and Peter Staar [0000-0002-8088-0823]
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 587.6192626953125
      coord_origin: BOTTOMLEFT
      l: 138.8135528564453
      r: 476.0126953125
      t: 622.9446411132812
    charspan:
    - 0
    - 208
    page_no: 1
  self_ref: '#/texts/2'
  text: Maksym Lysak [0000-0002-3723-$^{6960]}$, Ahmed Nassar[0000-0002-9468-$^{0822]}$,
    Nikolaos Livathinos [0000-0001-8513-$^{3491]}$, Christoph Auer[0000-0001-5761-$^{0422]}$,
    and Peter Staar [0000-0002-8088-0823]
- children: []
  content_layer: body
  label: text
  orig: IBM Research {mly,ahn,nli,cau,taa}@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 555.5532836914062
      coord_origin: BOTTOMLEFT
      l: 222.96609497070312
      r: 392.6954650878906
      t: 575.9751586914062
    charspan:
    - 0
    - 49
    page_no: 1
  self_ref: '#/texts/3'
  text: IBM Research {mly,ahn,nli,cau,taa}@zurich.ibm.com
- children: []
  content_layer: body
  label: text
  orig: Abstract. Extracting tables from documents is a crucial task in any document
    conversion pipeline. Recently, transformer-based models have demonstrated that
    table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence
    (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence
    of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since
    the token representation of the table structure has a significant impact on the
    accuracy and run-time performance of any Im2Seq model, we investigate in this
    paper how table-structure representation can be optimised. We propose a new, optimised
    table-structure language (OTSL) with a minimized vocabulary and specific rules.
    The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs
    28+) and shortens the sequence length to half of HTML on average. Consequently,
    model accuracy improves significantly, inference time is halved compared to HTML-based
    models, and the predicted table structures are always syntactically correct. This
    in turn eliminates most post-processing needs. Popular table structure data-sets
    will be published in OTSL format to the community.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 327.2655334472656
      coord_origin: BOTTOMLEFT
      l: 162.32940673828125
      r: 452.4580383300781
      t: 522.7750244140625
    charspan:
    - 0
    - 1198
    page_no: 1
  self_ref: '#/texts/4'
  text: Abstract. Extracting tables from documents is a crucial task in any document
    conversion pipeline. Recently, transformer-based models have demonstrated that
    table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence
    (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence
    of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since
    the token representation of the table structure has a significant impact on the
    accuracy and run-time performance of any Im2Seq model, we investigate in this
    paper how table-structure representation can be optimised. We propose a new, optimised
    table-structure language (OTSL) with a minimized vocabulary and specific rules.
    The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs
    28+) and shortens the sequence length to half of HTML on average. Consequently,
    model accuracy improves significantly, inference time is halved compared to HTML-based
    models, and the predicted table structures are always syntactically correct. This
    in turn eliminates most post-processing needs. Popular table structure data-sets
    will be published in OTSL format to the community.
- children: []
  content_layer: body
  label: text
  orig: "Keywords: Table Structure Recognition \xB7 Data Representation \xB7 Transformers\
    \ \xB7 Optimization."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 294.2145080566406
      coord_origin: BOTTOMLEFT
      l: 162.6809539794922
      r: 452.2415771484375
      t: 314.4485778808594
    charspan:
    - 0
    - 90
    page_no: 1
  self_ref: '#/texts/5'
  text: "Keywords: Table Structure Recognition \xB7 Data Representation \xB7 Transformers\
    \ \xB7 Optimization."
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 1 Introduction
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 259.3119201660156
      coord_origin: BOTTOMLEFT
      l: 134.76512145996094
      r: 228.933837890625
      t: 271.2332763671875
    charspan:
    - 0
    - 14
    page_no: 1
  self_ref: '#/texts/6'
  text: 1 Introduction
- children: []
  content_layer: body
  label: text
  orig: Tables are ubiquitous in documents such as scientific papers, patents, reports,
    manuals, specification sheets or marketing material. They often encode highly
    valuable information and therefore need to be extracted with high accuracy. Unfortunately,
    tables appear in documents in various sizes, styling and structure, making it
    difficult to recover their correct structure with simple analytical methods. Therefore,
    accurate table extraction is achieved these days with machine-learning based methods.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 163.18548583984375
      coord_origin: BOTTOMLEFT
      l: 134.23095703125
      r: 480.595947265625
      t: 244.9595947265625
    charspan:
    - 0
    - 500
    page_no: 1
  self_ref: '#/texts/7'
  text: Tables are ubiquitous in documents such as scientific papers, patents, reports,
    manuals, specification sheets or marketing material. They often encode highly
    valuable information and therefore need to be extracted with high accuracy. Unfortunately,
    tables appear in documents in various sizes, styling and structure, making it
    difficult to recover their correct structure with simple analytical methods. Therefore,
    accurate table extraction is achieved these days with machine-learning based methods.
- children: []
  content_layer: body
  label: text
  orig: In modern document understanding systems [1,15], table extraction is typically
    a two-step process. Firstly, every table on a page is located with a bounding
    box, and secondly, their logical row and column structure is recognized. As of
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.14546966552734
      coord_origin: BOTTOMLEFT
      l: 134.2904052734375
      r: 480.76031494140625
      t: 161.17213439941406
    charspan:
    - 0
    - 235
    page_no: 1
  self_ref: '#/texts/8'
  text: In modern document understanding systems [1,15], table extraction is typically
    a two-step process. Firstly, every table on a page is located with a bounding
    box, and secondly, their logical row and column structure is recognized. As of
- children: []
  content_layer: body
  label: page_header
  orig: '2'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.3629150390625
      r: 139.39122009277344
      t: 698.83056640625
    charspan:
    - 0
    - 1
    page_no: 2
  self_ref: '#/texts/9'
  text: '2'
- children: []
  content_layer: body
  label: page_header
  orig: M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 167.53773498535156
      r: 231.72227478027344
      t: 699.1365356445312
    charspan:
    - 0
    - 16
    page_no: 2
  self_ref: '#/texts/10'
  text: M. Lysak, et al.
- children: []
  content_layer: body
  label: text
  orig: 'Fig. 1. Comparison between HTML and OTSL table structure representation:
    (A) table-example with complex row and column headers, including a 2D empty span,
    (B) minimal graphical representation of table structure using rectangular layout,
    (C) HTML representation, (D) OTSL representation. This example demonstrates many
    of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in
    this case), its reduced sequence length (55 versus 30) and a enhanced internal
    structure (variable token sequence length per row in HTML versus a fixed length
    of rows in OTSL).'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 591.6729125976562
      coord_origin: BOTTOMLEFT
      l: 134.25772094726562
      r: 480.98919677734375
      t: 666.7674560546875
    charspan:
    - 0
    - 574
    page_no: 2
  self_ref: '#/texts/11'
  text: 'Fig. 1. Comparison between HTML and OTSL table structure representation:
    (A) table-example with complex row and column headers, including a 2D empty span,
    (B) minimal graphical representation of table structure using rectangular layout,
    (C) HTML representation, (D) OTSL representation. This example demonstrates many
    of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in
    this case), its reduced sequence length (55 versus 30) and a enhanced internal
    structure (variable token sequence length per row in HTML versus a fixed length
    of rows in OTSL).'
- children: []
  content_layer: body
  label: text
  orig: today, table detection in documents is a well understood problem, and the
    latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable
    to human observers [7,8,10,14,23]. On the other hand, the problem of table structure
    recognition (TSR) is a lot more challenging and remains a very active area of
    research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 270.687255859375
      coord_origin: BOTTOMLEFT
      l: 134.22119140625
      r: 480.5923156738281
      t: 340.94696044921875
    charspan:
    - 0
    - 435
    page_no: 2
  self_ref: '#/texts/12'
  text: today, table detection in documents is a well understood problem, and the
    latest state-of-the-art (SOTA) object detection methods provide an accuracy comparable
    to human observers [7,8,10,14,23]. On the other hand, the problem of table structure
    recognition (TSR) is a lot more challenging and remains a very active area of
    research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].
- children: []
  content_layer: body
  label: text
  orig: Recently emerging SOTA methods for table structure recognition employ transformer-based
    models, in which an image of the table is provided to the network in order to
    predict the structure of the table as a sequence of tokens. These image-to-sequence
    (Im2Seq) models are extremely powerful, since they allow for a purely data-driven
    solution. The tokens of the sequence typically belong to a markup language such
    as HTML, Latex or Markdown, which allow to describe table structure as rows, columns
    and spanning cells in various configurations. In Figure 1, we illustrate how HTML
    is used to represent the table-structure of a particular example table. Public
    table-structure data sets such as PubTab-Net [22], and FinTabNet [21], which were
    created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed
    Central), popularized primarily the use of HTML as ground-truth representation
    format for TSR.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.11603546142578
      coord_origin: BOTTOMLEFT
      l: 134.06126403808594
      r: 480.6329345703125
      t: 268.6296081542969
    charspan:
    - 0
    - 912
    page_no: 2
  self_ref: '#/texts/13'
  text: Recently emerging SOTA methods for table structure recognition employ transformer-based
    models, in which an image of the table is provided to the network in order to
    predict the structure of the table as a sequence of tokens. These image-to-sequence
    (Im2Seq) models are extremely powerful, since they allow for a purely data-driven
    solution. The tokens of the sequence typically belong to a markup language such
    as HTML, Latex or Markdown, which allow to describe table structure as rows, columns
    and spanning cells in various configurations. In Figure 1, we illustrate how HTML
    is used to represent the table-structure of a particular example table. Public
    table-structure data sets such as PubTab-Net [22], and FinTabNet [21], which were
    created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed
    Central), popularized primarily the use of HTML as ground-truth representation
    format for TSR.
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 689.908935546875
      coord_origin: BOTTOMLEFT
      l: 194.24490356445312
      r: 447.54290771484375
      t: 699.1016235351562
    charspan:
    - 0
    - 60
    page_no: 3
  self_ref: '#/texts/14'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '3'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 475.5311279296875
      r: 480.59124755859375
      t: 698.7382202148438
    charspan:
    - 0
    - 1
    page_no: 3
  self_ref: '#/texts/15'
  text: '3'
- children: []
  content_layer: body
  label: text
  orig: While the majority of research in TSR is currently focused on the development
    and application of novel neural model architectures, the table structure representation
    language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the
    sequence tokenization in Im2Seq models. In this paper, we aim for the opposite
    and investigate the impact of the table structure representation language with
    an otherwise unmodified Im2Seq transformer-based architecture. Since the current
    state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform
    our experiments.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 580.4091796875
      coord_origin: BOTTOMLEFT
      l: 134.1209716796875
      r: 480.8675842285156
      t: 673.9035034179688
    charspan:
    - 0
    - 584
    page_no: 3
  self_ref: '#/texts/16'
  text: While the majority of research in TSR is currently focused on the development
    and application of novel neural model architectures, the table structure representation
    language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the
    sequence tokenization in Im2Seq models. In this paper, we aim for the opposite
    and investigate the impact of the table structure representation language with
    an otherwise unmodified Im2Seq transformer-based architecture. Since the current
    state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform
    our experiments.
- children: []
  content_layer: body
  label: text
  orig: The main contribution of this paper is the introduction of a new optimised
    table structure language (OTSL), specifically designed to describe table-structure
    in an compact and structured way for Im2Seq models. OTSL has a number of key features,
    which make it very attractive to use in Im2Seq models. Specifically, compared
    to other languages such as HTML, OTSL has a minimized vocabulary which yields
    short sequence length, strong inherent structure (e.g. strict rectangular layout)
    and a strict syntax with rules that only look backwards. The latter allows for
    syntax validation during inference and ensures a syntactically correct table-structure.
    These OTSL features are illustrated in Figure 1, in comparison to HTML.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 460.7701416015625
      coord_origin: BOTTOMLEFT
      l: 134.0509490966797
      r: 480.9625549316406
      t: 578.3331909179688
    charspan:
    - 0
    - 721
    page_no: 3
  self_ref: '#/texts/17'
  text: The main contribution of this paper is the introduction of a new optimised
    table structure language (OTSL), specifically designed to describe table-structure
    in an compact and structured way for Im2Seq models. OTSL has a number of key features,
    which make it very attractive to use in Im2Seq models. Specifically, compared
    to other languages such as HTML, OTSL has a minimized vocabulary which yields
    short sequence length, strong inherent structure (e.g. strict rectangular layout)
    and a strict syntax with rules that only look backwards. The latter allows for
    syntax validation during inference and ensures a syntactically correct table-structure.
    These OTSL features are illustrated in Figure 1, in comparison to HTML.
- children: []
  content_layer: body
  label: text
  orig: The paper is structured as follows. In section 2, we give an overview of the
    latest developments in table-structure reconstruction. In section 3 we review
    the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss
    its flaws. Subsequently, we introduce OTSL in section 4, which includes the language
    definition, syntax rules and error-correction procedures. In section 5, we apply
    OTSL on the TableFormer architecture, compare it to Table-Former models trained
    on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section
    6 we conclude our work and outline next potential steps.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 352.9132385253906
      coord_origin: BOTTOMLEFT
      l: 133.9246368408203
      r: 480.6549377441406
      t: 458.8175048828125
    charspan:
    - 0
    - 627
    page_no: 3
  self_ref: '#/texts/18'
  text: The paper is structured as follows. In section 2, we give an overview of the
    latest developments in table-structure reconstruction. In section 3 we review
    the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss
    its flaws. Subsequently, we introduce OTSL in section 4, which includes the language
    definition, syntax rules and error-correction procedures. In section 5, we apply
    OTSL on the TableFormer architecture, compare it to Table-Former models trained
    on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section
    6 we conclude our work and outline next potential steps.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 2 Related Work
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 319.3436584472656
      coord_origin: BOTTOMLEFT
      l: 134.5620574951172
      r: 236.87063598632812
      t: 331.4511413574219
    charspan:
    - 0
    - 14
    page_no: 3
  self_ref: '#/texts/19'
  text: 2 Related Work
- children: []
  content_layer: body
  label: text
  orig: 'Approaches to formalize the logical structure and layout of tables in electronic
    documents date back more than two decades [16]. In the recent past, a wide variety
    of computer vision methods have been explored to tackle the problem of table structure
    recognition, i.e. the correct identification of columns, rows and spanning cells
    in a given table. Broadly speaking, the current deeplearning based approaches
    fall into three categories: object detection (OD) methods, Graph-Neural-Network
    (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection
    based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping)
    bounding boxes for training, and produce bounding-box predictions to define table
    cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods
    [3,6,17,18], as the name suggests, represent tables as graph structures. The graph
    nodes represent the content of each table cell, an embedding vector from the table
    image, or geometric coordinates of the table cell. The edges of the graph define
    the relationship between the nodes, e.g. if they belong to the same column, row,
    or table cell.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.04315185546875
      coord_origin: BOTTOMLEFT
      l: 133.90113830566406
      r: 484.1204833984375
      t: 304.8121032714844
    charspan:
    - 0
    - 1161
    page_no: 3
  self_ref: '#/texts/20'
  text: 'Approaches to formalize the logical structure and layout of tables in electronic
    documents date back more than two decades [16]. In the recent past, a wide variety
    of computer vision methods have been explored to tackle the problem of table structure
    recognition, i.e. the correct identification of columns, rows and spanning cells
    in a given table. Broadly speaking, the current deeplearning based approaches
    fall into three categories: object detection (OD) methods, Graph-Neural-Network
    (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection
    based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping)
    bounding boxes for training, and produce bounding-box predictions to define table
    cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods
    [3,6,17,18], as the name suggests, represent tables as graph structures. The graph
    nodes represent the content of each table cell, an embedding vector from the table
    image, or geometric coordinates of the table cell. The edges of the graph define
    the relationship between the nodes, e.g. if they belong to the same column, row,
    or table cell.'
- children: []
  content_layer: body
  label: page_header
  orig: 4 M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 231.72227478027344
      t: 699.1917724609375
    charspan:
    - 0
    - 18
    page_no: 4
  self_ref: '#/texts/21'
  text: 4 M. Lysak, et al.
- children: []
  content_layer: body
  label: text
  orig: Other work [20] aims at predicting a grid for each table and deciding which
    cells must be merged using an attention network. Im2Seq methods cast the problem
    as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure
    representation language, which is often implemented with standard markup languages
    (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage
    over the OD and GNN methods by virtue of directly predicting the table-structure.
    As such, no post-processing or rules are needed in order to obtain the table-structure,
    which is necessary with OD and GNN approaches. In practice, this is not entirely
    true, because a predicted sequence of table-structure markup does not necessarily
    have to be syntactically correct. Hence, depending on the quality of the predicted
    sequence, some post-processing needs to be performed to ensure a syntactically
    valid (let alone correct) sequence.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 532.7620849609375
      coord_origin: BOTTOMLEFT
      l: 134.082763671875
      r: 480.59576416015625
      t: 674.2169799804688
    charspan:
    - 0
    - 939
    page_no: 4
  self_ref: '#/texts/22'
  text: Other work [20] aims at predicting a grid for each table and deciding which
    cells must be merged using an attention network. Im2Seq methods cast the problem
    as a sequence generation task [4,5,9,22], and therefore need an internal tablestructure
    representation language, which is often implemented with standard markup languages
    (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage
    over the OD and GNN methods by virtue of directly predicting the table-structure.
    As such, no post-processing or rules are needed in order to obtain the table-structure,
    which is necessary with OD and GNN approaches. In practice, this is not entirely
    true, because a predicted sequence of table-structure markup does not necessarily
    have to be syntactically correct. Hence, depending on the quality of the predicted
    sequence, some post-processing needs to be performed to ensure a syntactically
    valid (let alone correct) sequence.
- children: []
  content_layer: body
  label: text
  orig: Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder
    model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses
    two consecutive long short-term memory (LSTM) decoders to predict a table in HTML
    representation. The tag decoder predicts a sequence of HTML tags. For each decoded
    table cell (<td>), the attention is passed to the cell decoder to predict the
    content with an embedded OCR approach. The latter makes it susceptible to transcription
    errors in the cell content of the table. TableFormer address this reliance on
    OCR and uses two transformer decoders for HTML structure and cell bounding box
    prediction in an end-to-end architecture. The predicted cell bounding box is then
    used to extract text tokens from an originating (digital) PDF page, circumventing
    any need for OCR. TabSplitter [2] proposes a compact double-matrix representation
    of table rows and columns to do error detection and error correction of HTML structure
    sequences based on predictions from [19]. This compact double-matrix representation
    can not be used directly by the Img2seq model training, so the model uses HTML
    as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method
    using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet
    transformers to predict LaTeX code, and a separate OCR module to extract content.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 305.3533020019531
      coord_origin: BOTTOMLEFT
      l: 133.92919921875
      r: 480.8441467285156
      t: 530.3263549804688
    charspan:
    - 0
    - 1402
    page_no: 4
  self_ref: '#/texts/23'
  text: Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder
    model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [19]. EDD uses
    two consecutive long short-term memory (LSTM) decoders to predict a table in HTML
    representation. The tag decoder predicts a sequence of HTML tags. For each decoded
    table cell (<td>), the attention is passed to the cell decoder to predict the
    content with an embedded OCR approach. The latter makes it susceptible to transcription
    errors in the cell content of the table. TableFormer address this reliance on
    OCR and uses two transformer decoders for HTML structure and cell bounding box
    prediction in an end-to-end architecture. The predicted cell bounding box is then
    used to extract text tokens from an originating (digital) PDF page, circumventing
    any need for OCR. TabSplitter [2] proposes a compact double-matrix representation
    of table rows and columns to do error detection and error correction of HTML structure
    sequences based on predictions from [19]. This compact double-matrix representation
    can not be used directly by the Img2seq model training, so the model uses HTML
    as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method
    using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet
    transformers to predict LaTeX code, and a separate OCR module to extract content.
- children: []
  content_layer: body
  label: text
  orig: Im2Seq approaches have shown to be well-suited for the TSR task and allow
    a full end-to-end network design that can output the final table structure without
    pre-or post-processing logic. Furthermore, Im2Seq models have demonstrated to
    deliver state-of-the-art prediction accuracy [9]. This motivated the authors to
    investigate if the performance (both in accuracy and inference time) can be further
    improved by optimising the table structure representation language. We believe
    this is a necessary step before further improving neural network architectures
    for this task.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 209.4513397216797
      coord_origin: BOTTOMLEFT
      l: 134.0952606201172
      r: 480.67919921875
      t: 303.43621826171875
    charspan:
    - 0
    - 571
    page_no: 4
  self_ref: '#/texts/24'
  text: Im2Seq approaches have shown to be well-suited for the TSR task and allow
    a full end-to-end network design that can output the final table structure without
    pre-or post-processing logic. Furthermore, Im2Seq models have demonstrated to
    deliver state-of-the-art prediction accuracy [9]. This motivated the authors to
    investigate if the performance (both in accuracy and inference time) can be further
    improved by optimising the table structure representation language. We believe
    this is a necessary step before further improving neural network architectures
    for this task.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3 Problem Statement
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 175.88177490234375
      coord_origin: BOTTOMLEFT
      l: 134.63693237304688
      r: 269.6244201660156
      t: 187.6386260986328
    charspan:
    - 0
    - 19
    page_no: 4
  self_ref: '#/texts/25'
  text: 3 Problem Statement
- children: []
  content_layer: body
  label: text
  orig: All known Im2Seq based models for TSR fundamentally work in similar ways.
    Given an image of a table, the Im2Seq model predicts the structure of the table
    by generating a sequence of tokens. These tokens originate from a finite vocab-
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.13204956054688
      coord_origin: BOTTOMLEFT
      l: 133.88845825195312
      r: 480.59368896484375
      t: 161.2521209716797
    charspan:
    - 0
    - 233
    page_no: 4
  self_ref: '#/texts/26'
  text: All known Im2Seq based models for TSR fundamentally work in similar ways.
    Given an image of a table, the Im2Seq model predicts the structure of the table
    by generating a sequence of tokens. These tokens originate from a finite vocab-
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.0726928710938
      coord_origin: BOTTOMLEFT
      l: 194.2025146484375
      r: 447.54290771484375
      t: 699.0702514648438
    charspan:
    - 0
    - 60
    page_no: 5
  self_ref: '#/texts/27'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '5'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 475.65594482421875
      r: 480.59124755859375
      t: 698.6350708007812
    charspan:
    - 0
    - 1
    page_no: 5
  self_ref: '#/texts/28'
  text: '5'
- children: []
  content_layer: body
  label: text
  orig: ulary and can be interpreted as a table structure. For example, with the HTML
    tokens <table>, </table>, <tr>, </tr>, <td> and </td>, one can construct simple
    table structures without any spanning cells. In reality though, one needs at least
    28 HTML tokens to describe the most common complex tables observed in real-world
    documents [21,22], due to a variety of spanning cells definitions in the HTML
    token vocabulary.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 604.4931640625
      coord_origin: BOTTOMLEFT
      l: 134.16241455078125
      r: 480.8180847167969
      t: 673.9935913085938
    charspan:
    - 0
    - 417
    page_no: 5
  self_ref: '#/texts/29'
  text: ulary and can be interpreted as a table structure. For example, with the HTML
    tokens <table>, </table>, <tr>, </tr>, <td> and </td>, one can construct simple
    table structures without any spanning cells. In reality though, one needs at least
    28 HTML tokens to describe the most common complex tables observed in real-world
    documents [21,22], due to a variety of spanning cells definitions in the HTML
    token vocabulary.
- children: []
  content_layer: body
  label: caption
  orig: Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 562.7882080078125
      coord_origin: BOTTOMLEFT
      l: 145.2456512451172
      r: 469.7522277832031
      t: 572.1326293945312
    charspan:
    - 0
    - 73
    page_no: 5
  self_ref: '#/texts/30'
  text: Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet.
- children: []
  content_layer: body
  label: text
  orig: Obviously, HTML and other general-purpose markup languages were not designed
    for Im2Seq models. As such, they have some serious drawbacks. First, the token
    vocabulary needs to be artificially large in order to describe all plausible tabular
    structures. Since most Im2Seq models use an autoregressive approach, they generate
    the sequence token by token. Therefore, to reduce inference time, a shorter sequence
    length is critical. Every table-cell is represented by at least two tokens (<td>
    and </td>). Furthermore, when tokenizing the HTML structure, one needs to explicitly
    enumerate possible column-spans and row-spans as words. In practice, this ends
    up requiring 28 different HTML tokens (when including column-and row-spans up
    to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not
    every token is equally represented, as is depicted in Figure 2. This skewed distribution
    of tokens in combination with variable token row-length makes it challenging for
    models to learn the HTML structure.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 259.57940673828125
      coord_origin: BOTTOMLEFT
      l: 133.90890502929688
      r: 480.7246398925781
      t: 425.2646789550781
    charspan:
    - 0
    - 1018
    page_no: 5
  self_ref: '#/texts/31'
  text: Obviously, HTML and other general-purpose markup languages were not designed
    for Im2Seq models. As such, they have some serious drawbacks. First, the token
    vocabulary needs to be artificially large in order to describe all plausible tabular
    structures. Since most Im2Seq models use an autoregressive approach, they generate
    the sequence token by token. Therefore, to reduce inference time, a shorter sequence
    length is critical. Every table-cell is represented by at least two tokens (<td>
    and </td>). Furthermore, when tokenizing the HTML structure, one needs to explicitly
    enumerate possible column-spans and row-spans as words. In practice, this ends
    up requiring 28 different HTML tokens (when including column-and row-spans up
    to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not
    every token is equally represented, as is depicted in Figure 2. This skewed distribution
    of tokens in combination with variable token row-length makes it challenging for
    models to learn the HTML structure.
- children: []
  content_layer: body
  label: text
  orig: Additionally, it would be desirable if the representation would easily allow
    an early detection of invalid sequences on-the-go, before the prediction of the
    entire table structure is completed. HTML is not well-suited for this purpose
    as the verification of incomplete sequences is non-trivial or even impossible.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 211.29440307617188
      coord_origin: BOTTOMLEFT
      l: 134.07424926757812
      r: 480.5928955078125
      t: 257.035400390625
    charspan:
    - 0
    - 313
    page_no: 5
  self_ref: '#/texts/32'
  text: Additionally, it would be desirable if the representation would easily allow
    an early detection of invalid sequences on-the-go, before the prediction of the
    entire table structure is completed. HTML is not well-suited for this purpose
    as the verification of incomplete sequences is non-trivial or even impossible.
- children: []
  content_layer: body
  label: text
  orig: In a valid HTML table, the token sequence must describe a 2D grid of table
    cells, serialised in row-major ordering, where each row and each column have the
    same length (while considering row-and column-spans). Furthermore, every opening
    tag in HTML needs to be matched by a closing tag in a correct hierarchical manner.
    Since the number of tokens for each table row and column can vary significantly,
    especially for large tables with many row-and column-spans, it is complex to verify
    the consistency of predicted structures during sequence
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.14539337158203
      coord_origin: BOTTOMLEFT
      l: 134.04122924804688
      r: 480.65728759765625
      t: 209.4727783203125
    charspan:
    - 0
    - 540
    page_no: 5
  self_ref: '#/texts/33'
  text: In a valid HTML table, the token sequence must describe a 2D grid of table
    cells, serialised in row-major ordering, where each row and each column have the
    same length (while considering row-and column-spans). Furthermore, every opening
    tag in HTML needs to be matched by a closing tag in a correct hierarchical manner.
    Since the number of tokens for each table row and column can vary significantly,
    especially for large tables with many row-and column-spans, it is complex to verify
    the consistency of predicted structures during sequence
- children: []
  content_layer: body
  label: page_header
  orig: '6'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.27459716796875
      r: 139.4002685546875
      t: 698.835693359375
    charspan:
    - 0
    - 1
    page_no: 6
  self_ref: '#/texts/34'
  text: '6'
- children: []
  content_layer: body
  label: page_header
  orig: M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 167.5254364013672
      r: 231.72227478027344
      t: 699.1442260742188
    charspan:
    - 0
    - 16
    page_no: 6
  self_ref: '#/texts/35'
  text: M. Lysak, et al.
- children: []
  content_layer: body
  label: text
  orig: generation. Implicitly, this also means that Im2Seq models need to learn these
    complex syntax rules, simply to deliver valid output.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 652.314208984375
      coord_origin: BOTTOMLEFT
      l: 134.2508544921875
      r: 480.68896484375
      t: 673.7930908203125
    charspan:
    - 0
    - 132
    page_no: 6
  self_ref: '#/texts/36'
  text: generation. Implicitly, this also means that Im2Seq models need to learn these
    complex syntax rules, simply to deliver valid output.
- children: []
  content_layer: body
  label: text
  orig: In practice, we observe two major issues with prediction quality when training
    Im2Seq models on HTML table structure generation from images. On the one hand,
    we find that on large tables, the visual attention of the model often starts to
    drift and is not accurately moving forward cell by cell anymore. This manifests
    itself in either in an increasing location drift for proposed table-cells in later
    rows on the same column or even complete loss of vertical alignment, as illustrated
    in Figure 5. Addressing this with post-processing is partially possible, but clearly
    undesired. On the other hand, we find many instances of predictions with structural
    inconsistencies or plain invalid HTML output, as shown in Figure 6, which are
    nearly impossible to properly correct. Both problems seriously impact the TSR
    model performance, since they reflect not only in the task of pure structure recognition
    but also in the equally crucial recognition or matching of table cell content.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 496.2580871582031
      coord_origin: BOTTOMLEFT
      l: 133.84449768066406
      r: 480.68035888671875
      t: 649.6573486328125
    charspan:
    - 0
    - 977
    page_no: 6
  self_ref: '#/texts/37'
  text: In practice, we observe two major issues with prediction quality when training
    Im2Seq models on HTML table structure generation from images. On the one hand,
    we find that on large tables, the visual attention of the model often starts to
    drift and is not accurately moving forward cell by cell anymore. This manifests
    itself in either in an increasing location drift for proposed table-cells in later
    rows on the same column or even complete loss of vertical alignment, as illustrated
    in Figure 5. Addressing this with post-processing is partially possible, but clearly
    undesired. On the other hand, we find many instances of predictions with structural
    inconsistencies or plain invalid HTML output, as shown in Figure 6, which are
    nearly impossible to properly correct. Both problems seriously impact the TSR
    model performance, since they reflect not only in the task of pure structure recognition
    but also in the equally crucial recognition or matching of table cell content.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 4 Optimised Table Structure Language
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 460.670166015625
      coord_origin: BOTTOMLEFT
      l: 134.27130126953125
      r: 372.50848388671875
      t: 472.23907470703125
    charspan:
    - 0
    - 36
    page_no: 6
  self_ref: '#/texts/38'
  text: 4 Optimised Table Structure Language
- children: []
  content_layer: body
  label: text
  orig: To mitigate the issues with HTML in Im2Seq-based TSR models laid out before,
    we propose here our Optimised Table Structure Language (OTSL). OTSL is designed
    to express table structure with a minimized vocabulary and a simple set of rules,
    which are both significantly reduced compared to HTML. At the same time, OTSL
    enables easy error detection and correction during sequence generation. We further
    demonstrate how the compact structure representation and minimized sequence length
    improves prediction accuracy and inference time in the TableFormer architecture.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 350.400146484375
      coord_origin: BOTTOMLEFT
      l: 134.02992248535156
      r: 480.6401062011719
      t: 444.1526794433594
    charspan:
    - 0
    - 563
    page_no: 6
  self_ref: '#/texts/39'
  text: To mitigate the issues with HTML in Im2Seq-based TSR models laid out before,
    we propose here our Optimised Table Structure Language (OTSL). OTSL is designed
    to express table structure with a minimized vocabulary and a simple set of rules,
    which are both significantly reduced compared to HTML. At the same time, OTSL
    enables easy error detection and correction during sequence generation. We further
    demonstrate how the compact structure representation and minimized sequence length
    improves prediction accuracy and inference time in the TableFormer architecture.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 4.1 Language Definition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 317.3211364746094
      coord_origin: BOTTOMLEFT
      l: 134.2662811279297
      r: 261.80108642578125
      t: 327.5660705566406
    charspan:
    - 0
    - 23
    page_no: 6
  self_ref: '#/texts/40'
  text: 4.1 Language Definition
- children: []
  content_layer: body
  label: text
  orig: In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines
    only 5 tokens that directly describe a tabular structure based on an atomic 2D
    grid.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 270.2941589355469
      coord_origin: BOTTOMLEFT
      l: 134.26510620117188
      r: 480.5887145996094
      t: 304.3414306640625
    charspan:
    - 0
    - 165
    page_no: 6
  self_ref: '#/texts/41'
  text: In Figure 3, we illustrate how the OTSL is defined. In essence, the OTSL defines
    only 5 tokens that directly describe a tabular structure based on an atomic 2D
    grid.
- children: []
  content_layer: body
  label: text
  orig: 'The OTSL vocabulary is comprised of the following tokens:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 257.701171875
      coord_origin: BOTTOMLEFT
      l: 149.708984375
      r: 409.3113708496094
      t: 267.5809631347656
    charspan:
    - 0
    - 57
    page_no: 6
  self_ref: '#/texts/42'
  text: 'The OTSL vocabulary is comprised of the following tokens:'
- children: []
  content_layer: body
  label: text
  orig: -'C ' cell a new table cell that either has or does not have cell content
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 235.22317504882812
      coord_origin: BOTTOMLEFT
      l: 140.21255493164062
      r: 460.54443359375
      t: 245.7514190673828
    charspan:
    - 0
    - 73
    page_no: 6
  self_ref: '#/texts/43'
  text: -'C ' cell a new table cell that either has or does not have cell content
- children: []
  content_layer: body
  label: text
  orig: -'L ' cell left-looking cell, merging with the left neighbor cell to create
    a span
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 210.5531768798828
      coord_origin: BOTTOMLEFT
      l: 140.17581176757812
      r: 480.59393310546875
      t: 233.2664794921875
    charspan:
    - 0
    - 82
    page_no: 6
  self_ref: '#/texts/44'
  text: -'L ' cell left-looking cell, merging with the left neighbor cell to create
    a span
- children: []
  content_layer: body
  label: text
  orig: -'U ' cell up-looking cell, merging with the upper neighbor cell to create
    a span
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 185.62986755371094
      coord_origin: BOTTOMLEFT
      l: 140.07015991210938
      r: 480.58856201171875
      t: 208.34060668945312
    charspan:
    - 0
    - 81
    page_no: 6
  self_ref: '#/texts/45'
  text: -'U ' cell up-looking cell, merging with the upper neighbor cell to create
    a span
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: -'X ' cell cross cell, to merge with both left and upper neighbor cells
  parent:
    $ref: '#/groups/0'
  prov:
  - bbox:
      b: 173.49615478515625
      coord_origin: BOTTOMLEFT
      l: 140.18663024902344
      r: 454.61480712890625
      t: 184.0304718017578
    charspan:
    - 0
    - 71
    page_no: 6
  self_ref: '#/texts/46'
  text: -'X ' cell cross cell, to merge with both left and upper neighbor cells
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: -'NL ' new-line, switch to the next row.
  parent:
    $ref: '#/groups/0'
  prov:
  - bbox:
      b: 160.93917846679688
      coord_origin: BOTTOMLEFT
      l: 140.0900421142578
      r: 328.61676025390625
      t: 171.588623046875
    charspan:
    - 0
    - 40
    page_no: 6
  self_ref: '#/texts/47'
  text: -'NL ' new-line, switch to the next row.
- children: []
  content_layer: body
  label: text
  orig: A notable attribute of OTSL is that it has the capability of achieving lossless
    conversion to HTML.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.14515686035156
      coord_origin: BOTTOMLEFT
      l: 134.5558624267578
      r: 480.5928039550781
      t: 148.9019775390625
    charspan:
    - 0
    - 99
    page_no: 6
  self_ref: '#/texts/48'
  text: A notable attribute of OTSL is that it has the capability of achieving lossless
    conversion to HTML.
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.0286865234375
      coord_origin: BOTTOMLEFT
      l: 194.18203735351562
      r: 447.54290771484375
      t: 699.0518188476562
    charspan:
    - 0
    - 60
    page_no: 7
  self_ref: '#/texts/49'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '7'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 475.8619689941406
      r: 480.59124755859375
      t: 698.810546875
    charspan:
    - 0
    - 1
    page_no: 7
  self_ref: '#/texts/50'
  text: '7'
- children: []
  content_layer: body
  label: caption
  orig: 'Fig. 3. OTSL description of table structure: A-table example; B-graphical
    representation of table structure; C-mapping structure on a grid; D-OTSL structure
    encoding; E-explanation on cell encoding'
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 636.1503295898438
      coord_origin: BOTTOMLEFT
      l: 134.21524047851562
      r: 480.58740234375
      t: 667.3877563476562
    charspan:
    - 0
    - 197
    page_no: 7
  self_ref: '#/texts/51'
  text: 'Fig. 3. OTSL description of table structure: A-table example; B-graphical
    representation of table structure; C-mapping structure on a grid; D-OTSL structure
    encoding; E-explanation on cell encoding'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 4.2 Language Syntax
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 477.8972473144531
      coord_origin: BOTTOMLEFT
      l: 134.43145751953125
      r: 246.7050323486328
      t: 487.5932312011719
    charspan:
    - 0
    - 19
    page_no: 7
  self_ref: '#/texts/52'
  text: 4.2 Language Syntax
- children: []
  content_layer: body
  label: text
  orig: 'The OTSL representation follows these syntax rules:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 457.95526123046875
      coord_origin: BOTTOMLEFT
      l: 134.3270263671875
      r: 363.7961730957031
      t: 467.96502685546875
    charspan:
    - 0
    - 51
    page_no: 7
  self_ref: '#/texts/53'
  text: 'The OTSL representation follows these syntax rules:'
- children: []
  content_layer: body
  label: text
  orig: '1. Left-looking cell rule : The left neighbour of an ''L '' cell must be
    either another ''L '' cell or a ''C '' cell.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 424.0662536621094
      coord_origin: BOTTOMLEFT
      l: 138.97299194335938
      r: 480.5890197753906
      t: 446.0029602050781
    charspan:
    - 0
    - 111
    page_no: 7
  self_ref: '#/texts/54'
  text: '1. Left-looking cell rule : The left neighbour of an ''L '' cell must be
    either another ''L '' cell or a ''C '' cell.'
- children: []
  content_layer: body
  label: text
  orig: '2. Up-looking cell rule : The upper neighbour of a ''U '' cell must be either
    another ''U '' cell or a ''C '' cell.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 400.15325927734375
      coord_origin: BOTTOMLEFT
      l: 138.55227661132812
      r: 480.59228515625
      t: 422.29931640625
    charspan:
    - 0
    - 109
    page_no: 7
  self_ref: '#/texts/55'
  text: '2. Up-looking cell rule : The upper neighbour of a ''U '' cell must be either
    another ''U '' cell or a ''C '' cell.'
- children: []
  content_layer: body
  label: text
  orig: '3. Cross cell rule : The left neighbour of an ''X '' cell must be either
    another ''X '' cell or a ''U '' cell, and the upper neighbour of an ''X '' cell
    must be either another ''X '' cell or an ''L '' cell.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 352.3262939453125
      coord_origin: BOTTOMLEFT
      l: 138.67137145996094
      r: 480.5968017578125
      t: 397.8113708496094
    charspan:
    - 0
    - 194
    page_no: 7
  self_ref: '#/texts/56'
  text: '3. Cross cell rule : The left neighbour of an ''X '' cell must be either
    another ''X '' cell or a ''U '' cell, and the upper neighbour of an ''X '' cell
    must be either another ''X '' cell or an ''L '' cell.'
- children: []
  content_layer: body
  label: text
  orig: '4. First row rule : Only ''L '' cells and ''C '' cells are allowed in the
    first row.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 340.3673095703125
      coord_origin: BOTTOMLEFT
      l: 138.74691772460938
      r: 474.5901794433594
      t: 350.66766357421875
    charspan:
    - 0
    - 80
    page_no: 7
  self_ref: '#/texts/57'
  text: '4. First row rule : Only ''L '' cells and ''C '' cells are allowed in the
    first row.'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '5. First column rule : Only ''U '' cells and ''C '' cells are allowed in
    the first column.'
  parent:
    $ref: '#/groups/1'
  prov:
  - bbox:
      b: 316.4543151855469
      coord_origin: BOTTOMLEFT
      l: 138.59597778320312
      r: 480.58746337890625
      t: 338.4661560058594
    charspan:
    - 0
    - 86
    page_no: 7
  self_ref: '#/texts/58'
  text: '5. First column rule : Only ''U '' cells and ''C '' cells are allowed in
    the first column.'
- children: []
  content_layer: body
  label: text
  orig: '6. Rectangular rule : The table representation is always rectangular-all
    rows must have an equal number of tokens, terminated with ''NL '' token.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 292.5403137207031
      coord_origin: BOTTOMLEFT
      l: 138.39498901367188
      r: 480.6828918457031
      t: 314.6161193847656
    charspan:
    - 0
    - 143
    page_no: 7
  self_ref: '#/texts/59'
  text: '6. Rectangular rule : The table representation is always rectangular-all
    rows must have an equal number of tokens, terminated with ''NL '' token.'
- children: []
  content_layer: body
  label: text
  orig: 'The application of these rules gives OTSL a set of unique properties. First
    of all, the OTSL enforces a strictly rectangular structure representation, where
    every new-line token starts a new row. As a consequence, all rows and all columns
    have exactly the same number of tokens, irrespective of cell spans. Secondly,
    the OTSL representation is unambiguous: Every table structure is represented in
    one way. In this representation every table cell corresponds to a ''C ''-cell
    token, which in case of spans is always located in the top-left corner of the
    table cell definition. Third, OTSL syntax rules are only backward-looking. As
    a consequence, every predicted token can be validated straight during sequence
    generation by looking at the previously predicted sequence. As such, OTSL can
    guarantee that every predicted sequence is syntactically valid.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 150.70535278320312
      coord_origin: BOTTOMLEFT
      l: 133.97727966308594
      r: 480.71917724609375
      t: 280.6726989746094
    charspan:
    - 0
    - 849
    page_no: 7
  self_ref: '#/texts/60'
  text: 'The application of these rules gives OTSL a set of unique properties. First
    of all, the OTSL enforces a strictly rectangular structure representation, where
    every new-line token starts a new row. As a consequence, all rows and all columns
    have exactly the same number of tokens, irrespective of cell spans. Secondly,
    the OTSL representation is unambiguous: Every table structure is represented in
    one way. In this representation every table cell corresponds to a ''C ''-cell
    token, which in case of spans is always located in the top-left corner of the
    table cell definition. Third, OTSL syntax rules are only backward-looking. As
    a consequence, every predicted token can be validated straight during sequence
    generation by looking at the previously predicted sequence. As such, OTSL can
    guarantee that every predicted sequence is syntactically valid.'
- children: []
  content_layer: body
  label: text
  orig: These characteristics can be easily learned by sequence generator networks,
    as we demonstrate further below. We find strong indications that this pattern
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.14533233642578
      coord_origin: BOTTOMLEFT
      l: 134.1024627685547
      r: 480.5926513671875
      t: 149.01119995117188
    charspan:
    - 0
    - 153
    page_no: 7
  self_ref: '#/texts/61'
  text: These characteristics can be easily learned by sequence generator networks,
    as we demonstrate further below. We find strong indications that this pattern
- children: []
  content_layer: body
  label: page_header
  orig: '8'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.44200134277344
      r: 139.41741943359375
      t: 698.7767944335938
    charspan:
    - 0
    - 1
    page_no: 8
  self_ref: '#/texts/62'
  text: '8'
- children: []
  content_layer: body
  label: page_header
  orig: M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 167.67686462402344
      r: 231.72227478027344
      t: 699.2385864257812
    charspan:
    - 0
    - 16
    page_no: 8
  self_ref: '#/texts/63'
  text: M. Lysak, et al.
- children: []
  content_layer: body
  label: text
  orig: reduces significantly the column drift seen in the HTML based models (see
    Figure 5).
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 651.2139892578125
      coord_origin: BOTTOMLEFT
      l: 134.43443298339844
      r: 480.5888366699219
      t: 674.0389404296875
    charspan:
    - 0
    - 84
    page_no: 8
  self_ref: '#/texts/64'
  text: reduces significantly the column drift seen in the HTML based models (see
    Figure 5).
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 4.3 Error-detection and-mitigation
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 621.5159301757812
      coord_origin: BOTTOMLEFT
      l: 134.21498107910156
      r: 319.3470764160156
      t: 631.508544921875
    charspan:
    - 0
    - 34
    page_no: 8
  self_ref: '#/texts/65'
  text: 4.3 Error-detection and-mitigation
- children: []
  content_layer: body
  label: text
  orig: The design of OTSL allows to validate a table structure easily on an unfinished
    sequence. The detection of an invalid sequence token is a clear indication of
    a prediction mistake, however a valid sequence by itself does not guarantee prediction
    correctness. Different heuristics can be used to correct token errors in an invalid
    sequence and thus increase the chances for accurate predictions. Such heuristics
    can be applied either after the prediction of each token, or at the end on the
    entire predicted sequence. For example a simple heuristic which can correct the
    predicted OTSL sequence on-the-fly is to verify if the token with the highest
    prediction confidence invalidates the predicted sequence, and replace it by the
    token with the next highest confidence until OTSL rules are satisfied.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 493.32415771484375
      coord_origin: BOTTOMLEFT
      l: 134.159912109375
      r: 480.59576416015625
      t: 610.6505737304688
    charspan:
    - 0
    - 797
    page_no: 8
  self_ref: '#/texts/66'
  text: The design of OTSL allows to validate a table structure easily on an unfinished
    sequence. The detection of an invalid sequence token is a clear indication of
    a prediction mistake, however a valid sequence by itself does not guarantee prediction
    correctness. Different heuristics can be used to correct token errors in an invalid
    sequence and thus increase the chances for accurate predictions. Such heuristics
    can be applied either after the prediction of each token, or at the end on the
    entire predicted sequence. For example a simple heuristic which can correct the
    predicted OTSL sequence on-the-fly is to verify if the token with the highest
    prediction confidence invalidates the predicted sequence, and replace it by the
    token with the next highest confidence until OTSL rules are satisfied.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5 Experiments
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 460.2676086425781
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 229.03533935546875
      t: 471.6722717285156
    charspan:
    - 0
    - 13
    page_no: 8
  self_ref: '#/texts/67'
  text: 5 Experiments
- children: []
  content_layer: body
  label: text
  orig: 'To evaluate the impact of OTSL on prediction accuracy and inference times,
    we conducted a series of experiments based on the TableFormer model (Figure 4)
    with two objectives: Firstly we evaluate the prediction quality and performance
    of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical
    PubTabNet data set. Secondly we pick the best hyper-parameters found in the first
    step and evaluate how OTSL impacts the performance of TableFormer after training
    on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground
    truth (GT) from all data sets has been converted into OTSL format for this purpose,
    and will be made publicly available.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 340.3122863769531
      coord_origin: BOTTOMLEFT
      l: 133.95254516601562
      r: 480.764892578125
      t: 446.1274719238281
    charspan:
    - 0
    - 684
    page_no: 8
  self_ref: '#/texts/68'
  text: 'To evaluate the impact of OTSL on prediction accuracy and inference times,
    we conducted a series of experiments based on the TableFormer model (Figure 4)
    with two objectives: Firstly we evaluate the prediction quality and performance
    of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical
    PubTabNet data set. Secondly we pick the best hyper-parameters found in the first
    step and evaluate how OTSL impacts the performance of TableFormer after training
    on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground
    truth (GT) from all data sets has been converted into OTSL format for this purpose,
    and will be made publicly available.'
- children: []
  content_layer: body
  label: caption
  orig: Fig. 4. Architecture sketch of the TableFormer model, which is a representative
    for the Im2Seq approach.
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 288.2603454589844
      coord_origin: BOTTOMLEFT
      l: 134.516845703125
      r: 480.5908203125
      t: 308.65484619140625
    charspan:
    - 0
    - 104
    page_no: 8
  self_ref: '#/texts/69'
  text: Fig. 4. Architecture sketch of the TableFormer model, which is a representative
    for the Im2Seq approach.
- children: []
  content_layer: body
  label: text
  orig: We rely on standard metrics such as Tree Edit Distance score (TEDs) for table
    structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection
    Over Union (IOU) threshold for the bounding-box predictions of table cells. The
    predicted OTSL structures were converted back to HTML format in
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.1452407836914
      coord_origin: BOTTOMLEFT
      l: 134.209228515625
      r: 480.59173583984375
      t: 173.22616577148438
    charspan:
    - 0
    - 299
    page_no: 8
  self_ref: '#/texts/70'
  text: We rely on standard metrics such as Tree Edit Distance score (TEDs) for table
    structure prediction, and Mean Average Precision (mAP) with 0.75 Intersection
    Over Union (IOU) threshold for the bounding-box predictions of table cells. The
    predicted OTSL structures were converted back to HTML format in
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.0243530273438
      coord_origin: BOTTOMLEFT
      l: 194.1836395263672
      r: 447.54290771484375
      t: 699.0405883789062
    charspan:
    - 0
    - 60
    page_no: 9
  self_ref: '#/texts/71'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '9'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 475.3871765136719
      r: 480.59368896484375
      t: 698.889404296875
    charspan:
    - 0
    - 1
    page_no: 9
  self_ref: '#/texts/72'
  text: '9'
- children: []
  content_layer: body
  label: text
  orig: order to compute the TED score. Inference timing results for all experiments
    were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45
    GHz.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 640.3582153320312
      coord_origin: BOTTOMLEFT
      l: 134.22311401367188
      r: 480.6358337402344
      t: 673.7454223632812
    charspan:
    - 0
    - 163
    page_no: 9
  self_ref: '#/texts/73'
  text: order to compute the TED score. Inference timing results for all experiments
    were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45
    GHz.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.1 Hyper Parameter Optimization
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 614.0072021484375
      coord_origin: BOTTOMLEFT
      l: 134.44198608398438
      r: 318.44842529296875
      t: 623.8135375976562
    charspan:
    - 0
    - 32
    page_no: 9
  self_ref: '#/texts/74'
  text: 5.1 Hyper Parameter Optimization
- children: []
  content_layer: body
  label: text
  orig: We have chosen the PubTabNet data set to perform HPO, since it includes a
    highly diverse set of tables. Also we report TED scores separately for simple
    and complex tables (tables with cell spans). Results are presented in Table. 1.
    It is evident that with OTSL, our model achieves the same TED score and slightly
    better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in
    the inference runtime over HTML.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 537.8411254882812
      coord_origin: BOTTOMLEFT
      l: 134.1274871826172
      r: 480.97845458984375
      t: 607.5176391601562
    charspan:
    - 0
    - 423
    page_no: 9
  self_ref: '#/texts/75'
  text: We have chosen the PubTabNet data set to perform HPO, since it includes a
    highly diverse set of tables. Also we report TED scores separately for simple
    and complex tables (tables with cell spans). Results are presented in Table. 1.
    It is evident that with OTSL, our model achieves the same TED score and slightly
    better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in
    the inference runtime over HTML.
- children: []
  content_layer: body
  label: caption
  orig: 'Table 1. HPO performed in OTSL and HTML representation on the same transformer-based
    TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing
    the # of layers in encoder and decoder stages of the model show that smaller models
    trained on OTSL perform better, especially in recognizing complex table structures,
    and maintain a much higher mAP score than the HTML counterpart.'
  parent:
    $ref: '#/tables/0'
  prov:
  - bbox:
      b: 464.9591979980469
      coord_origin: BOTTOMLEFT
      l: 134.23480224609375
      r: 480.59539794921875
      t: 518.3223266601562
    charspan:
    - 0
    - 398
    page_no: 9
  self_ref: '#/texts/76'
  text: 'Table 1. HPO performed in OTSL and HTML representation on the same transformer-based
    TableFormer [9] architecture, trained only on PubTabNet [22]. Effects of reducing
    the # of layers in encoder and decoder stages of the model show that smaller models
    trained on OTSL perform better, especially in recognizing complex table structures,
    and maintain a much higher mAP score than the HTML counterpart.'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.2 Quantitative Results
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 275.041259765625
      coord_origin: BOTTOMLEFT
      l: 134.5574493408203
      r: 264.4033203125
      t: 285.26324462890625
    charspan:
    - 0
    - 24
    page_no: 9
  self_ref: '#/texts/77'
  text: 5.2 Quantitative Results
- children: []
  content_layer: body
  label: text
  orig: 'We picked the model parameter configuration that produced the best prediction
    quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained
    and evaluated it on three publicly available data sets: PubTabNet (395k samples),
    FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results
    are presented in Table. 2. It is clearly evident that the model trained on OTSL
    outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult
    financial tables (FinTabNet) that contain sparse and large tables.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 174.9652557373047
      coord_origin: BOTTOMLEFT
      l: 134.18052673339844
      r: 480.59576416015625
      t: 268.6277160644531
    charspan:
    - 0
    - 555
    page_no: 9
  self_ref: '#/texts/78'
  text: 'We picked the model parameter configuration that produced the best prediction
    quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained
    and evaluated it on three publicly available data sets: PubTabNet (395k samples),
    FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results
    are presented in Table. 2. It is clearly evident that the model trained on OTSL
    outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult
    financial tables (FinTabNet) that contain sparse and large tables.'
- children: []
  content_layer: body
  label: text
  orig: Additionally, the results show that OTSL has an advantage over HTML when applied
    on a bigger data set like PubTables-1M and achieves significantly improved scores.
    Finally, OTSL achieves faster inference due to fewer decoding steps which is a
    result of the reduced sequence representation.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.1452407836914
      coord_origin: BOTTOMLEFT
      l: 134.0897216796875
      r: 480.86676025390625
      t: 173.40298461914062
    charspan:
    - 0
    - 289
    page_no: 9
  self_ref: '#/texts/79'
  text: Additionally, the results show that OTSL has an advantage over HTML when applied
    on a bigger data set like PubTables-1M and achieves significantly improved scores.
    Finally, OTSL achieves faster inference due to fewer decoding steps which is a
    result of the reduced sequence representation.
- children: []
  content_layer: body
  label: page_header
  orig: '10'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 143.97886657714844
      t: 698.5452880859375
    charspan:
    - 0
    - 2
    page_no: 10
  self_ref: '#/texts/80'
  text: '10'
- children: []
  content_layer: body
  label: page_header
  orig: M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 167.46546936035156
      r: 231.72048950195312
      t: 699.1473388671875
    charspan:
    - 0
    - 16
    page_no: 10
  self_ref: '#/texts/81'
  text: M. Lysak, et al.
- children: []
  content_layer: body
  label: caption
  orig: Table 2. TSR and cell detection results compared between OTSL and HTML on
    the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using Table-Former
    [9] (with enc=6, dec=6, heads=8).
  parent:
    $ref: '#/tables/1'
  prov:
  - bbox:
      b: 646.1133422851562
      coord_origin: BOTTOMLEFT
      l: 134.36965942382812
      r: 480.59356689453125
      t: 677.1688842773438
    charspan:
    - 0
    - 193
    page_no: 10
  self_ref: '#/texts/82'
  text: Table 2. TSR and cell detection results compared between OTSL and HTML on
    the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using Table-Former
    [9] (with enc=6, dec=6, heads=8).
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.3 Qualitative Results
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 494.247802734375
      coord_origin: BOTTOMLEFT
      l: 134.45773315429688
      r: 257.0867919921875
      t: 504.040283203125
    charspan:
    - 0
    - 23
    page_no: 10
  self_ref: '#/texts/83'
  text: 5.3 Qualitative Results
- children: []
  content_layer: body
  label: text
  orig: To illustrate the qualitative differences between OTSL and HTML, Figure 5
    demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure
    6, OTSL proves to be more effective in handling tables with longer token sequences,
    resulting in even more precise structure prediction and bounding boxes.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 425.5223083496094
      coord_origin: BOTTOMLEFT
      l: 134.13148498535156
      r: 481.6264343261719
      t: 483.7383728027344
    charspan:
    - 0
    - 309
    page_no: 10
  self_ref: '#/texts/84'
  text: To illustrate the qualitative differences between OTSL and HTML, Figure 5
    demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure
    6, OTSL proves to be more effective in handling tables with longer token sequences,
    resulting in even more precise structure prediction and bounding boxes.
- children: []
  content_layer: body
  label: caption
  orig: "Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap\
    \ (E) than the HTML model (D), when predicting the structure of a sparse table\
    \ (A), at twice the inference speed because of shorter sequence length (B),(C).\
    \ 'PMC2807444_006_00.png ' PubTabNet. \u03BC"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 352.2828369140625
      coord_origin: BOTTOMLEFT
      l: 134.40878295898438
      r: 480.591064453125
      t: 395.5643615722656
    charspan:
    - 0
    - 271
    page_no: 10
  self_ref: '#/texts/85'
  text: "Fig. 5. The OTSL model produces more accurate bounding boxes with less overlap\
    \ (E) than the HTML model (D), when predicting the structure of a sparse table\
    \ (A), at twice the inference speed because of shorter sequence length (B),(C).\
    \ 'PMC2807444_006_00.png ' PubTabNet. \u03BC"
- children: []
  content_layer: body
  label: text
  orig: "\u03BC"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 116.65360260009766
      coord_origin: BOTTOMLEFT
      l: 227.91465759277344
      r: 230.10028076171875
      t: 126.1739730834961
    charspan:
    - 0
    - 1
    page_no: 10
  self_ref: '#/texts/86'
  text: "\u03BC"
- children: []
  content_layer: body
  label: text
  orig: "\u2265"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 98.57134246826172
      coord_origin: BOTTOMLEFT
      l: 300.58056640625
      r: 302.72637939453125
      t: 108.3780517578125
    charspan:
    - 0
    - 1
    page_no: 10
  self_ref: '#/texts/87'
  text: "\u2265"
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.0098876953125
      coord_origin: BOTTOMLEFT
      l: 194.35484313964844
      r: 447.54290771484375
      t: 699.0291137695312
    charspan:
    - 0
    - 60
    page_no: 11
  self_ref: '#/texts/88'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '11'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 471.3756103515625
      r: 480.5894775390625
      t: 698.588623046875
    charspan:
    - 0
    - 2
    page_no: 11
  self_ref: '#/texts/89'
  text: '11'
- children: []
  content_layer: body
  label: caption
  orig: Fig. 6. Visualization of predicted structure and detected bounding boxes on
    a complex table with many rows. The OTSL model (B) captured repeating pattern
    of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML
    model also didn't complete the HTML sequence correctly and displayed a lot more
    of drift and overlap of bounding boxes. 'PMC5406406_003_01.png ' PubTabNet.
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 614.11962890625
      coord_origin: BOTTOMLEFT
      l: 134.38262939453125
      r: 480.9215393066406
      t: 667.3763427734375
    charspan:
    - 0
    - 391
    page_no: 11
  self_ref: '#/texts/90'
  text: Fig. 6. Visualization of predicted structure and detected bounding boxes on
    a complex table with many rows. The OTSL model (B) captured repeating pattern
    of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML
    model also didn't complete the HTML sequence correctly and displayed a lot more
    of drift and overlap of bounding boxes. 'PMC5406406_003_01.png ' PubTabNet.
- children: []
  content_layer: body
  label: page_header
  orig: 12 M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 231.72048950195312
      t: 699.1820678710938
    charspan:
    - 0
    - 19
    page_no: 12
  self_ref: '#/texts/91'
  text: 12 M. Lysak, et al.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 6 Conclusion
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 663.8826293945312
      coord_origin: BOTTOMLEFT
      l: 134.5717010498047
      r: 219.25479125976562
      t: 675.3252563476562
    charspan:
    - 0
    - 12
    page_no: 12
  self_ref: '#/texts/92'
  text: 6 Conclusion
- children: []
  content_layer: body
  label: text
  orig: We demonstrated that representing tables in HTML for the task of table structure
    recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore,
    we presented in this paper an Optimized Table Structure Language (OTSL) which,
    when compared to commonly used general purpose languages, has several key benefits.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 588.5181884765625
      coord_origin: BOTTOMLEFT
      l: 134.25390625
      r: 480.595703125
      t: 646.2549438476562
    charspan:
    - 0
    - 330
    page_no: 12
  self_ref: '#/texts/93'
  text: We demonstrated that representing tables in HTML for the task of table structure
    recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore,
    we presented in this paper an Optimized Table Structure Language (OTSL) which,
    when compared to commonly used general purpose languages, has several key benefits.
- children: []
  content_layer: body
  label: text
  orig: First and foremost, given the same network configuration, inference time for
    a table-structure prediction is about 2 times faster compared to the conventional
    HTML approach. This is primarily owed to the shorter sequence length of the OTSL
    representation. Additional performance benefits can be obtained with HPO (hyper
    parameter optimization). As we demonstrate in our experiments, models trained
    on OTSL can be significantly smaller, e.g. by reducing the number of encoder and
    decoder layers, while preserving comparatively good prediction quality. This can
    further improve inference performance, yielding 5-6 times faster inference speed
    in OTSL with prediction quality comparable to models trained on HTML (see Table
    1).
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 467.2823486328125
      coord_origin: BOTTOMLEFT
      l: 133.88514709472656
      r: 480.7315673828125
      t: 585.533447265625
    charspan:
    - 0
    - 724
    page_no: 12
  self_ref: '#/texts/94'
  text: First and foremost, given the same network configuration, inference time for
    a table-structure prediction is about 2 times faster compared to the conventional
    HTML approach. This is primarily owed to the shorter sequence length of the OTSL
    representation. Additional performance benefits can be obtained with HPO (hyper
    parameter optimization). As we demonstrate in our experiments, models trained
    on OTSL can be significantly smaller, e.g. by reducing the number of encoder and
    decoder layers, while preserving comparatively good prediction quality. This can
    further improve inference performance, yielding 5-6 times faster inference speed
    in OTSL with prediction quality comparable to models trained on HTML (see Table
    1).
- children: []
  content_layer: body
  label: text
  orig: Secondly, OTSL has more inherent structure and a significantly restricted
    vocabulary size. This allows autoregressive models to perform better in the TED
    metric, but especially with regards to prediction accuracy of the table-cell bounding
    boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically
    reduces the drift for table cell bounding boxes at high row count and in sparse
    tables. This leads to more accurate predictions and a significant reduction in
    post-processing complexity, which is an undesired necessity in HTML-based Im2Seq
    models. Significant novelty lies in OTSL syntactical rules, which are few, simple
    and always backwards looking. Each new token can be validated only by analyzing
    the sequence of previous tokens, without requiring the entire sequence to detect
    mistakes. This in return allows to perform structural error detection and correction
    on-the-fly during sequence generation.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 323.8958435058594
      coord_origin: BOTTOMLEFT
      l: 134.10235595703125
      r: 480.5948181152344
      t: 465.3463134765625
    charspan:
    - 0
    - 926
    page_no: 12
  self_ref: '#/texts/95'
  text: Secondly, OTSL has more inherent structure and a significantly restricted
    vocabulary size. This allows autoregressive models to perform better in the TED
    metric, but especially with regards to prediction accuracy of the table-cell bounding
    boxes (see Table 2). As shown in Figure 5, we observe that the OTSL drastically
    reduces the drift for table cell bounding boxes at high row count and in sparse
    tables. This leads to more accurate predictions and a significant reduction in
    post-processing complexity, which is an undesired necessity in HTML-based Im2Seq
    models. Significant novelty lies in OTSL syntactical rules, which are few, simple
    and always backwards looking. Each new token can be validated only by analyzing
    the sequence of previous tokens, without requiring the entire sequence to detect
    mistakes. This in return allows to perform structural error detection and correction
    on-the-fly during sequence generation.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: References
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 287.61077880859375
      coord_origin: BOTTOMLEFT
      l: 134.37088012695312
      r: 197.68641662597656
      t: 299.65625
    charspan:
    - 0
    - 10
    page_no: 12
  self_ref: '#/texts/96'
  text: References
- children: []
  content_layer: body
  label: reference
  orig: '1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering
    document conversion as a cloud service with high throughput and responsiveness.
    CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785, https://doi.org/10.48550/arXiv.2206.00785'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 227.4898681640625
      coord_origin: BOTTOMLEFT
      l: 139.37100219726562
      r: 480.5920104980469
      t: 270.19366455078125
    charspan:
    - 0
    - 269
    page_no: 12
  self_ref: '#/texts/97'
  text: '1. Auer, C., Dolfi, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering
    document conversion as a cloud service with high throughput and responsiveness.
    CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785, https://doi.org/10.48550/arXiv.2206.00785'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: "2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure\
    \ recognition in the wild using transformer and identity matrix-based augmentation.\
    \ In: Porwal, U., Forn\xE9s, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition.\
    \ pp. 545-561. Springer International Publishing, Cham (2022)"
  parent:
    $ref: '#/groups/2'
  prov:
  - bbox:
      b: 183.53439331054688
      coord_origin: BOTTOMLEFT
      l: 139.0526885986328
      r: 480.762939453125
      t: 225.8333282470703
    charspan:
    - 0
    - 302
    page_no: 12
  self_ref: '#/texts/98'
  text: "2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure\
    \ recognition in the wild using transformer and identity matrix-based augmentation.\
    \ In: Porwal, U., Forn\xE9s, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition.\
    \ pp. 545-561. Springer International Publishing, Cham (2022)"
- children: []
  content_layer: body
  label: reference
  orig: '3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated
    table structure recognition. arXiv preprint arXiv:1908.04729 (2019)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 160.81239318847656
      coord_origin: BOTTOMLEFT
      l: 138.96646118164062
      r: 480.5873107910156
      t: 181.62088012695312
    charspan:
    - 0
    - 140
    page_no: 12
  self_ref: '#/texts/99'
  text: '3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated
    table structure recognition. arXiv preprint arXiv:1908.04729 (2019)'
- children: []
  content_layer: body
  label: reference
  orig: '4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific
    table recognition. In: 2019 International Conference on Document Analysis and
    Recognition (ICDAR). pp. 894-901. IEEE (2019)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 127.05818176269531
      coord_origin: BOTTOMLEFT
      l: 139.2239227294922
      r: 480.6642150878906
      t: 158.688232421875
    charspan:
    - 0
    - 204
    page_no: 12
  self_ref: '#/texts/100'
  text: '4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific
    table recognition. In: 2019 International Conference on Document Analysis and
    Recognition (ICDAR). pp. 894-901. IEEE (2019)'
- children: []
  content_layer: body
  label: page_header
  orig: Optimized Table Tokenization for Table Structure Recognition
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 689.8847045898438
      coord_origin: BOTTOMLEFT
      l: 194.2587432861328
      r: 447.54290771484375
      t: 699.016357421875
    charspan:
    - 0
    - 60
    page_no: 13
  self_ref: '#/texts/101'
  text: Optimized Table Tokenization for Table Structure Recognition
- children: []
  content_layer: body
  label: page_header
  orig: '13'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 471.3756103515625
      r: 480.5894775390625
      t: 698.8148193359375
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/102'
  text: '13'
- children: []
  content_layer: body
  label: reference
  orig: '5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure
    and content extraction from scientific tables. International Journal on Document
    Analysis and Recognition (IJDAR) pp. 1-10 (2022)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 642.0465698242188
      coord_origin: BOTTOMLEFT
      l: 139.0633087158203
      r: 480.59478759765625
      t: 673.135986328125
    charspan:
    - 0
    - 203
    page_no: 13
  self_ref: '#/texts/103'
  text: '5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure
    and content extraction from scientific tables. International Journal on Document
    Analysis and Recognition (IJDAR) pp. 1-10 (2022)'
- children: []
  content_layer: body
  label: reference
  orig: '6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.:
    Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal
    and Information Processing Association Annual Summit and Conference (APSIPA ASC).
    pp. 1868-1873. IEEE (2022)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 598.4913940429688
      coord_origin: BOTTOMLEFT
      l: 138.89503479003906
      r: 480.9752502441406
      t: 640.6156616210938
    charspan:
    - 0
    - 265
    page_no: 13
  self_ref: '#/texts/104'
  text: '6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.:
    Table structure recognition based on grid shape graph. In: 2022 Asia-Pacific Signal
    and Information Processing Association Annual Summit and Conference (APSIPA ASC).
    pp. 1868-1873. IEEE (2022)'
- children: []
  content_layer: body
  label: reference
  orig: '7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark
    dataset for table detection and recognition (2019)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 576.5624389648438
      coord_origin: BOTTOMLEFT
      l: 139.06173706054688
      r: 480.6949462890625
      t: 597.01171875
    charspan:
    - 0
    - 131
    page_no: 13
  self_ref: '#/texts/105'
  text: '7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark
    dataset for table detection and recognition (2019)'
- children: []
  content_layer: body
  label: reference
  orig: '8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A.,
    Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document
    conversion using recurrent neural networks. Proceedings of the AAAI Conference
    on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/
    AAAI/article/view/17777'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 521.6362915039062
      coord_origin: BOTTOMLEFT
      l: 139.02195739746094
      r: 481.363525390625
      t: 574.87646484375
    charspan:
    - 0
    - 345
    page_no: 13
  self_ref: '#/texts/106'
  text: '8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A.,
    Carvalho, A., Dolfi, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document
    conversion using recurrent neural networks. Proceedings of the AAAI Conference
    on Artificial Intelligence 35 (17), 15137-15145 (May 2021), https://ojs.aaai.org/index.php/
    AAAI/article/view/17777'
- children: []
  content_layer: body
  label: reference
  orig: '9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure
    understanding with transformers. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 488.40643310546875
      coord_origin: BOTTOMLEFT
      l: 138.5304412841797
      r: 480.5938720703125
      t: 520.03466796875
    charspan:
    - 0
    - 234
    page_no: 13
  self_ref: '#/texts/107'
  text: '9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure
    understanding with transformers. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR). pp. 4614-4623 (June 2022)'
- children: []
  content_layer: body
  label: reference
  orig: '10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet:
    A large human-annotated dataset for document-layout segmentation. In: Zhang, A.,
    Rangwala, H. (eds.) KDD ''22: The 28th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining, Washington, DC, USA, August 14-18, 2022. pp. 3743-3751. ACM (2022).
    https://doi.org/10.1145/3534678.3539043, https:// doi.org/10.1145/3534678.3539043'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 422.0408630371094
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 481.3174133300781
      t: 486.98095703125
    charspan:
    - 0
    - 410
    page_no: 13
  self_ref: '#/texts/108'
  text: '10. Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.W.J.: Doclaynet:
    A large human-annotated dataset for document-layout segmentation. In: Zhang, A.,
    Rangwala, H. (eds.) KDD ''22: The 28th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining, Washington, DC, USA, August 14-18, 2022. pp. 3743-3751. ACM (2022).
    https://doi.org/10.1145/3534678.3539043, https:// doi.org/10.1145/3534678.3539043'
- children: []
  content_layer: body
  label: reference
  orig: '11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet:
    An approach for end to end table detection and structure recognition from imagebased
    documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition workshops. pp. 572-573 (2020)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 379.2555236816406
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 480.59295654296875
      t: 421.3261413574219
    charspan:
    - 0
    - 295
    page_no: 13
  self_ref: '#/texts/109'
  text: '11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet:
    An approach for end to end table detection and structure recognition from imagebased
    documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition workshops. pp. 572-573 (2020)'
- children: []
  content_layer: body
  label: reference
  orig: '12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt:
    Deep learning for detection and structure recognition of tables in document images.
    In: 2017 14th IAPR international conference on document analysis and recognition
    (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 335.25286865234375
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 480.6842956542969
      t: 377.8777770996094
    charspan:
    - 0
    - 281
    page_no: 13
  self_ref: '#/texts/110'
  text: '12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt:
    Deep learning for detection and structure recognition of tables in document images.
    In: 2017 14th IAPR international conference on document analysis and recognition
    (ICDAR). vol. 1, pp. 1162-1167. IEEE (2017)'
- children: []
  content_layer: body
  label: reference
  orig: '13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr:
    Deep learning based table structure recognition. In: 2019 International Conference
    on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 290.8941345214844
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 481.833740234375
      t: 333.5302734375
    charspan:
    - 0
    - 275
    page_no: 13
  self_ref: '#/texts/111'
  text: '13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr:
    Deep learning based table structure recognition. In: 2019 International Conference
    on Document Analysis and Recognition (ICDAR). pp. 1403-1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226'
- children: []
  content_layer: body
  label: reference
  orig: '14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive
    table extraction from unstructured documents. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June
    2022)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 247.7145538330078
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 480.5928649902344
      t: 290.0293273925781
    charspan:
    - 0
    - 241
    page_no: 13
  self_ref: '#/texts/112'
  text: '14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive
    table extraction from unstructured documents. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634-4642 (June
    2022)'
- children: []
  content_layer: body
  label: reference
  orig: '15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service:
    A machine learning platform to ingest documents at scale. In: Proceedings of the
    24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
    pp. 774-782. KDD ''18, Association for Computing Machinery, New York, NY, USA
    (2018). https://doi.org/10.1145/3219819.3219834, https://doi.org/10. 1145/3219819.3219834'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 181.6173553466797
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 481.1046447753906
      t: 246.4440460205078
    charspan:
    - 0
    - 404
    page_no: 13
  self_ref: '#/texts/113'
  text: '15. Staar, P.W.J., Dolfi, M., Auer, C., Bekas, C.: Corpus conversion service:
    A machine learning platform to ingest documents at scale. In: Proceedings of the
    24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
    pp. 774-782. KDD ''18, Association for Computing Machinery, New York, NY, USA
    (2018). https://doi.org/10.1145/3219819.3219834, https://doi.org/10. 1145/3219819.3219834'
- children: []
  content_layer: body
  label: reference
  orig: '16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis,
    CAN (1996), aAINN09397'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 160.0205535888672
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 480.5954284667969
      t: 180.87478637695312
    charspan:
    - 0
    - 96
    page_no: 13
  self_ref: '#/texts/114'
  text: '16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis,
    CAN (1996), aAINN09397'
- children: []
  content_layer: body
  label: reference
  orig: '17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from
    table images. In: 2019 International Conference on Document Analysis and Recognition
    (ICDAR). pp. 749-755. IEEE (2019)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 126.86089324951172
      coord_origin: BOTTOMLEFT
      l: 134.76400756835938
      r: 480.6768493652344
      t: 158.51412963867188
    charspan:
    - 0
    - 195
    page_no: 13
  self_ref: '#/texts/115'
  text: '17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from
    table images. In: 2019 International Conference on Document Analysis and Recognition
    (ICDAR). pp. 749-755. IEEE (2019)'
- children: []
  content_layer: body
  label: page_header
  orig: '14'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 143.97886657714844
      t: 698.4273681640625
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/116'
  text: '14'
- children: []
  content_layer: body
  label: page_header
  orig: M. Lysak, et al.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 690.1593017578125
      coord_origin: BOTTOMLEFT
      l: 167.52684020996094
      r: 231.72048950195312
      t: 699.1132202148438
    charspan:
    - 0
    - 16
    page_no: 14
  self_ref: '#/texts/117'
  text: M. Lysak, et al.
- children: []
  content_layer: body
  label: reference
  orig: '18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction
    network for table structure recognition. In: Proceedings of the IEEE/CVF International
    Conference on Computer Vision. pp. 1295-1304 (2021)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 642.1295166015625
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 480.59112548828125
      t: 673.19140625
    charspan:
    - 0
    - 223
    page_no: 14
  self_ref: '#/texts/118'
  text: '18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruction
    network for table structure recognition. In: Proceedings of the IEEE/CVF International
    Conference on Computer Vision. pp. 1295-1304 (2021)'
- children: []
  content_layer: body
  label: reference
  orig: '19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup''s
    solution for icdar 2021 competition on scientific literature parsing task b: Table
    recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848, https://arxiv.org/abs/2105.01848'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 597.1889038085938
      coord_origin: BOTTOMLEFT
      l: 134.76499938964844
      r: 480.6068420410156
      t: 640.3915405273438
    charspan:
    - 0
    - 268
    page_no: 14
  self_ref: '#/texts/119'
  text: '19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup''s
    solution for icdar 2021 competition on scientific literature parsing task b: Table
    recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848, https://arxiv.org/abs/2105.01848'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate
    table structure recognizer. Pattern Recognition 126, 108565 (2022)'
  parent:
    $ref: '#/groups/3'
  prov:
  - bbox:
      b: 576.5853881835938
      coord_origin: BOTTOMLEFT
      l: 134.48666381835938
      r: 480.5935363769531
      t: 596.872802734375
    charspan:
    - 0
    - 146
    page_no: 14
  self_ref: '#/texts/120'
  text: '20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate
    table structure recognizer. Pattern Recognition 126, 108565 (2022)'
- children: []
  content_layer: body
  label: reference
  orig: '21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table
    extractor (gte): A framework for joint table identification and cell structure
    recognition using visual context. In: 2021 IEEE Winter Conference on Applications
    of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021.
    00074'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 521.74560546875
      coord_origin: BOTTOMLEFT
      l: 134.44717407226562
      r: 481.17742919921875
      t: 574.9745483398438
    charspan:
    - 0
    - 329
    page_no: 14
  self_ref: '#/texts/121'
  text: '21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table
    extractor (gte): A framework for joint table identification and cell structure
    recognition using visual context. In: 2021 IEEE Winter Conference on Applications
    of Computer Vision (WACV). pp. 697-706 (2021). https://doi.org/10.1109/WACV48630.2021.
    00074'
- children: []
  content_layer: body
  label: reference
  orig: '22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition:
    Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.
    (eds.) Computer Vision-ECCV 2020. pp. 564-580. Springer International Publishing,
    Cham (2020)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 477.7259216308594
      coord_origin: BOTTOMLEFT
      l: 134.34185791015625
      r: 480.5955810546875
      t: 520.2467041015625
    charspan:
    - 0
    - 257
    page_no: 14
  self_ref: '#/texts/122'
  text: '22. Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition:
    Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.
    (eds.) Computer Vision-ECCV 2020. pp. 564-580. Springer International Publishing,
    Cham (2020)'
- children: []
  content_layer: body
  label: reference
  orig: '23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for
    document layout analysis. In: 2019 International Conference on Document Analysis
    and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 445.0785217285156
      coord_origin: BOTTOMLEFT
      l: 134.46534729003906
      r: 480.59454345703125
      t: 476.1098327636719
    charspan:
    - 0
    - 206
    page_no: 14
  self_ref: '#/texts/123'
  text: '23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for
    document layout analysis. In: 2019 International Conference on Document Analysis
    and Recognition (ICDAR). pp. 1015-1022. IEEE (2019)'
version: 1.1.0
