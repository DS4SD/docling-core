body:
  children:
  - $ref: '#/texts/0'
  - $ref: '#/texts/1'
  - $ref: '#/texts/2'
  - $ref: '#/texts/3'
  - $ref: '#/texts/4'
  - $ref: '#/texts/5'
  - $ref: '#/texts/6'
  - $ref: '#/texts/7'
  - $ref: '#/texts/8'
  - $ref: '#/texts/9'
  - $ref: '#/texts/10'
  - $ref: '#/texts/11'
  - $ref: '#/texts/12'
  - $ref: '#/figures/0'
  - $ref: '#/texts/13'
  - $ref: '#/texts/14'
  - $ref: '#/texts/15'
  - $ref: '#/texts/16'
  - $ref: '#/texts/17'
  - $ref: '#/texts/18'
  - $ref: '#/texts/19'
  - $ref: '#/texts/20'
  - $ref: '#/texts/21'
  - $ref: '#/texts/22'
  - $ref: '#/texts/23'
  - $ref: '#/texts/24'
  - $ref: '#/texts/25'
  - $ref: '#/texts/26'
  - $ref: '#/texts/27'
  - $ref: '#/texts/28'
  - $ref: '#/texts/29'
  - $ref: '#/texts/30'
  - $ref: '#/texts/31'
  - $ref: '#/texts/32'
  - $ref: '#/texts/33'
  - $ref: '#/texts/34'
  - $ref: '#/texts/35'
  - $ref: '#/texts/36'
  - $ref: '#/texts/37'
  - $ref: '#/texts/38'
  - $ref: '#/texts/39'
  - $ref: '#/figures/1'
  - $ref: '#/texts/40'
  - $ref: '#/texts/41'
  - $ref: '#/texts/42'
  - $ref: '#/texts/43'
  - $ref: '#/texts/44'
  - $ref: '#/texts/45'
  - $ref: '#/texts/46'
  - $ref: '#/texts/47'
  - $ref: '#/texts/48'
  - $ref: '#/texts/49'
  - $ref: '#/texts/50'
  - $ref: '#/tables/0'
  - $ref: '#/texts/51'
  - $ref: '#/texts/52'
  - $ref: '#/texts/53'
  - $ref: '#/texts/54'
  - $ref: '#/texts/55'
  - $ref: '#/texts/56'
  - $ref: '#/texts/57'
  - $ref: '#/texts/58'
  - $ref: '#/texts/59'
  - $ref: '#/texts/60'
  - $ref: '#/texts/61'
  - $ref: '#/texts/62'
  - $ref: '#/texts/63'
  - $ref: '#/texts/64'
  - $ref: '#/texts/65'
  - $ref: '#/texts/66'
  - $ref: '#/texts/67'
  - $ref: '#/texts/68'
  - $ref: '#/texts/69'
  - $ref: '#/texts/70'
  - $ref: '#/figures/2'
  - $ref: '#/texts/71'
  - $ref: '#/texts/72'
  - $ref: '#/texts/73'
  - $ref: '#/tables/1'
  - $ref: '#/texts/74'
  - $ref: '#/texts/75'
  - $ref: '#/texts/76'
  - $ref: '#/figures/3'
  - $ref: '#/texts/77'
  - $ref: '#/texts/78'
  - $ref: '#/texts/79'
  - $ref: '#/texts/80'
  - $ref: '#/texts/81'
  - $ref: '#/texts/82'
  - $ref: '#/tables/2'
  - $ref: '#/texts/83'
  - $ref: '#/texts/84'
  - $ref: '#/texts/85'
  - $ref: '#/texts/86'
  - $ref: '#/texts/87'
  - $ref: '#/tables/3'
  - $ref: '#/texts/88'
  - $ref: '#/texts/89'
  - $ref: '#/texts/90'
  - $ref: '#/texts/91'
  - $ref: '#/texts/92'
  - $ref: '#/texts/93'
  - $ref: '#/tables/4'
  - $ref: '#/texts/94'
  - $ref: '#/texts/95'
  - $ref: '#/texts/96'
  - $ref: '#/texts/97'
  - $ref: '#/texts/98'
  - $ref: '#/texts/99'
  - $ref: '#/texts/100'
  - $ref: '#/texts/101'
  - $ref: '#/texts/102'
  - $ref: '#/texts/103'
  - $ref: '#/texts/104'
  - $ref: '#/texts/105'
  - $ref: '#/texts/106'
  - $ref: '#/texts/107'
  - $ref: '#/texts/108'
  - $ref: '#/texts/109'
  - $ref: '#/texts/110'
  - $ref: '#/texts/111'
  - $ref: '#/texts/112'
  - $ref: '#/texts/113'
  - $ref: '#/texts/114'
  - $ref: '#/texts/115'
  - $ref: '#/texts/116'
  - $ref: '#/texts/117'
  - $ref: '#/figures/4'
  - $ref: '#/texts/118'
  - $ref: '#/texts/119'
  - $ref: '#/texts/120'
  - $ref: '#/texts/121'
  - $ref: '#/texts/122'
  - $ref: '#/texts/123'
  - $ref: '#/texts/124'
  - $ref: '#/texts/125'
  - $ref: '#/texts/126'
  - $ref: '#/texts/127'
  dloc: '#/body'
  hash: 1876595454579351028
  name: _root_
  parent: null
description: {}
figures:
- captions:
  - $ref: '#/texts/12'
  children: []
  data: {}
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/figures/0
  footnotes: []
  hash: 3823827261264467155
  image: null
  label: picture
  parent:
    $ref: '#/body'
  prov: []
  references: []
- captions:
  - $ref: '#/texts/39'
  children: []
  data: {}
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/figures/1
  footnotes: []
  hash: 2717789230650946439
  image: null
  label: picture
  parent:
    $ref: '#/body'
  prov: []
  references: []
- captions:
  - $ref: '#/texts/70'
  children: []
  data: {}
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/figures/2
  footnotes: []
  hash: 11874686886604579344
  image: null
  label: picture
  parent:
    $ref: '#/body'
  prov: []
  references: []
- captions:
  - $ref: '#/texts/76'
  children: []
  data: {}
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/figures/3
  footnotes: []
  hash: 13157758373214615403
  image: null
  label: picture
  parent:
    $ref: '#/body'
  prov: []
  references: []
- captions:
  - $ref: '#/texts/117'
  children: []
  data: {}
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/figures/4
  footnotes: []
  hash: 3241646916892239195
  image: null
  label: picture
  parent:
    $ref: '#/body'
  prov: []
  references: []
file_info:
  document_hash: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc
furniture:
  children: []
  dloc: '#/furniture'
  hash: 5280524054814059340
  name: _root_
  parent: null
groups: []
key_value_items: []
pages:
  '1':
    hash: 3c76b6d3fd82865e42c51d5cbd7d1a9996dba7902643b919acc581e866b92716
    image: null
    page_no: 1
    size:
      height: 792.0
      width: 612.0
  '2':
    hash: 5ccfaddd314d3712cbabc857c8c0f33d1268341ce37b27089857cbf09f0522d4
    image: null
    page_no: 2
    size:
      height: 792.0
      width: 612.0
  '3':
    hash: d2dc51ad0a01ee9486ffe248649ee1cd10ce35773de8e4b21abf30d310f4fc26
    image: null
    page_no: 3
    size:
      height: 792.0
      width: 612.0
  '4':
    hash: 310121977375f8f1106412189943bd70f121629b2b4d35394077233dedbfb041
    image: null
    page_no: 4
    size:
      height: 792.0
      width: 612.0
  '5':
    hash: 09fa72b602eb0640669844acabc17ef494802a4a9188aeaaf0e0131c496e6951
    image: null
    page_no: 5
    size:
      height: 792.0
      width: 612.0
  '6':
    hash: ec3fa60f136f3d9f5fa790ab27f5d1c14e5622573c52377b909b591d0be0ea44
    image: null
    page_no: 6
    size:
      height: 792.0
      width: 612.0
  '7':
    hash: ec1bc56fe581ce95615b1fab11c3ba8fc89662acf2f53446decd380a155b06dd
    image: null
    page_no: 7
    size:
      height: 792.0
      width: 612.0
  '8':
    hash: fbd2b06876dddc19ee08e0a9751d978c03e6943b74bedf1d83d6528cd4f8954d
    image: null
    page_no: 8
    size:
      height: 792.0
      width: 612.0
  '9':
    hash: 6cfa4eb4410fa9972da289dbf8d8cc585d317a192e1214c778ddd7768e98f311
    image: null
    page_no: 9
    size:
      height: 792.0
      width: 612.0
tables:
- captions:
  - $ref: '#/texts/50'
  children: []
  data:
    grid: []
    num_cols: 0
    num_rows: 0
    table_cells: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/tables/0
  footnotes: []
  hash: 14148577749296175318
  image: null
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 498.30108642578125
      coord_origin: BOTTOMLEFT
      l: 98.96420288085938
      r: 512.7739868164062
      t: 654.1231689453125
    charspan:
    - 0
    - 0
    page_no: 4
  references: []
- captions:
  - $ref: '#/texts/73'
  children: []
  data:
    grid: []
    num_cols: 0
    num_rows: 0
    table_cells: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/tables/1
  footnotes: []
  hash: 17333450552515386005
  image: null
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 440.30438232421875
      coord_origin: BOTTOMLEFT
      l: 61.93328094482422
      r: 285.75616455078125
      t: 596.587158203125
    charspan:
    - 0
    - 0
    page_no: 6
  references: []
- captions:
  - $ref: '#/texts/82'
  children: []
  data:
    grid: []
    num_cols: 0
    num_rows: 0
    table_cells: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/tables/2
  footnotes: []
  hash: 16080913497667217474
  image: null
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 496.419189453125
      coord_origin: BOTTOMLEFT
      l: 80.5073471069336
      r: 267.3428649902344
      t: 640.9814453125
    charspan:
    - 0
    - 0
    page_no: 7
  references: []
- captions:
  - $ref: '#/texts/87'
  children: []
  data:
    grid: []
    num_cols: 0
    num_rows: 0
    table_cells: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/tables/3
  footnotes: []
  hash: 7071974284449481758
  image: null
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 485.2873840332031
      coord_origin: BOTTOMLEFT
      l: 353.065185546875
      r: 523.3069458007812
      t: 641.25341796875
    charspan:
    - 0
    - 0
    page_no: 7
  references: []
- captions:
  - $ref: '#/texts/93'
  children: []
  data:
    grid: []
    num_cols: 0
    num_rows: 0
    table_cells: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/tables/4
  footnotes: []
  hash: 8754037299649738038
  image: null
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 452.12615966796875
      coord_origin: BOTTOMLEFT
      l: 72.87370300292969
      r: 274.87945556640625
      t: 619.3699951171875
    charspan:
    - 0
    - 0
    page_no: 8
  references: []
texts:
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/0
  hash: 5801389470470321019
  label: section_header
  orig: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 672.3833618164062
      coord_origin: BOTTOMLEFT
      l: 107.30000305175781
      r: 505.1857604980469
      t: 709.082275390625
    charspan:
    - 0
    - 71
    page_no: 1
  text: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/1
  hash: 8511179082257553176
  label: text
  orig: Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 611.2825317382812
      coord_origin: BOTTOMLEFT
      l: 90.94670867919922
      r: 193.91998291015625
      t: 658.7803344726562
    charspan:
    - 0
    - 73
    page_no: 1
  text: Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/2
  hash: 8279137503716887272
  label: text
  orig: Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 611.7597045898438
      coord_origin: BOTTOMLEFT
      l: 254.97935485839844
      r: 357.8802490234375
      t: 658.7174072265625
    charspan:
    - 0
    - 71
    page_no: 1
  text: Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/3
  hash: 16452346600845753706
  label: text
  orig: Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 611.7597045898438
      coord_origin: BOTTOMLEFT
      l: 419.0672302246094
      r: 522.0595703125
      t: 658.9878540039062
    charspan:
    - 0
    - 70
    page_no: 1
  text: Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/4
  hash: 5753518757297767565
  label: text
  orig: Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 553.3746948242188
      coord_origin: BOTTOMLEFT
      l: 171.90907287597656
      r: 275.3072509765625
      t: 600.1580200195312
    charspan:
    - 0
    - 72
    page_no: 1
  text: Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/5
  hash: 400399309987224909
  label: text
  orig: Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 553.3746948242188
      coord_origin: BOTTOMLEFT
      l: 336.5292053222656
      r: 439.84405517578125
      t: 599.942626953125
    charspan:
    - 0
    - 68
    page_no: 1
  text: Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/6
  hash: 7981313731349902307
  label: section_header
  orig: ABSTRACT
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 533.9879760742188
      coord_origin: BOTTOMLEFT
      l: 53.33011245727539
      r: 112.2127456665039
      t: 544.47509765625
    charspan:
    - 0
    - 8
    page_no: 1
  text: ABSTRACT
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/7
  hash: 18185954695676845569
  label: text
  orig: Accurate document layout analysis is a key requirement for highquality PDF
    document conversion. With the recent availability of public, large ground-truth
    datasets such as PubLayNet and DocBank, deep-learning models have proven to be
    very effective at layout detection and segmentation. While these datasets are
    of adequate size to train such models, they severely lack in layout variability
    since they are sourced from scientific article repositories such as PubMed and
    arXiv only. Consequently, the accuracy of the layout segmentation drops significantly
    when these models are applied on more challenging and diverse layouts. In this
    paper, we present DocLayNet , a new, publicly available, document-layout annotation
    dataset in COCO format. It contains 80863 manually annotated pages from diverse
    data sources to represent a wide variability in layouts. For each PDF page, the
    layout annotations provide labelled bounding-boxes with a choice of 11 distinct
    classes. DocLayNet also provides a subset of double- and triple-annotated pages
    to determine the inter-annotator agreement. In multiple experiments, we provide
    baseline accuracy scores (in mAP) for a set of popular object detection models.
    We also demonstrate that these models fall approximately 10% behind the inter-annotator
    agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size.
    Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing
    that layout predictions of the DocLayNettrained models are more robust and thus
    the preferred choice for general-purpose document-layout analysis.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 257.10565185546875
      coord_origin: BOTTOMLEFT
      l: 52.857933044433594
      r: 295.5601806640625
      t: 529.5941162109375
    charspan:
    - 0
    - 1595
    page_no: 1
  text: Accurate document layout analysis is a key requirement for highquality PDF
    document conversion. With the recent availability of public, large ground-truth
    datasets such as PubLayNet and DocBank, deep-learning models have proven to be
    very effective at layout detection and segmentation. While these datasets are
    of adequate size to train such models, they severely lack in layout variability
    since they are sourced from scientific article repositories such as PubMed and
    arXiv only. Consequently, the accuracy of the layout segmentation drops significantly
    when these models are applied on more challenging and diverse layouts. In this
    paper, we present DocLayNet , a new, publicly available, document-layout annotation
    dataset in COCO format. It contains 80863 manually annotated pages from diverse
    data sources to represent a wide variability in layouts. For each PDF page, the
    layout annotations provide labelled bounding-boxes with a choice of 11 distinct
    classes. DocLayNet also provides a subset of double- and triple-annotated pages
    to determine the inter-annotator agreement. In multiple experiments, we provide
    baseline accuracy scores (in mAP) for a set of popular object detection models.
    We also demonstrate that these models fall approximately 10% behind the inter-annotator
    agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size.
    Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing
    that layout predictions of the DocLayNettrained models are more robust and thus
    the preferred choice for general-purpose document-layout analysis.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/8
  hash: 17745163365450621279
  label: section_header
  orig: CCS CONCEPTS
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 230.69398498535156
      coord_origin: BOTTOMLEFT
      l: 53.36912155151367
      r: 134.81988525390625
      t: 241.21551513671875
    charspan:
    - 0
    - 12
    page_no: 1
  text: CCS CONCEPTS
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/9
  hash: 12529050007388097730
  label: text
  orig: "\xB7 Information systems \u2192 Document structure ; \xB7 Applied computing\
    \ \u2192 Document analysis ; \xB7 Computing methodologies \u2192 Machine learning\
    \ ; Computer vision ; Object detection ;"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 194.8704071044922
      coord_origin: BOTTOMLEFT
      l: 53.02470016479492
      r: 297.8529357910156
      t: 226.241455078125
    charspan:
    - 0
    - 170
    page_no: 1
  text: "\xB7 Information systems \u2192 Document structure ; \xB7 Applied computing\
    \ \u2192 Document analysis ; \xB7 Computing methodologies \u2192 Machine learning\
    \ ; Computer vision ; Object detection ;"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/10
  hash: 11569407347437572994
  label: text
  orig: Permission to make digital or hard copies of part or all of this work for
    personal or classroom use is granted without fee provided that copies are not
    made or distributed for profit or commercial advantage and that copies bear this
    notice and the full citation on the first page. Copyrights for third-party components
    of this work must be honored. For all other uses, contact the owner/author(s).
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 117.82738494873047
      coord_origin: BOTTOMLEFT
      l: 53.33460235595703
      r: 295.11798095703125
      t: 158.33511352539062
    charspan:
    - 0
    - 397
    page_no: 1
  text: Permission to make digital or hard copies of part or all of this work for
    personal or classroom use is granted without fee provided that copies are not
    made or distributed for profit or commercial advantage and that copies bear this
    notice and the full citation on the first page. Copyrights for third-party components
    of this work must be honored. For all other uses, contact the owner/author(s).
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/11
  hash: 13344389659304761998
  label: text
  orig: "KDD '22, August 14-18, 2022, Washington, DC, USA \xA9 2022 Copyright held\
    \ by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 85.73310852050781
      coord_origin: BOTTOMLEFT
      l: 53.31700134277344
      r: 197.8627471923828
      t: 116.91976928710938
    charspan:
    - 0
    - 168
    page_no: 1
  text: "KDD '22, August 14-18, 2022, Washington, DC, USA \xA9 2022 Copyright held\
    \ by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/12
  hash: 3162927929825665449
  label: caption
  orig: 'Figure 1: Four examples of complex page layouts across different document
    categories'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 232.3291473388672
      coord_origin: BOTTOMLEFT
      l: 317.2291564941406
      r: 559.8057861328125
      t: 252.12974548339844
    charspan:
    - 0
    - 84
    page_no: 1
  text: 'Figure 1: Four examples of complex page layouts across different document
    categories'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/13
  hash: 13011367304084404613
  label: section_header
  orig: KEYWORDS
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 189.22499084472656
      coord_origin: BOTTOMLEFT
      l: 317.11431884765625
      r: 379.82049560546875
      t: 199.97215270996094
    charspan:
    - 0
    - 8
    page_no: 1
  text: KEYWORDS
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/14
  hash: 16726456449567869739
  label: text
  orig: PDF document conversion, layout segmentation, object-detection, data set,
    Machine Learning
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 164.9988250732422
      coord_origin: BOTTOMLEFT
      l: 317.2037658691406
      r: 559.2164306640625
      t: 184.67845153808594
    charspan:
    - 0
    - 90
    page_no: 1
  text: PDF document conversion, layout segmentation, object-detection, data set,
    Machine Learning
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/15
  hash: 5718230321549514887
  label: section_header
  orig: 'ACM Reference Format:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 144.41390991210938
      coord_origin: BOTTOMLEFT
      l: 317.3434753417969
      r: 404.6536560058594
      t: 152.36439514160156
    charspan:
    - 0
    - 21
    page_no: 1
  text: 'ACM Reference Format:'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/16
  hash: 17635312130661974579
  label: text
  orig: 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter
    Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis.
    In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining (KDD ''22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY,
    USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 84.62297058105469
      coord_origin: BOTTOMLEFT
      l: 317.1117248535156
      r: 559.5494995117188
      t: 142.41151428222656
    charspan:
    - 0
    - 374
    page_no: 1
  text: 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter
    Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis.
    In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining (KDD ''22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY,
    USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/17
  hash: 5293186016864745982
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph\
    \ Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 722.7692260742188
      coord_origin: BOTTOMLEFT
      l: 53.19501876831055
      r: 558.4357299804688
      t: 732.1524047851562
    charspan:
    - 0
    - 130
    page_no: 2
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph\
    \ Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/18
  hash: 5428450824043951937
  label: section_header
  orig: 1 INTRODUCTION
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 695.8309936523438
      coord_origin: BOTTOMLEFT
      l: 53.79800033569336
      r: 156.52899169921875
      t: 706.4523315429688
    charspan:
    - 0
    - 14
    page_no: 2
  text: 1 INTRODUCTION
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/19
  hash: 15747194476520587400
  label: text
  orig: Despite the substantial improvements achieved with machine-learning (ML) approaches
    and deep neural networks in recent years, document conversion remains a challenging
    problem, as demonstrated by the numerous public competitions held on this topic
    [1-4]. The challenge originates from the huge variability in PDF documents regarding
    layout, language and formats (scanned, programmatic or a combination of both).
    Engineering a single ML model that can be applied on all types of documents and
    provides high-quality layout segmentation remains to this day extremely challenging
    [5]. To highlight the variability in document layouts, we show a few example documents
    from the DocLayNet dataset in Figure 1.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 562.986572265625
      coord_origin: BOTTOMLEFT
      l: 52.80397415161133
      r: 303.1766357421875
      t: 681.3472290039062
    charspan:
    - 0
    - 702
    page_no: 2
  text: Despite the substantial improvements achieved with machine-learning (ML) approaches
    and deep neural networks in recent years, document conversion remains a challenging
    problem, as demonstrated by the numerous public competitions held on this topic
    [1-4]. The challenge originates from the huge variability in PDF documents regarding
    layout, language and formats (scanned, programmatic or a combination of both).
    Engineering a single ML model that can be applied on all types of documents and
    provides high-quality layout segmentation remains to this day extremely challenging
    [5]. To highlight the variability in document layouts, we show a few example documents
    from the DocLayNet dataset in Figure 1.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/20
  hash: 9815825593984971365
  label: text
  orig: 'A key problem in the process of document conversion is to understand the
    structure of a single document page, i.e. which segments of text should be grouped
    together in a unit. To train models for this task, there are currently two large
    datasets available to the community, PubLayNet [6] and DocBank [7]. They were
    introduced in 2019 and 2020 respectively and significantly accelerated the implementation
    of layout detection and segmentation models due to their sizes of 300K and 500K
    ground-truth pages. These sizes were achieved by leveraging an automation approach.
    The benefit of automated ground-truth generation is obvious: one can generate
    large ground-truth datasets at virtually no cost. However, the automation introduces
    a constraint on the variability in the dataset, because corresponding structured
    source data must be available. PubLayNet and DocBank were both generated from
    scientific document repositories (PubMed and arXiv), which provide XML or L A
    T E X sources. Those scientific documents present a limited variability in their
    layouts, because they are typeset in uniform templates provided by the publishers.
    Obviously, documents such as technical manuals, annual company reports, legal
    text, government tenders, etc. have very different and partially unique layouts.
    As a consequence, the layout predictions obtained from models trained on PubLayNet
    or DocBank is very reasonable when applied on scientific documents. However, for
    more artistic or free-style layouts, we see sub-par prediction quality from these
    models, which we demonstrate in Section 5.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 289.0808410644531
      coord_origin: BOTTOMLEFT
      l: 52.89326477050781
      r: 295.5641174316406
      t: 561.2902221679688
    charspan:
    - 0
    - 1580
    page_no: 2
  text: 'A key problem in the process of document conversion is to understand the
    structure of a single document page, i.e. which segments of text should be grouped
    together in a unit. To train models for this task, there are currently two large
    datasets available to the community, PubLayNet [6] and DocBank [7]. They were
    introduced in 2019 and 2020 respectively and significantly accelerated the implementation
    of layout detection and segmentation models due to their sizes of 300K and 500K
    ground-truth pages. These sizes were achieved by leveraging an automation approach.
    The benefit of automated ground-truth generation is obvious: one can generate
    large ground-truth datasets at virtually no cost. However, the automation introduces
    a constraint on the variability in the dataset, because corresponding structured
    source data must be available. PubLayNet and DocBank were both generated from
    scientific document repositories (PubMed and arXiv), which provide XML or L A
    T E X sources. Those scientific documents present a limited variability in their
    layouts, because they are typeset in uniform templates provided by the publishers.
    Obviously, documents such as technical manuals, annual company reports, legal
    text, government tenders, etc. have very different and partially unique layouts.
    As a consequence, the layout predictions obtained from models trained on PubLayNet
    or DocBank is very reasonable when applied on scientific documents. However, for
    more artistic or free-style layouts, we see sub-par prediction quality from these
    models, which we demonstrate in Section 5.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/21
  hash: 562391038162260731
  label: text
  orig: 'In this paper, we present the DocLayNet dataset. It provides pageby-page
    layout annotation ground-truth using bounding-boxes for 11 distinct class labels
    on 80863 unique document pages, of which a fraction carry double- or triple-annotations.
    DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made
    available to the public 1 in order to stimulate the document-layout analysis community.
    It distinguishes itself in the following aspects:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 212.36782836914062
      coord_origin: BOTTOMLEFT
      l: 53.12458419799805
      r: 295.56396484375
      t: 287.0208740234375
    charspan:
    - 0
    - 462
    page_no: 2
  text: 'In this paper, we present the DocLayNet dataset. It provides pageby-page
    layout annotation ground-truth using bounding-boxes for 11 distinct class labels
    on 80863 unique document pages, of which a fraction carry double- or triple-annotations.
    DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made
    available to the public 1 in order to stimulate the document-layout analysis community.
    It distinguishes itself in the following aspects:'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/22
  hash: 1687994490476660946
  label: list_item
  orig: '(1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on
    human annotation instead of automation approaches to generate the data set.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 176.96405029296875
      coord_origin: BOTTOMLEFT
      l: 64.64593505859375
      r: 295.5616455078125
      t: 208.28524780273438
    charspan:
    - 0
    - 149
    page_no: 2
  text: '(1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on
    human annotation instead of automation approaches to generate the data set.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/23
  hash: 8977022680477147526
  label: list_item
  orig: '(2) Large Layout Variability : We include diverse and complex layouts from
    a large variety of public sources.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 154.92233276367188
      coord_origin: BOTTOMLEFT
      l: 64.50244140625
      r: 294.3029479980469
      t: 174.95782470703125
    charspan:
    - 0
    - 109
    page_no: 2
  text: '(2) Large Layout Variability : We include diverse and complex layouts from
    a large variety of public sources.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/24
  hash: 2133234466113940345
  label: list_item
  orig: '(3) Detailed Label Set : We define 11 class labels to distinguish layout
    features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although
    not a superset of ours.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 121.99307250976562
      coord_origin: BOTTOMLEFT
      l: 64.18266296386719
      r: 294.6838073730469
      t: 153.57122802734375
    charspan:
    - 0
    - 180
    page_no: 2
  text: '(3) Detailed Label Set : We define 11 class labels to distinguish layout
    features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although
    not a superset of ours.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/25
  hash: 15972412295294232993
  label: list_item
  orig: '(4) Redundant Annotations : A fraction of the pages in the DocLayNet data
    set carry more than one human annotation.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 99.92230987548828
      coord_origin: BOTTOMLEFT
      l: 64.30329132080078
      r: 295.56439208984375
      t: 120.3491439819336
    charspan:
    - 0
    - 115
    page_no: 2
  text: '(4) Redundant Annotations : A fraction of the pages in the DocLayNet data
    set carry more than one human annotation.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/26
  hash: 338444530349878300
  label: footnote
  orig: $^{1}$https://developer.ibm.com/exchanges/data/all/doclaynet
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 82.76702880859375
      coord_origin: BOTTOMLEFT
      l: 53.60314178466797
      r: 216.05824279785156
      t: 90.63584899902344
    charspan:
    - 0
    - 60
    page_no: 2
  text: $^{1}$https://developer.ibm.com/exchanges/data/all/doclaynet
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/27
  hash: 3617521057052249807
  label: text
  orig: This enables experimentation with annotation uncertainty and quality control
    analysis.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 685.3028564453125
      coord_origin: BOTTOMLEFT
      l: 341.2403564453125
      r: 558.5009765625
      t: 705.5034790039062
    charspan:
    - 0
    - 86
    page_no: 2
  text: This enables experimentation with annotation uncertainty and quality control
    analysis.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/28
  hash: 14788267481324200655
  label: list_item
  orig: '(5) Pre-defined Train-, Test- & Validation-set : Like DocBank, we provide
    fixed train-, test- & validation-sets to ensure proportional representation of
    the class-labels. Further, we prevent leakage of unique layouts across sets, which
    has a large effect on model accuracy scores.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 630.4351806640625
      coord_origin: BOTTOMLEFT
      l: 328.06146240234375
      r: 559.7210083007812
      t: 683.4995727539062
    charspan:
    - 0
    - 280
    page_no: 2
  text: '(5) Pre-defined Train-, Test- & Validation-set : Like DocBank, we provide
    fixed train-, test- & validation-sets to ensure proportional representation of
    the class-labels. Further, we prevent leakage of unique layouts across sets, which
    has a large effect on model accuracy scores.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/29
  hash: 18119595765995049833
  label: text
  orig: All aspects outlined above are detailed in Section 3. In Section 4, we will
    elaborate on how we designed and executed this large-scale human annotation campaign.
    We will also share key insights and lessons learned that might prove helpful for
    other parties planning to set up annotation campaigns.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 571.292724609375
      coord_origin: BOTTOMLEFT
      l: 317.0706787109375
      r: 559.1903076171875
      t: 624.9239501953125
    charspan:
    - 0
    - 297
    page_no: 2
  text: All aspects outlined above are detailed in Section 3. In Section 4, we will
    elaborate on how we designed and executed this large-scale human annotation campaign.
    We will also share key insights and lessons learned that might prove helpful for
    other parties planning to set up annotation campaigns.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/30
  hash: 2634162194000949275
  label: text
  orig: In Section 5, we will present baseline accuracy numbers for a variety of object
    detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet.
    We further show how the model performance is impacted by varying the DocLayNet
    dataset size, reducing the label set and modifying the train/test-split. Last
    but not least, we compare the performance of models trained on PubLayNet, DocBank
    and DocLayNet and demonstrate that a model trained on DocLayNet provides overall
    more robust layout recovery.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 483.6390686035156
      coord_origin: BOTTOMLEFT
      l: 316.9918518066406
      r: 559.5819702148438
      t: 569.6455078125
    charspan:
    - 0
    - 506
    page_no: 2
  text: In Section 5, we will present baseline accuracy numbers for a variety of object
    detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet.
    We further show how the model performance is impacted by varying the DocLayNet
    dataset size, reducing the label set and modifying the train/test-split. Last
    but not least, we compare the performance of models trained on PubLayNet, DocBank
    and DocLayNet and demonstrate that a model trained on DocLayNet provides overall
    more robust layout recovery.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/31
  hash: 12785294041260556899
  label: section_header
  orig: 2 RELATED WORK
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 460.4820251464844
      coord_origin: BOTTOMLEFT
      l: 317.33935546875
      r: 422.0046081542969
      t: 471.2471923828125
    charspan:
    - 0
    - 14
    page_no: 2
  text: 2 RELATED WORK
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/32
  hash: 15532515360198720027
  label: text
  orig: While early approaches in document-layout analysis used rulebased algorithms
    and heuristics [8], the problem is lately addressed with deep learning methods.
    The most common approach is to leverage object detection models [9-15]. In the
    last decade, the accuracy and speed of these models has increased dramatically.
    Furthermore, most state-of-the-art object detection methods can be trained and
    applied with very little work, thanks to a standardisation effort of the ground-truth
    data format [16] and common deep-learning frameworks [17]. Reference data sets
    such as PubLayNet [6] and DocBank provide their data in the commonly accepted
    COCO format [16].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 327.7038269042969
      coord_origin: BOTTOMLEFT
      l: 316.9687805175781
      r: 559.7161254882812
      t: 446.38397216796875
    charspan:
    - 0
    - 655
    page_no: 2
  text: While early approaches in document-layout analysis used rulebased algorithms
    and heuristics [8], the problem is lately addressed with deep learning methods.
    The most common approach is to leverage object detection models [9-15]. In the
    last decade, the accuracy and speed of these models has increased dramatically.
    Furthermore, most state-of-the-art object detection methods can be trained and
    applied with very little work, thanks to a standardisation effort of the ground-truth
    data format [16] and common deep-learning frameworks [17]. Reference data sets
    such as PubLayNet [6] and DocBank provide their data in the commonly accepted
    COCO format [16].
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/33
  hash: 7759908539731899164
  label: text
  orig: Lately, new types of ML models for document-layout analysis have emerged in
    the community [18-21]. These models do not approach the problem of layout analysis
    purely based on an image representation of the page, as computer vision methods
    do. Instead, they combine the text tokens and image representation of a page in
    order to obtain a segmentation. While the reported accuracies appear to be promising,
    a broadly accepted data format which links geometric and textual features has
    yet to establish.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 239.59246826171875
      coord_origin: BOTTOMLEFT
      l: 317.156982421875
      r: 559.1864624023438
      t: 325.6906433105469
    charspan:
    - 0
    - 500
    page_no: 2
  text: Lately, new types of ML models for document-layout analysis have emerged in
    the community [18-21]. These models do not approach the problem of layout analysis
    purely based on an image representation of the page, as computer vision methods
    do. Instead, they combine the text tokens and image representation of a page in
    order to obtain a segmentation. While the reported accuracies appear to be promising,
    a broadly accepted data format which links geometric and textual features has
    yet to establish.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/34
  hash: 13224162835784204794
  label: section_header
  orig: 3 THE DOCLAYNET DATASET
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 216.37100219726562
      coord_origin: BOTTOMLEFT
      l: 317.58740234375
      r: 477.8531799316406
      t: 226.6800994873047
    charspan:
    - 0
    - 23
    page_no: 2
  text: 3 THE DOCLAYNET DATASET
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/35
  hash: 13840788721079437184
  label: text
  orig: DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances
    of human annotations, and 1591 carry three. This amounts to 91104 total annotation
    instances. The annotations provide layout information in the shape of labeled,
    rectangular boundingboxes. We define 11 distinct labels for layout features, namely
    Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture
    , Section-header , Table , Text , and Title . Our reasoning for picking this particular
    label set is detailed in Section 4.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 116.19312286376953
      coord_origin: BOTTOMLEFT
      l: 317.11236572265625
      r: 559.7131958007812
      t: 202.27523803710938
    charspan:
    - 0
    - 522
    page_no: 2
  text: DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances
    of human annotations, and 1591 carry three. This amounts to 91104 total annotation
    instances. The annotations provide layout information in the shape of labeled,
    rectangular boundingboxes. We define 11 distinct labels for layout features, namely
    Caption , Footnote , Formula , List-item , Page-footer , Page-header , Picture
    , Section-header , Table , Text , and Title . Our reasoning for picking this particular
    label set is detailed in Section 4.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/36
  hash: 8382469735566893423
  label: text
  orig: In addition to open intellectual property constraints for the source documents,
    we required that the documents in DocLayNet adhere to a few conditions. Firstly,
    we kept scanned documents
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 83.59282684326172
      coord_origin: BOTTOMLEFT
      l: 317.34619140625
      r: 558.5303344726562
      t: 114.41421508789062
    charspan:
    - 0
    - 186
    page_no: 2
  text: In addition to open intellectual property constraints for the source documents,
    we required that the documents in DocLayNet adhere to a few conditions. Firstly,
    we kept scanned documents
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/37
  hash: 15463014254960213695
  label: page_header
  orig: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 722.95458984375
      coord_origin: BOTTOMLEFT
      l: 53.4626579284668
      r: 347.0511779785156
      t: 732.11474609375
    charspan:
    - 0
    - 71
    page_no: 3
  text: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/38
  hash: 202003194997475932
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0569458007812
      coord_origin: BOTTOMLEFT
      l: 365.31488037109375
      r: 558.807861328125
      t: 731.9796142578125
    charspan:
    - 0
    - 48
    page_no: 3
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/39
  hash: 16454164006377695992
  label: caption
  orig: 'Figure 2: Distribution of DocLayNet pages across document categories.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 424.931396484375
      coord_origin: BOTTOMLEFT
      l: 53.244232177734375
      r: 294.5379943847656
      t: 510.7526550292969
    charspan:
    - 0
    - 513
    page_no: 3
  text: 'Figure 2: Distribution of DocLayNet pages across document categories.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/40
  hash: 17429156214159736783
  label: text
  orig: The pages in DocLayNet can be grouped into six distinct categories, namely
    Financial Reports , Manuals , Scientific Articles , Laws & Regulations , Patents
    and Government Tenders . Each document category was sourced from various repositories.
    For example, Financial Reports contain both free-style format annual reports 2
    which expose company-specific, artistic layouts as well as the more formal SEC
    filings. The two largest categories ( Financial Reports and Manuals ) contain
    a large amount of free-style layouts in order to obtain maximum variability. In
    the other four categories, we boosted the variability by mixing documents from
    independent providers, such as different government websites or publishers. In
    Figure 2, we show the document categories contained in DocLayNet with their respective
    sizes.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 282.6438293457031
      coord_origin: BOTTOMLEFT
      l: 53.10974884033203
      r: 295.5604553222656
      t: 423.1407775878906
    charspan:
    - 0
    - 810
    page_no: 3
  text: The pages in DocLayNet can be grouped into six distinct categories, namely
    Financial Reports , Manuals , Scientific Articles , Laws & Regulations , Patents
    and Government Tenders . Each document category was sourced from various repositories.
    For example, Financial Reports contain both free-style format annual reports 2
    which expose company-specific, artistic layouts as well as the more formal SEC
    filings. The two largest categories ( Financial Reports and Manuals ) contain
    a large amount of free-style layouts in order to obtain maximum variability. In
    the other four categories, we boosted the variability by mixing documents from
    independent providers, such as different government websites or publishers. In
    Figure 2, we show the document categories contained in DocLayNet with their respective
    sizes.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/41
  hash: 4835758972077135061
  label: text
  orig: We did not control the document selection with regard to language. The vast
    majority of documents contained in DocLayNet (close to 95%) are published in English
    language. However, DocLayNet also contains a number of documents in other languages
    such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language
    has negligible impact on the performance of computer vision methods such as object
    detection and segmentation models, it might prove challenging for layout analysis
    methods which exploit textual features.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 183.77932739257812
      coord_origin: BOTTOMLEFT
      l: 52.8973388671875
      r: 295.5615539550781
      t: 281.3227233886719
    charspan:
    - 0
    - 535
    page_no: 3
  text: We did not control the document selection with regard to language. The vast
    majority of documents contained in DocLayNet (close to 95%) are published in English
    language. However, DocLayNet also contains a number of documents in other languages
    such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language
    has negligible impact on the performance of computer vision methods such as object
    detection and segmentation models, it might prove challenging for layout analysis
    methods which exploit textual features.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/42
  hash: 6442074878702101187
  label: text
  orig: To ensure that future benchmarks in the document-layout analysis community
    can be easily compared, we have split up DocLayNet into pre-defined train-, test-
    and validation-sets. In this way, we can avoid spurious variations in the evaluation
    scores due to random splitting in train-, test- and validation-sets. We also ensured
    that less frequent labels are represented in train and test sets in equal proportions.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 106.8985824584961
      coord_origin: BOTTOMLEFT
      l: 53.209388732910156
      r: 295.56396484375
      t: 182.471923828125
    charspan:
    - 0
    - 413
    page_no: 3
  text: To ensure that future benchmarks in the document-layout analysis community
    can be easily compared, we have split up DocLayNet into pre-defined train-, test-
    and validation-sets. In this way, we can avoid spurious variations in the evaluation
    scores due to random splitting in train-, test- and validation-sets. We also ensured
    that less frequent labels are represented in train and test sets in equal proportions.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/43
  hash: 13873304636238013732
  label: footnote
  orig: $^{2}$e.g. AAPL from https://www.annualreports.com/
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 83.35768127441406
      coord_origin: BOTTOMLEFT
      l: 53.352603912353516
      r: 195.78997802734375
      t: 91.47167205810547
    charspan:
    - 0
    - 51
    page_no: 3
  text: $^{2}$e.g. AAPL from https://www.annualreports.com/
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/44
  hash: 6259711523792429489
  label: text
  orig: Table 1 shows the overall frequency and distribution of the labels among the
    different sets. Importantly, we ensure that subsets are only split on full-document
    boundaries. This avoids that pages of the same document are spread over train,
    test and validation set, which can give an undesired evaluation advantage to models
    and lead to overestimation of their prediction accuracy. We will show the impact
    of this decision in Section 5.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 630.5088500976562
      coord_origin: BOTTOMLEFT
      l: 317.0691833496094
      r: 559.1918334960938
      t: 705.8527221679688
    charspan:
    - 0
    - 435
    page_no: 3
  text: Table 1 shows the overall frequency and distribution of the labels among the
    different sets. Importantly, we ensure that subsets are only split on full-document
    boundaries. This avoids that pages of the same document are spread over train,
    test and validation set, which can give an undesired evaluation advantage to models
    and lead to overestimation of their prediction accuracy. We will show the impact
    of this decision in Section 5.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/45
  hash: 9126253445878309540
  label: text
  orig: "In order to accommodate the different types of models currently in use by\
    \ the community, we provide DocLayNet in an augmented COCO format [16]. This entails\
    \ the standard COCO ground-truth file (in JSON format) with the associated page\
    \ images (in PNG format, 1025 \xD7 1025 pixels). Furthermore, custom fields have\
    \ been added to each COCO record to specify document category, original document\
    \ filename and page number. In addition, we also provide the original PDF pages,\
    \ as well as sidecar files containing parsed PDF text and text-cell coordinates\
    \ (in JSON). All additional files are linked to the primary page images by their\
    \ matching filenames."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 520.8086547851562
      coord_origin: BOTTOMLEFT
      l: 317.05938720703125
      r: 558.862060546875
      t: 628.44580078125
    charspan:
    - 0
    - 645
    page_no: 3
  text: "In order to accommodate the different types of models currently in use by\
    \ the community, we provide DocLayNet in an augmented COCO format [16]. This entails\
    \ the standard COCO ground-truth file (in JSON format) with the associated page\
    \ images (in PNG format, 1025 \xD7 1025 pixels). Furthermore, custom fields have\
    \ been added to each COCO record to specify document category, original document\
    \ filename and page number. In addition, we also provide the original PDF pages,\
    \ as well as sidecar files containing parsed PDF text and text-cell coordinates\
    \ (in JSON). All additional files are linked to the primary page images by their\
    \ matching filenames."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/46
  hash: 17722516482300246985
  label: text
  orig: Despite being cost-intense and far less scalable than automation, human annotation
    has several benefits over automated groundtruth generation. The first and most
    obvious reason to leverage human annotations is the freedom to annotate any type
    of document without requiring a programmatic source. For most PDF documents, the
    original source document is not available. The latter is not a hard constraint
    with human annotation, but it is for automated methods. A second reason to use
    human annotations is that the latter usually provide a more natural interpretation
    of the page layout. The human-interpreted layout can significantly deviate from
    the programmatic layout used in typesetting. For example, "invisible" tables might
    be used solely for aligning text paragraphs on columns. Such typesetting tricks
    might be interpreted by automated methods incorrectly as an actual table, while
    the human annotation will interpret it correctly as Text or other styles. The
    same applies to multi-line text elements, when authors decided to space them as
    "invisible" list elements without bullet symbols. A third reason to gather ground-truth
    through human annotation is to estimate a "natural" upper bound on the segmentation
    accuracy. As we will show in Section 4, certain documents featuring complex layouts
    can have different but equally acceptable layout interpretations. This natural
    upper bound for segmentation accuracy can be found by annotating the same pages
    multiple times by different people and evaluating the inter-annotator agreement.
    Such a baseline consistency evaluation is very useful to define expectations for
    a good target accuracy in trained deep neural network models and avoid overfitting
    (see Table 1). On the flip side, achieving high annotation consistency proved
    to be a key challenge in human annotation, as we outline in Section 4.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 203.11082458496094
      coord_origin: BOTTOMLEFT
      l: 316.88604736328125
      r: 559.7215576171875
      t: 518.6715087890625
    charspan:
    - 0
    - 1854
    page_no: 3
  text: Despite being cost-intense and far less scalable than automation, human annotation
    has several benefits over automated groundtruth generation. The first and most
    obvious reason to leverage human annotations is the freedom to annotate any type
    of document without requiring a programmatic source. For most PDF documents, the
    original source document is not available. The latter is not a hard constraint
    with human annotation, but it is for automated methods. A second reason to use
    human annotations is that the latter usually provide a more natural interpretation
    of the page layout. The human-interpreted layout can significantly deviate from
    the programmatic layout used in typesetting. For example, "invisible" tables might
    be used solely for aligning text paragraphs on columns. Such typesetting tricks
    might be interpreted by automated methods incorrectly as an actual table, while
    the human annotation will interpret it correctly as Text or other styles. The
    same applies to multi-line text elements, when authors decided to space them as
    "invisible" list elements without bullet symbols. A third reason to gather ground-truth
    through human annotation is to estimate a "natural" upper bound on the segmentation
    accuracy. As we will show in Section 4, certain documents featuring complex layouts
    can have different but equally acceptable layout interpretations. This natural
    upper bound for segmentation accuracy can be found by annotating the same pages
    multiple times by different people and evaluating the inter-annotator agreement.
    Such a baseline consistency evaluation is very useful to define expectations for
    a good target accuracy in trained deep neural network models and avoid overfitting
    (see Table 1). On the flip side, achieving high annotation consistency proved
    to be a key challenge in human annotation, as we outline in Section 4.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/47
  hash: 8217803899333050095
  label: section_header
  orig: 4 ANNOTATION CAMPAIGN
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 174.8409881591797
      coord_origin: BOTTOMLEFT
      l: 317.66510009765625
      r: 470.2132568359375
      t: 185.15008544921875
    charspan:
    - 0
    - 21
    page_no: 3
  text: 4 ANNOTATION CAMPAIGN
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/48
  hash: 11051011402111064878
  label: text
  orig: The annotation campaign was carried out in four phases. In phase one, we identified
    and prepared the data sources for annotation. In phase two, we determined the
    class labels and how annotations should be done on the documents in order to obtain
    maximum consistency. The latter was guided by a detailed requirement analysis
    and exhaustive experiments. In phase three, we trained the annotation staff and
    performed exams for quality assurance. In phase four,
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 85.38961791992188
      coord_origin: BOTTOMLEFT
      l: 317.0245056152344
      r: 559.7138061523438
      t: 160.93588256835938
    charspan:
    - 0
    - 457
    page_no: 3
  text: The annotation campaign was carried out in four phases. In phase one, we identified
    and prepared the data sources for annotation. In phase two, we determined the
    class labels and how annotations should be done on the documents in order to obtain
    maximum consistency. The latter was guided by a detailed requirement analysis
    and exhaustive experiments. In phase three, we trained the annotation staff and
    performed exams for quality assurance. In phase four,
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/49
  hash: 6768525952307611424
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph\
    \ Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0101318359375
      coord_origin: BOTTOMLEFT
      l: 53.345272064208984
      r: 558.5491943359375
      t: 732.1525268554688
    charspan:
    - 0
    - 130
    page_no: 4
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph\
    \ Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/50
  hash: 5520931533029632037
  label: caption
  orig: ''
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 498.30108642578125
      coord_origin: BOTTOMLEFT
      l: 98.96420288085938
      r: 512.7739868164062
      t: 654.1231689453125
    charspan:
    - 0
    - 0
    page_no: 4
  text: ''
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/51
  hash: 10610193690990616567
  label: text
  orig: we distributed the annotation workload and performed continuous quality controls.
    Phase one and two required a small team of experts only. For phases three and
    four, a group of 40 dedicated annotators were assembled and supervised.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 116.45683288574219
      coord_origin: BOTTOMLEFT
      l: 52.954681396484375
      r: 294.3648681640625
      t: 158.3203887939453
    charspan:
    - 0
    - 231
    page_no: 4
  text: we distributed the annotation workload and performed continuous quality controls.
    Phase one and two required a small team of experts only. For phases three and
    four, a group of 40 dedicated annotators were assembled and supervised.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/52
  hash: 8450678124529756923
  label: text
  orig: 'Phase 1: Data selection and preparation. Our inclusion criteria for documents
    were described in Section 3. A large effort went into ensuring that all documents
    are free to use. The data sources'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 83.57982635498047
      coord_origin: BOTTOMLEFT
      l: 53.368797302246094
      r: 295.5584411621094
      t: 114.14925384521484
    charspan:
    - 0
    - 193
    page_no: 4
  text: 'Phase 1: Data selection and preparation. Our inclusion criteria for documents
    were described in Section 3. A large effort went into ensuring that all documents
    are free to use. The data sources'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/53
  hash: 12151724778915504838
  label: text
  orig: include publication repositories such as arXiv$^{3}$, government offices,
    company websites as well as data directory services for financial reports and
    patents. Scanned documents were excluded wherever possible because they can be
    rotated or skewed. This would not allow us to perform annotation with rectangular
    bounding-boxes and therefore complicate the annotation process.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 416.48919677734375
      coord_origin: BOTTOMLEFT
      l: 317.2582702636719
      r: 559.1853637695312
      t: 481.0997619628906
    charspan:
    - 0
    - 376
    page_no: 4
  text: include publication repositories such as arXiv$^{3}$, government offices,
    company websites as well as data directory services for financial reports and
    patents. Scanned documents were excluded wherever possible because they can be
    rotated or skewed. This would not allow us to perform annotation with rectangular
    bounding-boxes and therefore complicate the annotation process.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/54
  hash: 15887983992023577324
  label: text
  orig: Preparation work included uploading and parsing the sourced PDF documents
    in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides
    a visual annotation interface and allows for dataset inspection and analysis.
    The annotation interface of CCS is shown in Figure 3. The desired balance of pages
    between the different document categories was achieved by selective subsampling
    of pages with certain desired properties. For example, we made sure to include
    the title page of each document and bias the remaining page selection to those
    with figures or tables. The latter was achieved by leveraging pre-trained object
    detection models from PubLayNet, which helped us estimate how many figures and
    tables a given page contains.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 284.9187316894531
      coord_origin: BOTTOMLEFT
      l: 317.0777587890625
      r: 559.7130737304688
      t: 415.02398681640625
    charspan:
    - 0
    - 746
    page_no: 4
  text: Preparation work included uploading and parsing the sourced PDF documents
    in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides
    a visual annotation interface and allows for dataset inspection and analysis.
    The annotation interface of CCS is shown in Figure 3. The desired balance of pages
    between the different document categories was achieved by selective subsampling
    of pages with certain desired properties. For example, we made sure to include
    the title page of each document and bias the remaining page selection to those
    with figures or tables. The latter was achieved by leveraging pre-trained object
    detection models from PubLayNet, which helped us estimate how many figures and
    tables a given page contains.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/55
  hash: 11975880209884411763
  label: text
  orig: 'Phase 2: Label selection and guideline. We reviewed the collected documents
    and identified the most common structural features they exhibit. This was achieved
    by identifying recurrent layout elements and lead us to the definition of 11 distinct
    class labels. These 11 class labels are Caption , Footnote , Formula , List-item
    , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title
    . Critical factors that were considered for the choice of these class labels were
    (1) the overall occurrence of the label, (2) the specificity of the label, (3)
    recognisability on a single page (i.e. no need for context from previous or next
    page) and (4) overall coverage of the page. Specificity ensures that the choice
    of label is not ambiguous, while coverage ensures that all meaningful items on
    a page can be annotated. We refrained from class labels that are very specific
    to a document category, such as Abstract in the Scientific Articles category.
    We also avoided class labels that are tightly linked to the semantics of the text.
    Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable
    by discriminating on'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 98.9438247680664
      coord_origin: BOTTOMLEFT
      l: 316.9024963378906
      r: 559.7176513671875
      t: 283.8972473144531
    charspan:
    - 0
    - 1159
    page_no: 4
  text: 'Phase 2: Label selection and guideline. We reviewed the collected documents
    and identified the most common structural features they exhibit. This was achieved
    by identifying recurrent layout elements and lead us to the definition of 11 distinct
    class labels. These 11 class labels are Caption , Footnote , Formula , List-item
    , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title
    . Critical factors that were considered for the choice of these class labels were
    (1) the overall occurrence of the label, (2) the specificity of the label, (3)
    recognisability on a single page (i.e. no need for context from previous or next
    page) and (4) overall coverage of the page. Specificity ensures that the choice
    of label is not ambiguous, while coverage ensures that all meaningful items on
    a page can be annotated. We refrained from class labels that are very specific
    to a document category, such as Abstract in the Scientific Articles category.
    We also avoided class labels that are tightly linked to the semantics of the text.
    Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable
    by discriminating on'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/56
  hash: 723840717012406728
  label: footnote
  orig: $^{3}$https://arxiv.org/
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 82.5821304321289
      coord_origin: BOTTOMLEFT
      l: 317.7030029296875
      r: 369.40142822265625
      t: 90.54422760009766
    charspan:
    - 0
    - 24
    page_no: 4
  text: $^{3}$https://arxiv.org/
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/57
  hash: 15020658425504633198
  label: page_header
  orig: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0143432617188
      coord_origin: BOTTOMLEFT
      l: 53.456207275390625
      r: 347.07373046875
      t: 732.0245361328125
    charspan:
    - 0
    - 71
    page_no: 5
  text: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/58
  hash: 17688098678887076514
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0404663085938
      coord_origin: BOTTOMLEFT
      l: 365.2621765136719
      r: 558.9374389648438
      t: 731.9317626953125
    charspan:
    - 0
    - 48
    page_no: 5
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/59
  hash: 938373213925944417
  label: text
  orig: the textual content of an element, which goes beyond visual layout recognition,
    in particular outside the Scientific Articles category.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 684.8170166015625
      coord_origin: BOTTOMLEFT
      l: 53.24338912963867
      r: 294.04541015625
      t: 705.5283813476562
    charspan:
    - 0
    - 135
    page_no: 5
  text: the textual content of an element, which goes beyond visual layout recognition,
    in particular outside the Scientific Articles category.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/60
  hash: 11375379645979730878
  label: text
  orig: At first sight, the task of visual document-layout interpretation appears
    intuitive enough to obtain plausible annotations in most cases. However, during
    early trial-runs in the core team, we observed many cases in which annotators
    use different annotation styles, especially for documents with challenging layouts.
    For example, if a figure is presented with subfigures, one annotator might draw
    a single figure bounding-box, while another might annotate each subfigure separately.
    The same applies for lists, where one might annotate all list items in one block
    or each list item separately. In essence, we observed that challenging layouts
    would be annotated in different but plausible ways. To illustrate this, we show
    in Figure 4 multiple examples of plausible but inconsistent annotations on the
    same pages.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 542.8159790039062
      coord_origin: BOTTOMLEFT
      l: 53.124725341796875
      r: 295.5592346191406
      t: 683.8748168945312
    charspan:
    - 0
    - 812
    page_no: 5
  text: At first sight, the task of visual document-layout interpretation appears
    intuitive enough to obtain plausible annotations in most cases. However, during
    early trial-runs in the core team, we observed many cases in which annotators
    use different annotation styles, especially for documents with challenging layouts.
    For example, if a figure is presented with subfigures, one annotator might draw
    a single figure bounding-box, while another might annotate each subfigure separately.
    The same applies for lists, where one might annotate all list items in one block
    or each list item separately. In essence, we observed that challenging layouts
    would be annotated in different but plausible ways. To illustrate this, we show
    in Figure 4 multiple examples of plausible but inconsistent annotations on the
    same pages.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/61
  hash: 9627588927681567008
  label: text
  orig: 'Obviously, this inconsistency in annotations is not desirable for datasets
    which are intended to be used for model training. To minimise these inconsistencies,
    we created a detailed annotation guideline. While perfect consistency across 40
    annotation staff members is clearly not possible to achieve, we saw a huge improvement
    in annotation consistency after the introduction of our annotation guideline.
    A few selected, non-trivial highlights of the guideline are:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 455.16583251953125
      coord_origin: BOTTOMLEFT
      l: 53.339271545410156
      r: 295.56005859375
      t: 541.1383666992188
    charspan:
    - 0
    - 465
    page_no: 5
  text: 'Obviously, this inconsistency in annotations is not desirable for datasets
    which are intended to be used for model training. To minimise these inconsistencies,
    we created a detailed annotation guideline. While perfect consistency across 40
    annotation staff members is clearly not possible to achieve, we saw a huge improvement
    in annotation consistency after the introduction of our annotation guideline.
    A few selected, non-trivial highlights of the guideline are:'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/62
  hash: 5251173547193772936
  label: list_item
  orig: (1) Every list-item is an individual object instance with class label List-item
    . This definition is different from PubLayNet and DocBank, where all list-items
    are grouped together into one List object.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 402.13092041015625
      coord_origin: BOTTOMLEFT
      l: 64.39098358154297
      r: 294.42474365234375
      t: 444.29510498046875
    charspan:
    - 0
    - 202
    page_no: 5
  text: (1) Every list-item is an individual object instance with class label List-item
    . This definition is different from PubLayNet and DocBank, where all list-items
    are grouped together into one List object.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/63
  hash: 4202075218951637034
  label: list_item
  orig: (2) A List-item is a paragraph with hanging indentation. Singleline elements
    can qualify as List-item if the neighbour elements expose hanging indentation.
    Bullet or enumeration symbols are not a requirement.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 358.39984130859375
      coord_origin: BOTTOMLEFT
      l: 64.31100463867188
      r: 295.563720703125
      t: 400.2758483886719
    charspan:
    - 0
    - 208
    page_no: 5
  text: (2) A List-item is a paragraph with hanging indentation. Singleline elements
    can qualify as List-item if the neighbour elements expose hanging indentation.
    Bullet or enumeration symbols are not a requirement.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/64
  hash: 1780046845976491258
  label: list_item
  orig: (3) For every Caption , there must be exactly one corresponding Picture or
    Table .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 336.4728698730469
      coord_origin: BOTTOMLEFT
      l: 64.26787567138672
      r: 294.60943603515625
      t: 356.2404479980469
    charspan:
    - 0
    - 82
    page_no: 5
  text: (3) For every Caption , there must be exactly one corresponding Picture or
    Table .
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/65
  hash: 3653862969821232020
  label: list_item
  orig: (4) Connected sub-pictures are grouped together in one Picture object.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 314.5648193359375
      coord_origin: BOTTOMLEFT
      l: 64.2632064819336
      r: 294.7487487792969
      t: 334.179443359375
    charspan:
    - 0
    - 70
    page_no: 5
  text: (4) Connected sub-pictures are grouped together in one Picture object.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/66
  hash: 5448053117976841193
  label: list_item
  orig: (5) Formula numbers are included in a Formula object.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 303.59686279296875
      coord_origin: BOTTOMLEFT
      l: 63.9930305480957
      r: 264.5057067871094
      t: 312.8252868652344
    charspan:
    - 0
    - 53
    page_no: 5
  text: (5) Formula numbers are included in a Formula object.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/67
  hash: 5907142507865067888
  label: list_item
  orig: (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph
    is not considered a Section-header , unless it appears exclusively on its own
    line.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 270.048095703125
      coord_origin: BOTTOMLEFT
      l: 64.07823181152344
      r: 295.0240783691406
      t: 301.5160827636719
    charspan:
    - 0
    - 160
    page_no: 5
  text: (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph
    is not considered a Section-header , unless it appears exclusively on its own
    line.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/68
  hash: 13967274008596264343
  label: text
  orig: The complete annotation guideline is over 100 pages long and a detailed description
    is obviously out of scope for this paper. Nevertheless, it will be made publicly
    available alongside with DocLayNet for future reference.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 217.798828125
      coord_origin: BOTTOMLEFT
      l: 52.994422912597656
      r: 295.5625305175781
      t: 259.6097106933594
    charspan:
    - 0
    - 221
    page_no: 5
  text: The complete annotation guideline is over 100 pages long and a detailed description
    is obviously out of scope for this paper. Nevertheless, it will be made publicly
    available alongside with DocLayNet for future reference.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/69
  hash: 889554423716143140
  label: text
  orig: 'Phase 3: Training. After a first trial with a small group of people, we realised
    that providing the annotation guideline and a set of random practice pages did
    not yield the desired quality level for layout annotation. Therefore we prepared
    a subset of pages with two different complexity levels, each with a practice and
    an exam part. 974 pages were reference-annotated by one proficient core team member.
    Annotation staff were then given the task to annotate the same subsets (blinded
    from the reference). By comparing the annotations of each staff member with the
    reference annotations, we could quantify how closely their annotations matched
    the reference. Only after passing two exam levels with high annotation quality,
    staff were admitted into the production phase. Practice iterations'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 86.24749755859375
      coord_origin: BOTTOMLEFT
      l: 53.26631546020508
      r: 295.562255859375
      t: 215.95584106445312
    charspan:
    - 0
    - 792
    page_no: 5
  text: 'Phase 3: Training. After a first trial with a small group of people, we realised
    that providing the annotation guideline and a set of random practice pages did
    not yield the desired quality level for layout annotation. Therefore we prepared
    a subset of pages with two different complexity levels, each with a practice and
    an exam part. 974 pages were reference-annotated by one proficient core team member.
    Annotation staff were then given the task to annotate the same subsets (blinded
    from the reference). By comparing the annotations of each staff member with the
    reference annotations, we could quantify how closely their annotations matched
    the reference. Only after passing two exam levels with high annotation quality,
    staff were admitted into the production phase. Practice iterations'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/70
  hash: 15729892622341582110
  label: caption
  orig: 'Figure 4: Examples of plausible annotation alternatives for the same page.
    Criteria in our annotation guideline can resolve cases '
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 722.92333984375
      coord_origin: BOTTOMLEFT
      l: 53.30706024169922
      r: 558.4274291992188
      t: 732.1127319335938
    charspan:
    - 0
    - 130
    page_no: 6
  text: 'Figure 4: Examples of plausible annotation alternatives for the same page.
    Criteria in our annotation guideline can resolve cases '
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/71
  hash: 14428809639626034083
  label: text
  orig: were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially
    allocated annotators did not pass the bar.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 247.1688232421875
      coord_origin: BOTTOMLEFT
      l: 316.8349914550781
      r: 558.204345703125
      t: 266.81207275390625
    charspan:
    - 0
    - 123
    page_no: 5
  text: were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially
    allocated annotators did not pass the bar.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/72
  hash: 15056578085083744975
  label: text
  orig: 'Phase 4: Production annotation. The previously selected 80K pages were annotated
    with the defined 11 class labels by 32 annotators. This production phase took
    around three months to complete. All annotations were created online through CCS,
    which visualises the programmatic PDF text-cells as an overlay on the page. The
    page annotation are obtained by drawing rectangular bounding-boxes, as shown in
    Figure 3. With regard to the annotation practices, we implemented a few constraints
    and capabilities on the tooling level. First, we only allow non-overlapping, vertically
    oriented, rectangular boxes. For the large majority of documents, this constraint
    was sufficient and it speeds up the annotation considerably in comparison with
    arbitrary segmentation shapes. Second, annotator staff were not able to see each
    other''s annotations. This was enforced by design to avoid any bias in the annotation,
    which could skew the numbers of the inter-annotator agreement (see Table 1). We
    wanted'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 82.7375717163086
      coord_origin: BOTTOMLEFT
      l: 317.00592041015625
      r: 559.7149047851562
      t: 245.28392028808594
    charspan:
    - 0
    - 987
    page_no: 5
  text: 'Phase 4: Production annotation. The previously selected 80K pages were annotated
    with the defined 11 class labels by 32 annotators. This production phase took
    around three months to complete. All annotations were created online through CCS,
    which visualises the programmatic PDF text-cells as an overlay on the page. The
    page annotation are obtained by drawing rectangular bounding-boxes, as shown in
    Figure 3. With regard to the annotation practices, we implemented a few constraints
    and capabilities on the tooling level. First, we only allow non-overlapping, vertically
    oriented, rectangular boxes. For the large majority of documents, this constraint
    was sufficient and it speeds up the annotation considerably in comparison with
    arbitrary segmentation shapes. Second, annotator staff were not able to see each
    other''s annotations. This was enforced by design to avoid any bias in the annotation,
    which could skew the numbers of the inter-annotator agreement (see Table 1). We
    wanted'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/73
  hash: 2641059782471010186
  label: caption
  orig: 'Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks
    on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models
    with ResNet-50 or ResNet-101 backbone were trained based on the network architectures
    from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN
    3x), with default configurations. The YOLO implementation utilized was YOLOv5x6
    [13]. All models were initialised using pre-trained weights from the COCO 2017
    dataset.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 440.30438232421875
      coord_origin: BOTTOMLEFT
      l: 61.93328094482422
      r: 285.75616455078125
      t: 596.587158203125
    charspan:
    - 0
    - 584
    page_no: 6
  text: 'Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks
    on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models
    with ResNet-50 or ResNet-101 backbone were trained based on the network architectures
    from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN
    3x), with default configurations. The YOLO implementation utilized was YOLOv5x6
    [13]. All models were initialised using pre-trained weights from the COCO 2017
    dataset.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/74
  hash: 2611073847515650604
  label: text
  orig: to avoid this at any cost in order to have clear, unbiased baseline numbers
    for human document-layout annotation. Third, we introduced the feature of snapping
    boxes around text segments to obtain a pixel-accurate annotation and again reduce
    time and effort. The CCS annotation tool automatically shrinks every user-drawn
    box to the minimum bounding-box around the enclosed text-cells for all purely
    text-based segments, which excludes only Table and Picture . For the latter, we
    instructed annotation staff to minimise inclusion of surrounding whitespace while
    including all graphical lines. A downside of snapping boxes to enclosed text cells
    is that some wrongly parsed PDF pages cannot be annotated correctly and need to
    be skipped. Fourth, we established a way to flag pages as rejected for cases where
    no valid annotation according to the label guidelines could be achieved. Example
    cases for this would be PDF pages that render incorrectly or contain layouts that
    are impossible to capture with non-overlapping rectangles. Such rejected pages
    are not contained in the final dataset. With all these measures in place, experienced
    annotation staff managed to annotate a single page in a typical timeframe of 20s
    to 60s, depending on its complexity.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 214.2948760986328
      coord_origin: BOTTOMLEFT
      l: 53.25688552856445
      r: 295.5561218261719
      t: 421.4337158203125
    charspan:
    - 0
    - 1252
    page_no: 6
  text: to avoid this at any cost in order to have clear, unbiased baseline numbers
    for human document-layout annotation. Third, we introduced the feature of snapping
    boxes around text segments to obtain a pixel-accurate annotation and again reduce
    time and effort. The CCS annotation tool automatically shrinks every user-drawn
    box to the minimum bounding-box around the enclosed text-cells for all purely
    text-based segments, which excludes only Table and Picture . For the latter, we
    instructed annotation staff to minimise inclusion of surrounding whitespace while
    including all graphical lines. A downside of snapping boxes to enclosed text cells
    is that some wrongly parsed PDF pages cannot be annotated correctly and need to
    be skipped. Fourth, we established a way to flag pages as rejected for cases where
    no valid annotation according to the label guidelines could be achieved. Example
    cases for this would be PDF pages that render incorrectly or contain layouts that
    are impossible to capture with non-overlapping rectangles. Such rejected pages
    are not contained in the final dataset. With all these measures in place, experienced
    annotation staff managed to annotate a single page in a typical timeframe of 20s
    to 60s, depending on its complexity.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/75
  hash: 19275708379815350
  label: section_header
  orig: 5 EXPERIMENTS
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 193.5609893798828
      coord_origin: BOTTOMLEFT
      l: 53.62337875366211
      r: 147.4853515625
      t: 203.87008666992188
    charspan:
    - 0
    - 13
    page_no: 6
  text: 5 EXPERIMENTS
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/76
  hash: 12611643145785449119
  label: caption
  orig: 'Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 722.9555053710938
      coord_origin: BOTTOMLEFT
      l: 53.35094451904297
      r: 347.0172424316406
      t: 732.038818359375
    charspan:
    - 0
    - 71
    page_no: 7
  text: 'Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/77
  hash: 10405449111938146973
  label: text
  orig: paper and leave the detailed evaluation of more recent methods mentioned in
    Section 2 for future work.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 388.6548156738281
      coord_origin: BOTTOMLEFT
      l: 317.2011413574219
      r: 558.2041625976562
      t: 408.8042297363281
    charspan:
    - 0
    - 102
    page_no: 6
  text: paper and leave the detailed evaluation of more recent methods mentioned in
    Section 2 for future work.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/78
  hash: 5970556147693056683
  label: text
  orig: In this section, we will present several aspects related to the performance
    of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate
    the quality of their predictions using mean average precision (mAP) with 10 overlaps
    that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are
    computed by leveraging the evaluation code provided by the COCO API [16].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 311.45587158203125
      coord_origin: BOTTOMLEFT
      l: 317.0830078125
      r: 558.4364013671875
      t: 386.632568359375
    charspan:
    - 0
    - 397
    page_no: 6
  text: In this section, we will present several aspects related to the performance
    of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate
    the quality of their predictions using mean average precision (mAP) with 10 overlaps
    that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are
    computed by leveraging the evaluation code provided by the COCO API [16].
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/79
  hash: 7797862272567426572
  label: section_header
  orig: Baselines for Object Detection
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 284.5037841796875
      coord_origin: BOTTOMLEFT
      l: 317.1941223144531
      r: 466.8532409667969
      t: 295.42913818359375
    charspan:
    - 0
    - 30
    page_no: 6
  text: Baselines for Object Detection
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/80
  hash: 7611035121604324850
  label: text
  orig: "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN\
    \ [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were\
    \ performed on RGB images with dimensions of 1025 \xD7 1025 pixels. For training,\
    \ we only used one annotation in case of redundantly annotated pages. As one can\
    \ observe, the variation in mAP between the models is rather low, but overall\
    \ between 6 and 10% lower than the mAP computed from the pairwise human annotations\
    \ on triple-annotated pages. This gives a good indication that the DocLayNet dataset\
    \ poses a worthwhile challenge for the research community to close the gap between\
    \ human recognition and ML approaches. It is interesting to see that Mask R-CNN\
    \ and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based\
    \ image segmentation derived from bounding-boxes does not help to obtain better\
    \ predictions. On the other hand, the more recent Yolov5x model does very well\
    \ and even out-performs humans on selected labels such as Text , Table and Picture\
    \ . This is not entirely surprising, as Text , Table and Picture are abundant\
    \ and the most visually distinctive in a document."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 85.2998275756836
      coord_origin: BOTTOMLEFT
      l: 317.0144348144531
      r: 558.7822875976562
      t: 280.8944396972656
    charspan:
    - 0
    - 1146
    page_no: 6
  text: "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN\
    \ [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were\
    \ performed on RGB images with dimensions of 1025 \xD7 1025 pixels. For training,\
    \ we only used one annotation in case of redundantly annotated pages. As one can\
    \ observe, the variation in mAP between the models is rather low, but overall\
    \ between 6 and 10% lower than the mAP computed from the pairwise human annotations\
    \ on triple-annotated pages. This gives a good indication that the DocLayNet dataset\
    \ poses a worthwhile challenge for the research community to close the gap between\
    \ human recognition and ML approaches. It is interesting to see that Mask R-CNN\
    \ and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based\
    \ image segmentation derived from bounding-boxes does not help to obtain better\
    \ predictions. On the other hand, the more recent Yolov5x model does very well\
    \ and even out-performs humans on selected labels such as Text , Table and Picture\
    \ . This is not entirely surprising, as Text , Table and Picture are abundant\
    \ and the most visually distinctive in a document."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/81
  hash: 4524736109232879114
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0802001953125
      coord_origin: BOTTOMLEFT
      l: 365.1936950683594
      r: 558.7797241210938
      t: 731.8773803710938
    charspan:
    - 0
    - 48
    page_no: 7
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/82
  hash: 8640149219266946286
  label: caption
  orig: 'Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained
    on DocLayNet with different class label sets. The reduced label sets were obtained
    by either down-mapping or '
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 496.419189453125
      coord_origin: BOTTOMLEFT
      l: 80.5073471069336
      r: 267.3428649902344
      t: 640.9814453125
    charspan:
    - 0
    - 189
    page_no: 7
  text: 'Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained
    on DocLayNet with different class label sets. The reduced label sets were obtained
    by either down-mapping or '
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/83
  hash: 6812192561276511295
  label: section_header
  orig: Learning Curve
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 461.592041015625
      coord_origin: BOTTOMLEFT
      l: 53.446834564208984
      r: 131.05624389648438
      t: 472.6955871582031
    charspan:
    - 0
    - 14
    page_no: 7
  text: Learning Curve
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/84
  hash: 5745526209173602420
  label: text
  orig: One of the fundamental questions related to any dataset is if it is "large
    enough". To answer this question for DocLayNet, we performed a data ablation study
    in which we evaluated a Mask R-CNN model trained on increasing fractions of the
    DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in
    the beginning and eventually levels out. To estimate the error-bar on the metrics,
    we ran the training five times on the entire data-set. This resulted in a 1% error-bar,
    depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the
    exact same data-points, but with a logarithmic scale on the x-axis. As is expected,
    the mAP score increases linearly as a function of the data-size in the inset.
    The curve ultimately flattens out between the 80% and 100% mark, with the 80%
    mark falling within the error-bars of the 100% mark. This provides a good indication
    that the model would not improve significantly by yet increasing the data size.
    Rather, it would probably benefit more from improved data consistency (as discussed
    in Section 3), data augmentation methods [23], or the addition of more document
    categories and styles.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 262.38037109375
      coord_origin: BOTTOMLEFT
      l: 52.78499984741211
      r: 295.558349609375
      t: 457.72955322265625
    charspan:
    - 0
    - 1157
    page_no: 7
  text: One of the fundamental questions related to any dataset is if it is "large
    enough". To answer this question for DocLayNet, we performed a data ablation study
    in which we evaluated a Mask R-CNN model trained on increasing fractions of the
    DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in
    the beginning and eventually levels out. To estimate the error-bar on the metrics,
    we ran the training five times on the entire data-set. This resulted in a 1% error-bar,
    depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the
    exact same data-points, but with a logarithmic scale on the x-axis. As is expected,
    the mAP score increases linearly as a function of the data-size in the inset.
    The curve ultimately flattens out between the 80% and 100% mark, with the 80%
    mark falling within the error-bars of the 100% mark. This provides a good indication
    that the model would not improve significantly by yet increasing the data size.
    Rather, it would probably benefit more from improved data consistency (as discussed
    in Section 3), data augmentation methods [23], or the addition of more document
    categories and styles.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/85
  hash: 7824280854281589640
  label: section_header
  orig: Impact of Class Labels
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 239.1809844970703
      coord_origin: BOTTOMLEFT
      l: 53.37664794921875
      r: 164.3289794921875
      t: 250.044677734375
    charspan:
    - 0
    - 22
    page_no: 7
  text: Impact of Class Labels
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/86
  hash: 17522119297822048539
  label: text
  orig: "The choice and number of labels can have a significant effect on the overall\
    \ model performance. Since PubLayNet, DocBank and DocLayNet all have different\
    \ label sets, it is of particular interest to understand and quantify this influence\
    \ of the label set on the model performance. We investigate this by either down-mapping\
    \ labels into more common ones (e.g. Caption \u2192 Text ) or excluding them from\
    \ the annotations entirely. Furthermore, it must be stressed that all mappings\
    \ and exclusions were performed on the data before model training. In Table 3,\
    \ we present the mAP scores for a Mask R-CNN R50 network on different label sets.\
    \ Where a label is down-mapped, we show its corresponding label, otherwise it\
    \ was excluded. We present three different label sets, with 6, 5 and 4 different\
    \ labels respectively. The set of 5 labels contains the same labels as PubLayNet.\
    \ However, due to the different definition of"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 83.39567565917969
      coord_origin: BOTTOMLEFT
      l: 53.06760787963867
      r: 295.5567932128906
      t: 235.12689208984375
    charspan:
    - 0
    - 910
    page_no: 7
  text: "The choice and number of labels can have a significant effect on the overall\
    \ model performance. Since PubLayNet, DocBank and DocLayNet all have different\
    \ label sets, it is of particular interest to understand and quantify this influence\
    \ of the label set on the model performance. We investigate this by either down-mapping\
    \ labels into more common ones (e.g. Caption \u2192 Text ) or excluding them from\
    \ the annotations entirely. Furthermore, it must be stressed that all mappings\
    \ and exclusions were performed on the data before model training. In Table 3,\
    \ we present the mAP scores for a Mask R-CNN R50 network on different label sets.\
    \ Where a label is down-mapped, we show its corresponding label, otherwise it\
    \ was excluded. We present three different label sets, with 6, 5 and 4 different\
    \ labels respectively. The set of 5 labels contains the same labels as PubLayNet.\
    \ However, due to the different definition of"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/87
  hash: 12736595303563933946
  label: caption
  orig: 'Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise
    split for different label sets. Naive page-wise '
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 485.2873840332031
      coord_origin: BOTTOMLEFT
      l: 353.065185546875
      r: 523.3069458007812
      t: 641.25341796875
    charspan:
    - 0
    - 130
    page_no: 7
  text: 'Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise
    split for different label sets. Naive page-wise '
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/88
  hash: 7783869837125225
  label: text
  orig: lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items),
    the label set of size 4 is the closest to PubLayNet, in the assumption that the
    List is down-mapped to Text in PubLayNet. The results in Table 3 show that the
    prediction accuracy on the remaining class labels does not change significantly
    when other classes are merged into them. The overall macro-average improves by
    around 5%, in particular when Page-footer and Page-header are excluded.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 375.50982666015625
      coord_origin: BOTTOMLEFT
      l: 317.03326416015625
      r: 559.5849609375
      t: 460.6855163574219
    charspan:
    - 0
    - 469
    page_no: 7
  text: lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items),
    the label set of size 4 is the closest to PubLayNet, in the assumption that the
    List is down-mapped to Text in PubLayNet. The results in Table 3 show that the
    prediction accuracy on the remaining class labels does not change significantly
    when other classes are merged into them. The overall macro-average improves by
    around 5%, in particular when Page-footer and Page-header are excluded.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/89
  hash: 5117058535300881242
  label: section_header
  orig: Impact of Document Split in Train and Test Set
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 351.4896545410156
      coord_origin: BOTTOMLEFT
      l: 317.4661865234375
      r: 549.860595703125
      t: 362.8900451660156
    charspan:
    - 0
    - 46
    page_no: 7
  text: Impact of Document Split in Train and Test Set
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/90
  hash: 1569991188631703948
  label: text
  orig: "Many documents in DocLayNet have a unique styling. In order to avoid overfitting\
    \ on a particular style, we have split the train-, test- and validation-sets of\
    \ DocLayNet on document boundaries, i.e. every document contributes pages to only\
    \ one set. To the best of our knowledge, this was not considered in PubLayNet\
    \ or DocBank. To quantify how this affects model performance, we trained and evaluated\
    \ a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test-\
    \ and validation-sets were obtained by a randomised draw over the individual pages.\
    \ As can be seen in Table 4, the difference in model performance is surprisingly\
    \ large: pagewise splitting gains \u02DC 10% in mAP over the document-wise splitting.\
    \ Thus, random page-wise splitting of DocLayNet can easily lead to accidental\
    \ overestimation of model performance and should be avoided."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 196.5628204345703
      coord_origin: BOTTOMLEFT
      l: 316.9546813964844
      r: 559.7138061523438
      t: 348.10198974609375
    charspan:
    - 0
    - 852
    page_no: 7
  text: "Many documents in DocLayNet have a unique styling. In order to avoid overfitting\
    \ on a particular style, we have split the train-, test- and validation-sets of\
    \ DocLayNet on document boundaries, i.e. every document contributes pages to only\
    \ one set. To the best of our knowledge, this was not considered in PubLayNet\
    \ or DocBank. To quantify how this affects model performance, we trained and evaluated\
    \ a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test-\
    \ and validation-sets were obtained by a randomised draw over the individual pages.\
    \ As can be seen in Table 4, the difference in model performance is surprisingly\
    \ large: pagewise splitting gains \u02DC 10% in mAP over the document-wise splitting.\
    \ Thus, random page-wise splitting of DocLayNet can easily lead to accidental\
    \ overestimation of model performance and should be avoided."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/91
  hash: 16424003151594388576
  label: section_header
  orig: Dataset Comparison
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 173.20875549316406
      coord_origin: BOTTOMLEFT
      l: 317.3337707519531
      r: 418.5477600097656
      t: 183.94322204589844
    charspan:
    - 0
    - 18
    page_no: 7
  text: Dataset Comparison
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/92
  hash: 3914983503582730759
  label: text
  orig: Throughout this paper, we claim that DocLayNet's wider variety of document
    layouts leads to more robust layout detection models. In Table 5, we provide evidence
    for that. We trained models on each of the available datasets (PubLayNet, DocBank
    and DocLayNet) and evaluated them on the test sets of the other datasets. Due
    to the different label sets and annotation styles, a direct comparison is not
    possible. Hence, we focussed on the common labels among the datasets. Between
    PubLayNet and DocLayNet, these are Picture ,
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 83.24566650390625
      coord_origin: BOTTOMLEFT
      l: 316.7283935546875
      r: 559.1881713867188
      t: 168.86700439453125
    charspan:
    - 0
    - 521
    page_no: 7
  text: Throughout this paper, we claim that DocLayNet's wider variety of document
    layouts leads to more robust layout detection models. In Table 5, we provide evidence
    for that. We trained models on each of the available datasets (PubLayNet, DocBank
    and DocLayNet) and evaluated them on the test sets of the other datasets. Due
    to the different label sets and annotation styles, a direct comparison is not
    possible. Hence, we focussed on the common labels among the datasets. Between
    PubLayNet and DocLayNet, these are Picture ,
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/93
  hash: 1407046376659880848
  label: caption
  orig: 'Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network
    across the PubLayNet, DocBank & DocLayNet data-sets. By evaluating on common label
    classes of each dataset, we observe that the DocLayNet-trained model has much
    less pronounced variations in performance across all datasets.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 452.12615966796875
      coord_origin: BOTTOMLEFT
      l: 72.87370300292969
      r: 274.87945556640625
      t: 619.3699951171875
    charspan:
    - 0
    - 573
    page_no: 8
  text: 'Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network
    across the PubLayNet, DocBank & DocLayNet data-sets. By evaluating on common label
    classes of each dataset, we observe that the DocLayNet-trained model has much
    less pronounced variations in performance across all datasets.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/94
  hash: 908797690688183444
  label: text
  orig: Section-header , Table and Text . Before training, we either mapped or excluded
    DocLayNet's other labels as specified in table 3, and also PubLayNet's List to
    Text . Note that the different clustering of lists (by list-element vs. whole
    list objects) naturally decreases the mAP score for Text .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 348.85986328125
      coord_origin: BOTTOMLEFT
      l: 53.279537200927734
      r: 294.6396789550781
      t: 401.5162658691406
    charspan:
    - 0
    - 295
    page_no: 8
  text: Section-header , Table and Text . Before training, we either mapped or excluded
    DocLayNet's other labels as specified in table 3, and also PubLayNet's List to
    Text . Note that the different clustering of lists (by list-element vs. whole
    list objects) naturally decreases the mAP score for Text .
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/95
  hash: 10214562574915474626
  label: text
  orig: For comparison of DocBank with DocLayNet, we trained only on Picture and Table
    clusters of each dataset. We had to exclude Text because successive paragraphs
    are often grouped together into a single object in DocBank. This paragraph grouping
    is incompatible with the individual paragraphs of DocLayNet. As can be seen in
    Table 5, DocLayNet trained models yield better performance compared to the previous
    datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform
    very well on their own test set, but have a much lower performance on the foreign
    datasets. While this also applies to DocLayNet, the difference is far less pronounced.
    Thus we conclude that DocLayNet trained models are overall more robust and will
    produce better results for challenging, unseen layouts.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 205.98951721191406
      coord_origin: BOTTOMLEFT
      l: 53.04817581176758
      r: 295.55908203125
      t: 346.9607849121094
    charspan:
    - 0
    - 793
    page_no: 8
  text: For comparison of DocBank with DocLayNet, we trained only on Picture and Table
    clusters of each dataset. We had to exclude Text because successive paragraphs
    are often grouped together into a single object in DocBank. This paragraph grouping
    is incompatible with the individual paragraphs of DocLayNet. As can be seen in
    Table 5, DocLayNet trained models yield better performance compared to the previous
    datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform
    very well on their own test set, but have a much lower performance on the foreign
    datasets. While this also applies to DocLayNet, the difference is far less pronounced.
    Thus we conclude that DocLayNet trained models are overall more robust and will
    produce better results for challenging, unseen layouts.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/96
  hash: 13986119087538501170
  label: section_header
  orig: Example Predictions
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 176.33340454101562
      coord_origin: BOTTOMLEFT
      l: 53.05388259887695
      r: 156.02235412597656
      t: 187.29098510742188
    charspan:
    - 0
    - 19
    page_no: 8
  text: Example Predictions
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/97
  hash: 1038155047615801598
  label: text
  orig: To conclude this section, we illustrate the quality of layout predictions
    one can expect from DocLayNet-trained models by providing a selection of examples
    without any further post-processing applied. Figure 6 shows selected layout predictions
    on pages from the test-set of DocLayNet. Results look decent in general across
    document categories, however one can also observe mistakes such as overlapping
    clusters of different classes, or entirely missing boxes due to low confidence.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 86.64982604980469
      coord_origin: BOTTOMLEFT
      l: 53.07720184326172
      r: 295.5584411621094
      t: 172.26492309570312
    charspan:
    - 0
    - 481
    page_no: 8
  text: To conclude this section, we illustrate the quality of layout predictions
    one can expect from DocLayNet-trained models by providing a selection of examples
    without any further post-processing applied. Figure 6 shows selected layout predictions
    on pages from the test-set of DocLayNet. Results look decent in general across
    document categories, however one can also observe mistakes such as overlapping
    clusters of different classes, or entirely missing boxes due to low confidence.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/98
  hash: 8801089031972856173
  label: section_header
  orig: 6 CONCLUSION
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 695.8309936523438
      coord_origin: BOTTOMLEFT
      l: 317.4961853027344
      r: 405.7296142578125
      t: 706.4700317382812
    charspan:
    - 0
    - 12
    page_no: 8
  text: 6 CONCLUSION
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/99
  hash: 15710626894768820561
  label: text
  orig: In this paper, we presented the DocLayNet dataset. It provides the document
    conversion and layout analysis research community a new and challenging dataset
    to improve and fine-tune novel ML methods on. In contrast to many other datasets,
    DocLayNet was created by human annotation in order to obtain reliable layout ground-truth
    on a wide variety of publication- and typesettingstyles. Including a large proportion
    of documents outside the scientific publishing domain adds significant value in
    this respect.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 605.4117431640625
      coord_origin: BOTTOMLEFT
      l: 317.0487976074219
      r: 559.7137451171875
      t: 691.6207275390625
    charspan:
    - 0
    - 507
    page_no: 8
  text: In this paper, we presented the DocLayNet dataset. It provides the document
    conversion and layout analysis research community a new and challenging dataset
    to improve and fine-tune novel ML methods on. In contrast to many other datasets,
    DocLayNet was created by human annotation in order to obtain reliable layout ground-truth
    on a wide variety of publication- and typesettingstyles. Including a large proportion
    of documents outside the scientific publishing domain adds significant value in
    this respect.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/100
  hash: 5999203225419292280
  label: text
  orig: To date, there is still a significant gap between human and ML accuracy on
    the layout interpretation task, and we hope that this work will inspire the research
    community to close that gap.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 474.2935791015625
      coord_origin: BOTTOMLEFT
      l: 317.1865234375
      r: 558.6325073242188
      t: 505.4895324707031
    charspan:
    - 0
    - 188
    page_no: 8
  text: To date, there is still a significant gap between human and ML accuracy on
    the layout interpretation task, and we hope that this work will inspire the research
    community to close that gap.
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/101
  hash: 4445410344359338123
  label: section_header
  orig: REFERENCES
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 446.5990295410156
      coord_origin: BOTTOMLEFT
      l: 317.4455871582031
      r: 387.5806579589844
      t: 457.4013366699219
    charspan:
    - 0
    - 10
    page_no: 8
  text: REFERENCES
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/102
  hash: 16616106884325138631
  label: list_item
  orig: "[1] Max G\xF6bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013\
    \ table competition. In 2013 12th International Conference on Document Analysis\
    \ and Recognition , pages 1449-1453, 2013."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 420.8371276855469
      coord_origin: BOTTOMLEFT
      l: 320.5848693847656
      r: 559.0187377929688
      t: 444.4063415527344
    charspan:
    - 0
    - 191
    page_no: 8
  text: "[1] Max G\xF6bel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013\
    \ table competition. In 2013 12th International Conference on Document Analysis\
    \ and Recognition , pages 1449-1453, 2013."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/103
  hash: 16787040176255257341
  label: list_item
  orig: '[2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher.
    Icdar2017 competition on recognition of documents with complex layouts rdcl2017.
    In 2017 14th IAPR International Conference on Document Analysis and Recognition
    (ICDAR) , volume 01, pages 1404-1410, 2017.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 388.9571228027344
      coord_origin: BOTTOMLEFT
      l: 320.76806640625
      r: 559.7276000976562
      t: 420.2254333496094
    charspan:
    - 0
    - 279
    page_no: 8
  text: '[2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher.
    Icdar2017 competition on recognition of documents with complex layouts rdcl2017.
    In 2017 14th IAPR International Conference on Document Analysis and Recognition
    (ICDAR) , volume 01, pages 1404-1410, 2017.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/104
  hash: 16229494543393695243
  label: list_item
  orig: "[3] Herv\xE9 D\xE9jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang,\
    \ Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection\
    \ and Recognition (cTDaR), April 2019. http://sac.founderit.com/."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 364.88128662109375
      coord_origin: BOTTOMLEFT
      l: 320.58111572265625
      r: 558.4269409179688
      t: 388.028076171875
    charspan:
    - 0
    - 213
    page_no: 8
  text: "[3] Herv\xE9 D\xE9jean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang,\
    \ Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection\
    \ and Recognition (cTDaR), April 2019. http://sac.founderit.com/."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/105
  hash: 15423145939859734104
  label: list_item
  orig: '[4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on
    scientific literature parsing. In Proceedings of the International Conference
    on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag,
    sep 2021.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 333.173095703125
      coord_origin: BOTTOMLEFT
      l: 320.72210693359375
      r: 559.3787231445312
      t: 364.17962646484375
    charspan:
    - 0
    - 251
    page_no: 8
  text: '[4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on
    scientific literature parsing. In Proceedings of the International Conference
    on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag,
    sep 2021.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/106
  hash: 5249151387680038785
  label: list_item
  orig: '[5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang
    Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis:
    not dead yet. International Journal on Document Analysis and Recognition (IJDAR)
    , pages 1-11, 01 2022.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 300.9960021972656
      coord_origin: BOTTOMLEFT
      l: 320.47723388671875
      r: 559.2555541992188
      t: 332.2057800292969
    charspan:
    - 0
    - 261
    page_no: 8
  text: '[5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang
    Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis:
    not dead yet. International Journal on Document Analysis and Recognition (IJDAR)
    , pages 1-11, 01 2022.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/107
  hash: 16511389590086473870
  label: list_item
  orig: '[6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest
    dataset ever for document layout analysis. In Proceedings of the International
    Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep
    2019.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 277.3751220703125
      coord_origin: BOTTOMLEFT
      l: 320.7210998535156
      r: 558.6044921875
      t: 300.1542053222656
    charspan:
    - 0
    - 235
    page_no: 8
  text: '[6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest
    dataset ever for document layout analysis. In Proceedings of the International
    Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep
    2019.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/108
  hash: 5841239213590061604
  label: list_item
  orig: '[7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li,
    and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings
    of the 28th International Conference on Computational Linguistics , COLING, pages
    949-960. International Committee on Computational Linguistics, dec 2020.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 237.53111267089844
      coord_origin: BOTTOMLEFT
      l: 320.7048034667969
      r: 559.0962524414062
      t: 276.57550048828125
    charspan:
    - 0
    - 316
    page_no: 8
  text: '[7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li,
    and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings
    of the 28th International Conference on Computational Linguistics , COLING, pages
    949-960. International Committee on Computational Linguistics, dec 2020.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/109
  hash: 11745041684012725305
  label: list_item
  orig: '[8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction
    from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC
    , 2016.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 213.6141357421875
      coord_origin: BOTTOMLEFT
      l: 320.6175537109375
      r: 558.9022216796875
      t: 236.84490966796875
    charspan:
    - 0
    - 172
    page_no: 8
  text: '[8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction
    from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC
    , 2016.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/110
  hash: 8213734949810000799
  label: list_item
  orig: '[9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587.
    IEEE Computer Society, jun 2014.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 181.74110412597656
      coord_origin: BOTTOMLEFT
      l: 320.695556640625
      r: 559.2744750976562
      t: 212.77767944335938
    charspan:
    - 0
    - 271
    page_no: 8
  text: '[9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587.
    IEEE Computer Society, jun 2014.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/111
  hash: 4869209929442963000
  label: list_item
  orig: '[10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference
    on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 165.5072479248047
      coord_origin: BOTTOMLEFT
      l: 317.74908447265625
      r: 558.8585205078125
      t: 181.0753173828125
    charspan:
    - 0
    - 149
    page_no: 8
  text: '[10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference
    on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/112
  hash: 16420654594074141837
  label: list_item
  orig: '[11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. IEEE Transactions
    on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 141.8831329345703
      coord_origin: BOTTOMLEFT
      l: 317.71527099609375
      r: 558.4170532226562
      t: 164.63047790527344
    charspan:
    - 0
    - 227
    page_no: 8
  text: '[11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. IEEE Transactions
    on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/113
  hash: 453358893855311407
  label: list_item
  orig: "[12] Kaiming He, Georgia Gkioxari, Piotr Doll\xE1r, and Ross B. Girshick.\
    \ Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages\
    \ 2980-2988. IEEE Computer Society, Oct 2017."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 117.60646057128906
      coord_origin: BOTTOMLEFT
      l: 317.5010986328125
      r: 559.278076171875
      t: 141.50643920898438
    charspan:
    - 0
    - 192
    page_no: 8
  text: "[12] Kaiming He, Georgia Gkioxari, Piotr Doll\xE1r, and Ross B. Girshick.\
    \ Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages\
    \ 2980-2988. IEEE Computer Society, Oct 2017."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/114
  hash: 3393294654140361785
  label: list_item
  orig: '[13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012,
    TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing,
    tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana,
    Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 86.09910583496094
      coord_origin: BOTTOMLEFT
      l: 317.4837341308594
      r: 559.0487670898438
      t: 116.94155883789062
    charspan:
    - 0
    - 305
    page_no: 8
  text: '[13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012,
    TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing,
    tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana,
    Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/115
  hash: 13779849536941554365
  label: page_header
  orig: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 722.9329223632812
      coord_origin: BOTTOMLEFT
      l: 53.55940246582031
      r: 347.0838623046875
      t: 731.9924926757812
    charspan:
    - 0
    - 71
    page_no: 9
  text: 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/116
  hash: 16675011465179482522
  label: page_header
  orig: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 723.0497436523438
      coord_origin: BOTTOMLEFT
      l: 365.1275329589844
      r: 558.905029296875
      t: 731.96435546875
    charspan:
    - 0
    - 48
    page_no: 9
  text: "KDD \u201922, August 14-18, 2022, Washington, DC, USA"
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/117
  hash: 12604501010340547619
  label: caption
  orig: 'Figure 6: Example layout predictions on selected pages from the DocLayNet
    test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show
    accurate list-item and paragraph '
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 226.54010009765625
      coord_origin: BOTTOMLEFT
      l: 317.53033447265625
      r: 559.0158081054688
      t: 249.28826904296875
    charspan:
    - 0
    - 188
    page_no: 9
  text: 'Figure 6: Example layout predictions on selected pages from the DocLayNet
    test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show
    accurate list-item and paragraph '
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/118
  hash: 15606020167439278095
  label: text
  orig: 'Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang.
    ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow
    export, opencv dnn support, October 2021.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 242.22409057617188
      coord_origin: BOTTOMLEFT
      l: 68.69137573242188
      r: 295.22406005859375
      t: 265.4314270019531
    charspan:
    - 0
    - 195
    page_no: 9
  text: 'Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang.
    ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow
    export, opencv dnn support, October 2021.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/119
  hash: 14342144244909907366
  label: list_item
  orig: '[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,
    Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.
    CoRR , abs/2005.12872, 2020.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 218.56314086914062
      coord_origin: BOTTOMLEFT
      l: 53.56020736694336
      r: 295.12176513671875
      t: 241.63282775878906
    charspan:
    - 0
    - 190
    page_no: 9
  text: '[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,
    Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.
    CoRR , abs/2005.12872, 2020.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/120
  hash: 8955370194868803712
  label: list_item
  orig: '[15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and
    efficient object detection. CoRR , abs/1911.09070, 2019.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 202.62213134765625
      coord_origin: BOTTOMLEFT
      l: 53.61275863647461
      r: 294.3653869628906
      t: 217.57615661621094
    charspan:
    - 0
    - 132
    page_no: 9
  text: '[15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and
    efficient object detection. CoRR , abs/1911.09070, 2019.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/121
  hash: 13212807811422473787
  label: list_item
  orig: "[16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev,\
    \ Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\xE1r,\
    \ and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 178.71910095214844
      coord_origin: BOTTOMLEFT
      l: 53.668941497802734
      r: 295.2226257324219
      t: 201.57443237304688
    charspan:
    - 0
    - 219
    page_no: 9
  text: "[16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev,\
    \ Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\xE1r,\
    \ and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014."
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/122
  hash: 7441487755804462640
  label: list_item
  orig: '[17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross
    Girshick. Detectron2, 2019.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 162.77911376953125
      coord_origin: BOTTOMLEFT
      l: 53.54263687133789
      r: 295.1200866699219
      t: 178.3345947265625
    charspan:
    - 0
    - 100
    page_no: 9
  text: '[17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross
    Girshick. Detectron2, 2019.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/123
  hash: 17408271425993029853
  label: list_item
  orig: '[18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk,
    Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and
    Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks.
    In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages
    1513715145, feb 2021.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 122.92810821533203
      coord_origin: BOTTOMLEFT
      l: 53.569610595703125
      r: 294.8847351074219
      t: 162.23497009277344
    charspan:
    - 0
    - 339
    page_no: 9
  text: '[18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk,
    Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and
    Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks.
    In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages
    1513715145, feb 2021.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/124
  hash: 8781691199018342705
  label: list_item
  orig: '[19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.
    Layoutlm: Pre-training of text and layout for document image understanding. In
    Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing
    Machinery.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 82.67352294921875
      coord_origin: BOTTOMLEFT
      l: 53.4610595703125
      r: 295.22174072265625
      t: 122.19474029541016
    charspan:
    - 0
    - 336
    page_no: 9
  text: '[19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.
    Layoutlm: Pre-training of text and layout for document image understanding. In
    Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing
    Machinery.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/125
  hash: 2159895940565677367
  label: list_item
  orig: '[20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang.
    Vtlayout: Fusion of visual and text features for document layout analysis, 2021.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 249.62921142578125
      coord_origin: BOTTOMLEFT
      l: 317.6278076171875
      r: 559.0263671875
      t: 265.5798645019531
    charspan:
    - 0
    - 153
    page_no: 9
  text: '[20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang.
    Vtlayout: Fusion of visual and text features for document layout analysis, 2021.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/126
  hash: 15008793456124101567
  label: list_item
  orig: '[22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus
    conversion service: A machine learning platform to ingest documents at scale.
    In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining , KDD, pages 774-782. ACM, 2018.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 194.28546142578125
      coord_origin: BOTTOMLEFT
      l: 317.6616516113281
      r: 559.275390625
      t: 225.54457092285156
    charspan:
    - 0
    - 290
    page_no: 9
  text: '[22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus
    conversion service: A machine learning platform to ingest documents at scale.
    In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining , KDD, pages 774-782. ACM, 2018.'
- children: []
  dloc: 5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc#/texts/127
  hash: 17416865681467935095
  label: list_item
  orig: '[23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of Big Data , 6(1):60, 2019.'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 178.71212768554688
      coord_origin: BOTTOMLEFT
      l: 317.65606689453125
      r: 559.3782958984375
      t: 193.30506896972656
    charspan:
    - 0
    - 138
    page_no: 9
  text: '[23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of Big Data , 6(1):60, 2019.'
version: 0.0.1
